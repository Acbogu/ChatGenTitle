[{"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning has been applied to the problem of X-ray diffraction phaseprediction with promising results. In this paper, we describe a method forusing machine learning to predict crystal structure phases from X-raydiffraction data of transition metals and their oxides. We evaluate theperformance of our method and compare the variety of its settings. Our resultsdemonstrate that the proposed machine learning framework achieves competitiveperformance. This demonstrates the potential for machine learning tosignificantly impact the field of X-ray diffraction and crystal structuredetermination. Open-source implementation:", "output": "Machine learning-assisted close-set X-ray diffraction phase identification of transition metals."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Building Information Modeling (BIM) technology is a key component of modernconstruction engineering and project management workflows. As-is BIM modelsthat represent the spatial reality of a project site can offer crucialinformation to stakeholders for construction progress monitoring, errorchecking, and building maintenance purposes. Geometric methods forautomatically converting raw scan data into BIM models (Scan-to-BIM) often failto make use of higher-level semantic information in the data. Whereas, semanticsegmentation methods only output labels at the point level without creatingobject level models that is necessary for BIM. To address these issues, thisresearch proposes a hybrid semantic-geometric approach for clutter-resistantfloorplan generation from laser-scanned building point clouds. The input pointclouds are first pre-processed by normalizing the coordinate system andremoving outliers. Then, a semantic segmentation network based on PointNet++ isused to label each point as ceiling, floor, wall, door, stair, and clutter. Theclutter points are removed whereas the wall, door, and stair points are usedfor 2D floorplan generation. A region-growing segmentation algorithm pairedwith geometric reasoning rules is applied to group the points together intoindividual building elements. Finally, a 2-fold Random Sample Consensus(RANSAC) algorithm is applied to parameterize the building elements into 2Dlines which are used to create the output floorplan. The proposed method isevaluated using the metrics of precision, recall, Intersection-over-Union(IOU), Betti error, and warping error.", "output": "A Hybrid Semantic-Geometric Approach for Clutter-Resistant Floorplan Generation from Building Point Clouds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Evaluating canine electrocardiograms (ECG) require skilled veterinarians, butcurrent availability of veterinary cardiologists for ECG interpretation anddiagnostic support is limited. Developing tools for automated assessment of ECGsequences can improve veterinary care by providing clinicians real-time resultsand decision support tools. We implement a deep convolutional neural network(CNN) approach for classifying canine electrocardiogram sequences as eithernormal or abnormal. ECG records are converted into 8 second Lead II sequencesand classified as either normal (no evidence of cardiac abnormalities) orabnormal (presence of one or more cardiac abnormalities). For training ECGsequences are randomly augmented using RandomAugmentECG, a new augmentationlibrary implemented specifically for this project. Each chunk is then isconverted using a continuous wavelet transform into a 2D scalogram. The 2Dscalogram are then classified as either normal or abnormal by a binary CNNclassifier. Experimental results are validated against three boarded veterinarycardiologists achieving an AUC-ROC score of 0.9506 on test dataset matchinghuman level performance. Additionally, we describe model deployment toMicrosoft Azure using an MLOps approach. To our knowledge, this work is one ofthe first attempts to implement a deep learning model to automatically classifyECG sequences for canines.Implementing automated ECG classification willenhance veterinary care through improved diagnostic performance and increasedclinic efficiency.", "output": "PulseNet: Deep Learning ECG-signal classification using random augmentation policy and continous wavelet transform for canines."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In a surprising turn, Large Language Models (LLMs) together with a growingarsenal of prompt-based heuristics now offer powerful off-the-shelf approachesproviding few-shot solutions to myriad classic NLP problems. However, despitepromising early results, these LLM-based few-shot methods remain far from thestate of the art in Named Entity Recognition (NER), where prevailing methodsinclude learning representations via end-to-end structural understanding andfine-tuning on standard labeled corpora. In this paper, we introduce PromptNER,a new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt toany new NER task PromptNER requires a set of entity definitions in addition tothe standard few-shot examples. Given a sentence, PromptNER prompts an LLM toproduce a list of potential entities along with corresponding explanationsjustifying their compatibility with the provided entity type definitions.Remarkably, PromptNER achieves state-of-the-art performance on few-shot NER,achieving an 11% (absolute) improvement in F1 score on the ConLL dataset, and a10% (absolute) improvement on the FewNERD dataset. PromptNER also moves thestate of the art on Cross Domain NER, outperforming all prior methods(including those not limited to the few-shot setting), setting a new mark onall 5 CrossNER target domains, with an average F1 gain of 9%, despite usingless than 2% of the available data.", "output": "PromptNER: Prompting For Named Entity Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The adoption of data science brings vast benefits to Small and Medium-sizedEnterprises (SMEs) including business productivity, economic growth, innovationand jobs creation. Data Science can support SMEs to optimise productionprocesses, anticipate customers' needs, predict machinery failures and deliverefficient smart services. Businesses can also harness the power of ArtificialIntelligence (AI) and Big Data and the smart use of digital technologies toenhance productivity and performance, paving the way for innovation. However,integrating data science decisions into an SME requires both skills and ITinvestments. In most cases, such expenses are beyond the means of SMEs due tolimited resources and restricted access to financing. This paper presentstrends and challenges towards an effective data-driven decision making fororganisations based on a case study of 85 SMEs, mostly from the West Midlandsregion of England. The work is supported as part of a 3 years ERDF (EuropeanRegional Development Funded project) in the areas of big data management,analytics and business intelligence. We present two case studies thatdemonstrates the potential of Digitisation, AI and Machine Learning and usethese as examples to unveil challenges and showcase the wealth of currentavailable opportunities for SMEs.", "output": "Trends and Challenges Towards an Effective Data-Driven Decision Making in UK SMEs: Case Studies and Lessons Learnt from the Analysis of 85 SMEs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Open-world survival games pose significant challenges for AI algorithms dueto their multi-tasking, deep exploration, and goal prioritization requirements.Despite reinforcement learning (RL) being popular for solving games, its highsample complexity limits its effectiveness in complex open-world games likeCrafter or Minecraft. We propose a novel approach, SPRING, to read the game'soriginal academic paper and use the knowledge learned to reason and play thegame through a large language model (LLM). Prompted with the LaTeX source asgame context and a description of the agent's current observation, our SPRINGframework employs a directed acyclic graph (DAG) with game-related questions asnodes and dependencies as edges. We identify the optimal action to take in theenvironment by traversing the DAG and calculating LLM responses for each nodein topological order, with the LLM's answer to final node directly translatingto environment actions. In our experiments, we study the quality of in-context\"reasoning\" induced by different forms of prompts under the setting of theCrafter open-world environment. Our experiments suggest that LLMs, whenprompted with consistent chain-of-thought, have great potential in completingsophisticated high-level trajectories. Quantitatively, SPRING with GPT-4outperforms all state-of-the-art RL baselines, trained for 1M steps, withoutany training. Finally, we show the potential of games as a test bed for LLMs.", "output": "SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (LLMs) have shown impressive capabilities in naturallanguage understanding and generation. Their potential for deeper userunderstanding and improved personalized user experience on recommendationplatforms is, however, largely untapped. This paper aims to address this gap.Recommender systems today capture users' interests through encoding theirhistorical activities on the platforms. The generated user representations arehard to examine or interpret. On the other hand, if we were to ask people aboutinterests they pursue in their life, they might talk about their hobbies, likeI just started learning the ukulele, or their relaxation routines, e.g., I liketo watch Saturday Night Live, or I want to plant a vertical garden. We argue,and demonstrate through extensive experiments, that LLMs as foundation modelscan reason through user activities, and describe their interests in nuanced andinteresting ways, similar to how a human would.We define interest journeys as the persistent and overarching user interests,in other words, the non-transient ones. These are the interests that we believewill benefit most from the nuanced and personalized descriptions. We introducea framework in which we first perform personalized extraction of interestjourneys, and then summarize the extracted journeys via LLMs, using techniqueslike few-shot prompting, prompt-tuning and fine-tuning. Together, our resultsin prompting LLMs to name extracted user journeys in a large-scale industrialplatform demonstrate great potential of these models in providing deeper, moreinterpretable, and controllable user understanding. We believe LLM powered userunderstanding can be a stepping stone to entirely new user experiences onrecommendation platforms that are journey-aware, assistive, and enablingfrictionless conversation down the line.", "output": "Large Language Models for User Interest Journeys."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large Language Models (LLMs) have successfully been applied to codegeneration tasks, raising the question of how well these models understandprogramming. Typical programming languages have invariances and equivariancesin their semantics that human programmers intuitively understand and exploit,such as the (near) invariance to the renaming of identifiers. We show that LLMsnot only fail to properly generate correct Python code when default functionnames are swapped, but some of them even become more confident in theirincorrect predictions as the model size increases, an instance of the recentlydiscovered phenomenon of Inverse Scaling, which runs contrary to the commonlyobserved trend of increasing prediction quality with increasing model size. Ourfindings indicate that, despite their astonishing typical-case performance,LLMs still lack a deep, abstract understanding of the content they manipulate,making them unsuitable for tasks that statistically deviate from their trainingdata, and that mere scaling is not enough to achieve such capability.", "output": "The Larger They Are, the Harder They Fail: Language Models do not Recognize Identifier Swaps in Python."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Humans have developed the capability to teach relevant aspects of new oradapted tasks to a social peer with very few task demonstrations by making useof scaffolding strategies that leverage prior knowledge and importantly priorjoint experience to yield a joint understanding and a joint execution of therequired steps to solve the task. This process has been discovered and analyzedin parent-infant interaction and constitutes a ``co-construction'' as it allowsboth, the teacher and the learner, to jointly contribute to the task. Wepropose to focus research in robot interactive learning on this co-constructionprocess to enable robots to learn from non-expert users in everyday situations.In the following, we will review current proposals for interactive tasklearning and discuss their main contributions with respect to the entailinginteraction. We then discuss our notion of co-construction and summarizeresearch insights from adult-child and human-robot interactions to elucidateits nature in more detail. From this overview we finally derive researchdesiderata that entail the dimensions architecture, representation, interactionand explainability.", "output": "From Interactive to Co-Constructive Task Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Translating natural language sentences to first-order logic (NL-FOLtranslation) is a longstanding challenge in the NLP and formal logicliterature. This paper introduces LogicLLaMA, a LLaMA-7B model fine-tuned forNL-FOL translation using LoRA on a single GPU. LogicLLaMA is capable ofdirectly translating natural language into FOL rules, which outperformsGPT-3.5. LogicLLaMA is also equipped to correct FOL rules predicted by GPT-3.5,and can achieve similar performance as GPT-4 with a fraction of the cost. Thiscorrection ability was achieved by a novel supervised fine-tuning (SFT) +reinforcement learning with human feedback (RLHF) framework, which initiallytrains on synthetically perturbed NL-FOL pairs to encourage chain-of-thoughtreasoning and then fine-tunes with RLHF on GPT-3.5 outputs using a FOL verifieras the reward model.To train LogicLLaMA, we present MALLS (large language $textbf{M}$odelgener$textbf{A}$ted N$textbf{L}$-FO$textbf{L}$ pair$textbf{S}$), a datasetof 34K high-quality and diverse sentence-level NL-FOL pairs collected fromGPT-4. The dataset was created by implementing a pipeline that prompts GPT-4for pairs, and dynamically adjusts the prompts to ensure the collection ofpairs with rich and diverse contexts at different levels of complexity, andverifies the validity of the generated FOL rules. Codes, weights, and data areavailable at $href{", "output": "Harnessing the Power of Large Language Models for Natural Language to First-Order Logic Translation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A growing body of evidence suggests that neural networks employed in deepreinforcement learning (RL) gradually lose their plasticity, the ability tolearn from new data; however, the analysis and mitigation of this phenomenon ishampered by the complex relationship between plasticity, exploration, andperformance in RL. This paper introduces plasticity injection, a minimalisticintervention that increases the network plasticity without changing the numberof trainable parameters or biasing the predictions. The applications of thisintervention are two-fold: first, as a diagnostic tool $unicode{x2014}$ ifinjection increases the performance, we may conclude that an agent's networkwas losing its plasticity. This tool allows us to identify a subset of Atarienvironments where the lack of plasticity causes performance plateaus,motivating future studies on understanding and combating plasticity loss.Second, plasticity injection can be used to improve the computationalefficiency of RL training if the agent has to re-learn from scratch due toexhausted plasticity or by growing the agent's network dynamically withoutcompromising performance. The results on Atari show that plasticity injectionattains stronger performance compared to alternative methods while beingcomputationally efficient.", "output": "Deep Reinforcement Learning with Plasticity Injection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The Levy walk in which the frequency of occurrence of step lengths follows apower-law distribution, can be observed in the migratory behavior of organismsat various levels. Levy walks with power exponents close to 2 are observed, andthe reasons are unclear. This study aims to propose a model that universallygenerates inverse square Levy walks (called Cauchy walks) and to identify theconditions under which Cauchy walks appear. We demonstrate that Cauchy walksemerge universally in goal-oriented tasks. We use the term \"goal-oriented\" whenthe goal is clear, but this can be achieved in different ways, which cannot beuniquely determined. We performed a simulation in which an agent observed thedata generated from a probability distribution in a two-dimensional space andsuccessively estimated the central coordinates of that probabilitydistribution. The agent has a model of probability distribution as a hypothesisfor data-generating distribution and can modify the model such that each time adata point is observed, thereby increasing the estimated probability ofoccurrence of the observed data. To achieve this, the center coordinates of themodel must be close to those of the observed data. However, in the case of atwo-dimensional space, arbitrariness arises in the direction of correction ofthe center; this task is goal oriented. We analyze two cases: a strategy thatallocates the amount of modification randomly in the x- and y-directions, and astrategy that determines allocation such that movement is minimized. Theresults reveal that when a random strategy is used, the frequency of occurrenceof the movement lengths shows a power-law distribution with exponent 2. Whenthe minimum strategy is used, the Brownian walk appears. The presence orabsence of the constraint of minimizing the amount of movement may be a factorthat causes the difference between Brownian and Levy walks.", "output": "Inverse square Levy walk emerging universally in goal-oriented tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Natural Language Processing (NLP) models based on Machine Learning (ML) aresusceptible to adversarial attacks -- malicious algorithms that imperceptiblymodify input text to force models into making incorrect predictions. However,evaluations of these attacks ignore the property of imperceptibility or studyit under limited settings. This entails that adversarial perturbations wouldnot pass any human quality gate and do not represent real threats tohuman-checked NLP systems. To bypass this limitation and enable properassessment (and later, improvement) of NLP model robustness, we have surveyed378 human participants about the perceptibility of text adversarial examplesproduced by state-of-the-art methods. Our results underline that existing textattacks are impractical in real-world scenarios where humans are involved. Thiscontrasts with previous smaller-scale human studies, which reported overlyoptimistic conclusions regarding attack success. Through our work, we hope toposition human perceptibility as a first-class success criterion for textattacks, and provide guidance for research to build effective attack algorithmsand, in turn, design appropriate defence mechanisms.", "output": "How do humans perceive adversarial text? A reality check on the validity and naturalness of word-based adversarial attacks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work we consider a generalization of the well-known multivehiclerouting problem: given a network, a set of agents occupying a subset of itsnodes, and a set of tasks, we seek a minimum cost sequence of movements subjectto the constraint that each task is visited by some agent at least once. Theclassical version of this problem assumes a central computational server thatobserves the entire state of the system perfectly and directs individual agentsaccording to a centralized control scheme. In contrast, we assume that there isno centralized server and that each agent is an individual processor with no apriori knowledge of the underlying network (including task and agentlocations). Moreover, our agents possess strictly local communication andsensing capabilities (restricted to a fixed radius around their respectivelocations), aligning more closely with several real-world multiagentapplications. These restrictions introduce many challenges that are overcomethrough local information sharing and direct coordination between agents. Wepresent a fully distributed, online, and scalable reinforcement learningalgorithm for this problem whereby agents self-organize into local clusters andindependently apply a multiagent rollout scheme locally to each cluster. Wedemonstrate empirically via extensive simulations that there exists a criticalsensing radius beyond which the distributed rollout algorithm begins to improveover a greedy base policy. This critical sensing radius grows proportionally tothe $log^*$ function of the size of the network, and is, therefore, a smallconstant for any relevant network. Our decentralized reinforcement learningalgorithm achieves approximately a factor of two cost improvement over the basepolicy for a range of radii bounded from below and above by two and three timesthe critical sensing radius, respectively.", "output": "Distributed Online Rollout for Multivehicle Routing in Unmapped Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The mission of open knowledge graph (KG) completion is to draw new findingsfrom known facts. Existing works that augment KG completion require either (1)factual triples to enlarge the graph reasoning space or (2) manually designedprompts to extract knowledge from a pre-trained language model (PLM),exhibiting limited performance and requiring expensive efforts from experts. Tothis end, we propose TAGREAL that automatically generates quality query promptsand retrieves support information from large text corpora to probe knowledgefrom PLM for KG completion. The results show that TAGREAL achievesstate-of-the-art performance on two benchmark datasets. We find that TAGREALhas superb performance even with limited training data, outperforming existingembedding-based, graph-based, and PLM-based methods.", "output": "Text-Augmented Open Knowledge Graph Completion via Pre-Trained Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reinforcement learning (RL) is an area of significant research interest, andsafe RL in particular is attracting attention due to its ability to handlesafety-driven constraints that are crucial for real-world applications. Thiswork proposes a novel approach to RL training, called control invariant set(CIS) enhanced RL, which leverages the advantages of utilizing the explicitform of CIS to improve stability guarantees and sampling efficiency.Furthermore, the robustness of the proposed approach is investigated in thepresence of uncertainty. The approach consists of two learning stages: offlineand online. In the offline stage, CIS is incorporated into the reward design,initial state sampling, and state reset procedures. This incorporation of CISfacilitates improved sampling efficiency during the offline training process.In the online stage, RL is retrained whenever the predicted next step state isoutside of the CIS, which serves as a stability criterion, by introducing aSafety Supervisor to examine the safety of the action and make necessarycorrections. The stability analysis is conducted for both cases, with andwithout uncertainty. To evaluate the proposed approach, we apply it to asimulated chemical reactor. The results show a significant improvement insampling efficiency during offline training and closed-loop stability guaranteein the online implementation, with and without uncertainty.", "output": "Control invariant set enhanced safe reinforcement learning: improved sampling efficiency, guaranteed stability and robustness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semantic segmentation is a critical task in computer vision that aims toidentify and classify individual pixels in an image, with numerous applicationsfor example autonomous driving and medical image analysis. However, semanticsegmentation can be super challenging particularly due to the need for largeamounts of annotated data. Annotating images is a time-consuming and costlyprocess, often requiring expert knowledge and significant effort. In thispaper, we propose a novel approach for semantic segmentation by eliminating theneed of ground-truth segmentation maps. Instead, our approach requires only therough information of individual semantic class proportions, shortened assemantic proportions. It greatly simplifies the data annotation process andthus will significantly reduce the annotation time and cost, making it morefeasible for large-scale applications. Moreover, it opens up new possibilitiesfor semantic segmentation tasks where obtaining the full ground-truthsegmentation maps may not be feasible or practical. Extensive experimentalresults demonstrate that our approach can achieve comparable and sometimes evenbetter performance against the benchmark method that relies on the ground-truthsegmentation maps. Utilising semantic proportions suggested in this work offersa promising direction for future research in the field of semanticsegmentation.", "output": "Semantic Segmentation by Semantic Proportions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate the question of whether the knowledge learned by graph neuralnetworks (GNNs) from small graphs is generalizable to large graphs in the samedomain. Prior works suggest that the distribution shift, particularly in thedegree distribution, between graphs of different sizes can lead to performancedegradation in the graph classification task. However, this may not be the casefor biological datasets where the degrees are bounded and the distributionshift of degrees is small. Even with little degree distribution shift, ourobservations show that GNNs' performance on larger graphs from the samedatasets still degrades, suggesting other causes. In fact, there has been alack of exploration in real datasets to understand the types and properties ofdistribution shifts caused by various graph sizes. Furthermore, previousanalyses of size generalizability mostly focus on the spatial domain.To fill these gaps, we take the spectral perspective and study the sizegeneralizability of GNNs on biological data. We identify a distribution shiftbetween small and large graphs in the eigenvalues of the normalizedLaplacian/adjacency matrix, indicating a difference in the global nodeconnectivity, which is found to be correlated with the node closenesscentrality. We further find that despite of the variations in globalconnectivity, graphs of different sizes share similar local connectivity, whichcan be utilized to improve the size generalizability of GNNs. Based on ourspectral insights and empirical observations, we propose a model-agnosticstrategy, SIA, which uses size-irrelevant local structural features, i.e., thelocal closeness centrality of a node, to guide the learning process. Ourempirical results demonstrate that our strategy improves the graphclassification performance of various GNNs on small and large graphs whentraining with only small graphs.", "output": "Size Generalizability of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised learning (SSL) is a powerful tool in machine learning, butunderstanding the learned representations and their underlying mechanismsremains a challenge. This paper presents an in-depth empirical analysis ofSSL-trained representations, encompassing diverse models, architectures, andhyperparameters. Our study reveals an intriguing aspect of the SSL trainingprocess: it inherently facilitates the clustering of samples with respect tosemantic labels, which is surprisingly driven by the SSL objective'sregularization term. This clustering process not only enhances downstreamclassification but also compresses the data information. Furthermore, weestablish that SSL-trained representations align more closely with semanticclasses rather than random classes. Remarkably, we show that learnedrepresentations align with semantic classes across various hierarchical levels,and this alignment increases during training and when moving deeper into thenetwork. Our findings provide valuable insights into SSL's representationlearning mechanisms and their impact on performance across different sets ofclasses.", "output": "Reverse Engineering Self-Supervised Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Problem definition: Access to accurate predictions of patients' outcomes canenhance medical staff's decision-making, which ultimately benefits allstakeholders in the hospitals. A large hospital network in the US has beencollaborating with academics and consultants to predict short-term andlong-term outcomes for all inpatients across their seven hospitals.Methodology/results: We develop machine learning models that predict theprobabilities of next 24-hr/48-hr discharge and intensive care unit transfers,end-of-stay mortality and discharge dispositions. All models achieve highout-of-sample AUC (75.7%-92.5%) and are well calibrated. In addition, combining48-hr discharge predictions with doctors' predictions simultaneously enablesmore patient discharges (10%-28.7%) and fewer 7-day/30-day readmissions($p$-value $&lt;0.001$). We implement an automated pipeline that extracts data andupdates predictions every morning, as well as user-friendly software and acolor-coded alert system to communicate these patient-level predictions(alongside explanations) to clinical teams. Managerial implications: Since wehave been gradually deploying the tool, and training medical staff, over 200doctors, nurses, and case managers across seven hospitals use it in their dailypatient review process. We observe a significant reduction in the averagelength of stay (0.67 days per patient) following its adoption and anticipatesubstantial financial benefits (between $55 and $72 million annually) for thehealthcare system.", "output": "Patient Outcome Predictions Improve Operations at a Large Hospital Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Increasing demands in software industry and scarcity of software engineersmotivates researchers and practitioners to automate the process of softwaregeneration and configuration. Large scale automatic software generation andconfiguration is a very complex and challenging task. In this proposal, we setout to investigate this problem by breaking down automatic software generationand configuration into two different tasks. In first task, we propose tosynthesize software automatically with input output specifications. This taskis further broken down into two sub-tasks. The first sub-task is aboutsynthesizing programs with a genetic algorithm which is driven by a neuralnetwork based fitness function trained with program traces and specifications.For the second sub-task, we formulate program synthesis as a continuousoptimization problem and synthesize programs with covariance matrix adaptionevolutionary strategy (a state-of-the-art continuous optimization method).Finally, for the second task, we propose to synthesize configurations of largescale software from different input files (e.g. software manuals,configurations files, online blogs, etc.) using a sequence-to-sequence deeplearning mechanism.", "output": "Learning-Based Automatic Synthesis of Software Code and Configuration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To ensure the out-of-distribution (OOD) generalization performance,traditional domain generalization (DG) methods resort to training on data frommultiple sources with different underlying distributions. And the success ofthose DG methods largely depends on the fact that there are diverse trainingdistributions. However, it usually needs great efforts to obtain enoughheterogeneous data due to the high expenses, privacy issues or the scarcity ofdata. Thus an interesting yet seldom investigated problem arises: how toimprove the OOD generalization performance when the perceived heterogeneity islimited. In this paper, we instantiate a new framework called few-domaingeneralization (FDG), which aims to learn a generalizable model from very fewdomains of novel tasks with the knowledge acquired from previous learningexperiences on base tasks. Moreover, we propose a Meta Adaptive Task Sampling(MATS) procedure to differentiate base tasks according to their semantic anddomain-shift similarity to the novel task. Empirically, we show that the newlyintroduced FDG framework can substantially improve the OOD generalizationperformance on the novel task and further combining MATS with episodic trainingcould outperform several state-of-the-art DG baselines on widely usedbenchmarks like PACS and DomainNet.", "output": "Meta Adaptive Task Sampling for Few-Domain Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Although existing image anomaly detection methods yield impressive results,they are mostly an offline learning paradigm that requires excessive datapre-collection, limiting their adaptability in industrial scenarios with onlinestreaming data. Online learning-based image anomaly detection methods are morecompatible with industrial online streaming data but are rarely noticed. Forthe first time, this paper presents a fully online learning image anomalydetection method, namely LeMO, learning memory for online image anomalydetection. LeMO leverages learnable memory initialized with orthogonal randomnoise, eliminating the need for excessive data in memory initialization andcircumventing the inefficiencies of offline data collection. Moreover, acontrastive learning-based loss function for anomaly detection is designed toenable online joint optimization of memory and image target-oriented features.The presented method is simple and highly effective. Extensive experimentsdemonstrate the superior performance of LeMO in the online setting.Additionally, in the offline setting, LeMO is also competitive with the currentstate-of-the-art methods and achieves excellent performance in few-shotscenarios.", "output": "Towards Total Online Unsupervised Anomaly Detection and Localization in Industrial Vision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modern machine learning applications have seen a remarkable success ofoptimization algorithms that are designed to find flat minima. Motivated bythis paradigm, this work formulates and studies the algorithmic question of howto find flat minima. As an initial effort, this work adopts the trace ofhessian of the cost function as the measure of flatness, and formally definesthe notion of approximate flat minima. Under this notion, we then designalgorithms that find approximate flat minima efficiently. For general costfunctions, we present a gradient-based algorithm that finds an approximate flatlocal minimum efficiently. The main component of the algorithm is to usegradients computed from randomly perturbed iterates to estimate a directionthat leads to flatter minima. For the setting where the cost function is anempirical risk over training data, we present a faster algorithm that isinspired by a recently proposed practical algorithm called sharpness-awareminimization, supporting its success in practice.", "output": "How to escape sharp minima."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Offline-to-online reinforcement learning (RL), by combining the benefits ofoffline pretraining and online finetuning, promises enhanced sample efficiencyand policy performance. However, existing methods, effective as they are,suffer from suboptimal performance, limited adaptability, and unsatisfactorycomputational efficiency. We propose a novel framework, PROTO, which overcomesthe aforementioned limitations by augmenting the standard RL objective with aniteratively evolving regularization term. Performing a trust-region-styleupdate, PROTO yields stable initial finetuning and optimal final performance bygradually evolving the regularization term to relax the constraint strength. Byadjusting only a few lines of code, PROTO can bridge any offline policypretraining and standard off-policy RL finetuning to form a powerfuloffline-to-online RL pathway, birthing great adaptability to diverse methods.Simple yet elegant, PROTO imposes minimal additional computation and enableshighly efficient online finetuning. Extensive experiments demonstrate thatPROTO achieves superior performance over SOTA baselines, offering an adaptableand efficient offline-to-online RL framework.", "output": "PROTO: Iterative Policy Regularized Offline-to-Online Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent advancements in high-quality, large-scale English resources havepushed the frontier of English Automatic Text Simplification (ATS) research.However, less work has been done on multilingual text simplification due to thelack of a diverse evaluation benchmark that covers complex-simple sentencepairs in many languages. This paper introduces the MultiSim benchmark, acollection of 27 resources in 12 distinct languages containing over 1.7 millioncomplex-simple sentence pairs. This benchmark will encourage research indeveloping more effective multilingual text simplification models andevaluation metrics. Our experiments using MultiSim with pre-trainedmultilingual language models reveal exciting performance improvements frommultilingual training in non-English settings. We observe strong performancefrom Russian in zero-shot cross-lingual transfer to low-resource languages. Wefurther show that few-shot prompting with BLOOM-176b achieves comparablequality to reference simplifications outperforming fine-tuned models in mostlanguages. We validate these findings through human evaluation.", "output": "Revisiting non-English Text Simplification: A Unified Multilingual Benchmark."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large Language Models (LLMs) have demonstrated impressive zero-shotcapabilities in long-form text generation tasks expressed through naturallanguage instructions. However, user expectations for long-form text rewritingis high, and unintended rewrites (''hallucinations'') produced by the model cannegatively impact its overall performance. Existing evaluation benchmarksprimarily focus on limited rewriting styles and sentence-level rewriting ratherthan long-form open-ended rewriting.We introduce OpenRewriteEval, a novelbenchmark that covers a wide variety of rewriting types expressed throughnatural language instructions. It is specifically designed to facilitate theevaluation of open-ended rewriting of long-form texts. In addition, we proposea strong baseline model, RewriteLM, an instruction-tuned large language modelfor long-form text rewriting. We develop new strategies that facilitate thegeneration of diverse instructions and preference data with minimal humanintervention. We conduct empirical experiments and demonstrate that our modeloutperforms the current state-of-the-art LLMs in text rewriting. Specifically,it excels in preserving the essential content and meaning of the source text,minimizing the generation of ''hallucinated'' content, while showcasing theability to generate rewrites with diverse wording and structures.", "output": "RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent studies have demonstrated that natural-language prompts can help toleverage the knowledge learned by pre-trained language models for the binarysentence-level sentiment classification task. Specifically, these methodsutilize few-shot learning settings to fine-tune the sentiment classificationmodel using manual or automatically generated prompts. However, the performanceof these methods is sensitive to the perturbations of the utilized prompts.Furthermore, these methods depend on a few labeled instances for automaticprompt generation and prompt ranking. This study aims to find high-qualityprompts for the given task in a zero-shot setting. Given a base prompt, ourproposed approach automatically generates multiple prompts similar to the baseprompt employing positional, reasoning, and paraphrasing techniques and thenranks the prompts using a novel metric. We empirically demonstrate that thetop-ranked prompts are high-quality and significantly outperform the baseprompt and the prompts generated using few-shot learning for the binarysentence-level sentiment classification task.", "output": "Zero-shot Approach to Overcome Perturbation Sensitivity of Prompts."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Currently, video behavior recognition is one of the most foundational tasksof computer vision. The 2D neural networks of deep learning are built forrecognizing pixel-level information such as images with RGB, RGB-D, or opticalflow formats, with the current increasingly wide usage of surveillance videoand more tasks related to human action recognition. There are increasing tasksrequiring temporal information for frames dependency analysis. The researchershave widely studied video-based recognition rather thanimage-based(pixel-based) only to extract more informative elements fromgeometry tasks. Our current related research addresses multiple novel proposedresearch works and compares their advantages and disadvantages between thederived deep learning frameworks rather than machine learning frameworks. Thecomparison happened between existing frameworks and datasets, which are videoformat data only. Due to the specific properties of human actions and theincreasingly wide usage of deep neural networks, we collected all researchworks within the last three years between 2020 to 2022. In our article, theperformance of deep neural networks surpassed most of the techniques in thefeature learning and extraction tasks, especially video action recognition.", "output": "Deep Neural Networks in Video Human Action Recognition: A Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With strong capabilities of reasoning and a generic understanding of theworld, Large Language Models (LLMs) have shown great potential in buildingversatile embodied decision making agents capable of performing diverse tasks.However, when deployed to unfamiliar environments, we show that LLM agents facechallenges in efficiently gathering necessary information, leading tosuboptimal performance. On the other hand, in unfamiliar scenarios, humanindividuals often seek additional information from their peers before takingaction, leveraging external knowledge to avoid unnecessary trial and error.Building upon this intuition, we propose textit{Asking Before Action} (ABA), amethod that empowers the agent to proactively query external sources forpertinent information using natural language during their interactions in theenvironment. In this way, the agent is able to enhance its efficiency andperformance by mitigating wasteful steps and circumventing the difficultiesassociated with exploration in unfamiliar environments. We empirically evaluateour method on an embodied decision making benchmark, ALFWorld, and demonstratethat despite modest modifications in prompts, our method exceeds baseline LLMagents by more than $40$%. Further experiments on two variants of ALFWorldillustrate that by imitation learning, ABA effectively retains and reusesqueried and known information in subsequent tasks, mitigating the need forrepetitive inquiries. Both qualitative and quantitative results exhibitremarkable performance on tasks that previous methods struggle to solve.", "output": "Asking Before Action: Gather Information in Embodied Decision Making with Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, inference privacy has attracted increasing attention. The inferenceprivacy concern arises most notably in the widely deployed edge-cloud videoanalytics systems, where the cloud needs the videos captured from the edge. Thevideo data can contain sensitive information and subject to attack when theyare transmitted to the cloud for inference. Many privacy protection schemeshave been proposed. Yet, the performance of a scheme needs to be determined byexperiments or inferred by analyzing the specific case. In this paper, wepropose a new metric, textit{privacy protectability}, to characterize to whatdegree a video stream can be protected given a certain video analytics task.Such a metric has strong operational meaning. For example, low protectabilitymeans that it may be necessary to set up an overall secure environment. We canalso evaluate a privacy protection scheme, e.g., assume it obfuscates the videodata, what level of protection this scheme has achieved after obfuscation. Ourdefinition of privacy protectability is rooted in information theory and wedevelop efficient algorithms to estimate the metric. We use experiments on realdata to validate that our metric is consistent with empirical measurements onhow well a video stream can be protected for a video analytics task.", "output": "Privacy Protectability: An Information-theoretical Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep neural networks (DNNs) have demonstrated extraordinary capabilities andare an integral part of modern software systems. However, they also suffer fromvarious vulnerabilities such as adversarial attacks and unfairness. Testingdeep learning (DL) systems is therefore an important task, to detect andmitigate those vulnerabilities. Motivated by the success of traditionalsoftware testing, which often employs diversity heuristics, various diversitymeasures on DNNs have been proposed to help efficiently expose the buggybehavior of DNNs. In this work, we argue that many DNN testing tasks should betreated as directed testing problems rather than general-purpose testing tasks,because these tasks are specific and well-defined. Hence, the diversity-basedapproach is less effective.Following our argument based on the semantics of DNNs and the testing goal,we derive $6$ metrics that can be used for DNN testing and carefully analyzetheir application scopes. We empirically show their efficacy in exposing bugsin DNNs compared to recent diversity-based metrics. Moreover, we also noticediscrepancies between the practices of the software engineering (SE) communityand the DL community. We point out some of these gaps, and hopefully, this canlead to bridging the SE practice and DL findings.", "output": "Rethink Diversity in Deep Learning Testing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While distributional reinforcement learning (RL) has demonstrated empiricalsuccess, the question of when and why it is beneficial has remained unanswered.In this work, we provide one explanation for the benefits of distributional RLthrough the lens of small-loss bounds, which scale with the instance-dependentoptimal cost. If the optimal cost is small, our bounds are stronger than thosefrom non-distributional approaches. As warmup, we show that learning the costdistribution leads to small-loss regret bounds in contextual bandits (CB), andwe find that distributional CB empirically outperforms the state-of-the-art onthree challenging tasks. For online RL, we propose a distributionalversion-space algorithm that constructs confidence sets using maximumlikelihood estimation, and we prove that it achieves small-loss regret in thetabular MDPs and enjoys small-loss PAC bounds in latent variable models.Building on similar insights, we propose a distributional offline RL algorithmbased on the pessimism principle and prove that it enjoys small-loss PACbounds, which exhibit a novel robustness property. For both online and offlineRL, our results provide the first theoretical benefits of learningdistributions even when we only need the mean for making decisions.", "output": "The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The representation gap between teacher and student is an emerging topic inknowledge distillation (KD). To reduce the gap and improve the performance,current methods often resort to complicated training schemes, loss functions,and feature alignments, which are task-specific and feature-specific. In thispaper, we state that the essence of these methods is to discard the noisyinformation and distill the valuable information in the feature, and propose anovel KD method dubbed DiffKD, to explicitly denoise and match features usingdiffusion models. Our approach is based on the observation that studentfeatures typically contain more noises than teacher features due to the smallercapacity of student model. To address this, we propose to denoise studentfeatures using a diffusion model trained by teacher features. This allows us toperform better distillation between the refined clean feature and teacherfeature. Additionally, we introduce a light-weight diffusion model with alinear autoencoder to reduce the computation cost and an adpative noisematching module to improve the denoising performance. Extensive experimentsdemonstrate that DiffKD is effective across various types of features andachieves state-of-the-art performance consistently on image classification,object detection, and semantic segmentation tasks. Code will be available at", "output": "Knowledge Diffusion for Distillation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent progress in music generation has been remarkably advanced by thestate-of-the-art MusicLM, which comprises a hierarchy of three LMs,respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet,sampling with the MusicLM requires processing through these LMs one by one toobtain the fine-grained acoustic tokens, making it computationally expensiveand prohibitive for a real-time generation. Efficient music generation with aquality on par with MusicLM remains a significant challenge. In this paper, wepresent MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusionmodel that generates music audios of state-of-the-art quality meanwhilereducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM forsemantic modeling, and applies a novel dual-path diffusion (DPD) model and anaudio VAE-GAN to efficiently decode the conditioning semantic tokens intowaveform. DPD is proposed to simultaneously model the coarse and fine acousticsby incorporating the semantic information into segments of latents effectivelyvia cross-attention at each denoising step. Our experimental results suggestthe superiority of MeLoDy, not only in its practical advantages on samplingspeed and infinitely continuable generation, but also in its state-of-the-artmusicality, audio quality, and text correlation.Our samples are available at ", "output": "Efficient Neural Music Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Several recent studies have elucidated why knowledge distillation (KD)improves model performance. However, few have researched the other advantagesof KD in addition to its improving model performance. In this study, we haveattempted to show that KD enhances the interpretability as well as the accuracyof models. We measured the number of concept detectors identified in networkdissection for a quantitative comparison of model interpretability. Weattributed the improvement in interpretability to the class-similarityinformation transferred from the teacher to student models. First, we confirmedthe transfer of class-similarity information from the teacher to student modelvia logit distillation. Then, we analyzed how class-similarity informationaffects model interpretability in terms of its presence or absence and degreeof similarity information. We conducted various quantitative and qualitativeexperiments and examined the results on different datasets, different KDmethods, and according to different measures of interpretability. Our researchshowed that KD models by large models could be used more reliably in variousfields.", "output": "On the Impact of Knowledge Distillation for Model Interpretability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most MPC (Model Predictive Control) algorithms used in industries and studiedin the control academia use a two-term QP (quadratic programming), where thefirst term is the weighted norm of the output errors, and the second term isthat of the input increments. In this work, a DMC (Dynamic Matrix Control)algorithm that uses three-term QP is studied, where the third term is theweighted norm of the output increments. In the analysis, a relationship betweenthe three-term DMC and the two-term DMC is established; based on that, theclosed-loop response curves are derived. Based on the analysis, two controllertuning procedures are developed for the three-term DMC, one for closed-loopstep response and one for disturbance reduction. Finally, it will be proventhat the three-term DMC can achieve a higher performance and robustness thanthe two-term DMC can. Simulation studies are used to demonstrate the findingsand the tuning methods.", "output": "Analysis and tuning of a three-term DMC."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "When virtual agents interact with humans, gestures are crucial to deliveringtheir intentions with speech. Previous multimodal co-speech gesture generationmodels required encoded features of all modalities to generate gestures. Ifsome input modalities are removed or contain noise, the model may not generatethe gestures properly. To acquire robust and generalized encodings, we proposea novel framework with a multimodal pre-trained encoder for co-speech gesturegeneration. In the proposed method, the multi-head-attention-based encoder istrained with self-supervised learning to contain the information on eachmodality. Moreover, we collect full-body gestures that consist of 3D jointrotations to improve visualization and apply gestures to the extensible bodymodel. Through the series of experiments and human evaluation, the proposedmethod renders realistic co-speech gestures not only when all input modalitiesare given but also when the input modalities are missing or noisy.", "output": "MPE4G: Multimodal Pretrained Encoder for Co-Speech Gesture Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Traffic simulation is a crucial tool for transportation decision-making andpolicy development. However, achieving realistic simulations in the face of thehigh dimensionality and heterogeneity of traffic environments is a longstandingchallenge. In this paper, we present TransWordNG, a traffic simulator that usesData-driven algorithms and Graph Computing techniques to learn traffic dynamicsfrom real data. The functionality and structure of TransWorldNG are introduced,which utilize a foundation model for transportation management and control. Theresults demonstrate that TransWorldNG can generate more realistic trafficpatterns compared to traditional simulators. Additionally, TransWorldNGexhibits better scalability, as it shows linear growth in computation time asthe scenario scale increases. To the best of our knowledge, this is the firsttraffic simulator that can automatically learn traffic patterns from real-worlddata and efficiently generate accurate and realistic traffic environments.", "output": "TransWorldNG: Traffic Simulation via Foundation Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent years have seen increasing concerns about the unsafe responsegeneration of large-scale dialogue systems, where agents will learn offensiveor biased behaviors from the real-world corpus. Some methods are proposed toaddress the above issue by detecting and replacing unsafe training examples ina pipeline style. Though effective, they suffer from a high annotation cost andadapt poorly to unseen scenarios as well as adversarial attacks. Besides, theneglect of providing safe responses (e.g. simply replacing with templates) willcause the information-missing problem of dialogues. To address these issues, wepropose an unsupervised pseudo-label sampling method, TEMP, that canautomatically assign potential safe responses. Specifically, our TEMP methodgroups responses into several clusters and samples multiple labels with anadaptively sharpened sampling strategy, inspired by the observation that unsafesamples in the clusters are usually few and distribute in the tail. Extensiveexperiments in chitchat and task-oriented dialogues show that our TEMPoutperforms state-of-the-art models with weak supervision signals and obtainscomparable results under unsupervised learning settings.", "output": "Healing Unsafe Dialogue Responses with Weak Supervision Signals."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper addresses the problem of 3D referring expression comprehension(REC) in autonomous driving scenario, which aims to ground a natural languageto the targeted region in LiDAR point clouds. Previous approaches for RECusually focus on the 2D or 3D-indoor domain, which is not suitable foraccurately predicting the location of the queried 3D region in an autonomousdriving scene. In addition, the upper-bound limitation and the heavycomputation cost motivate us to explore a better solution. In this work, wepropose a new multi-modal visual grounding task, termed LiDAR Grounding. Thenwe devise a Multi-modal Single Shot Grounding (MSSG) approach with an effectivetoken fusion strategy. It jointly learns the LiDAR-based object detector withthe language features and predicts the targeted region directly from thedetector without any post-processing. Moreover, the image feature can beflexibly integrated into our approach to provide rich texture and colorinformation. The cross-modal learning enforces the detector to concentrate onimportant regions in the point cloud by considering the informative languageexpressions, thus leading to much better accuracy and efficiency. Extensiveexperiments on the Talk2Car dataset demonstrate the effectiveness of theproposed methods. Our work offers a deeper insight into the LiDAR-basedgrounding task and we expect it presents a promising direction for theautonomous driving community.", "output": "Language-Guided 3D Object Detection in Point Cloud for Autonomous Driving."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent years have seen increasing concerns about the private inference of NLPservices and Transformer models. However, existing two-party privacy-preservingmethods solely consider NLU scenarios, while the private inference of textgeneration such as translation, dialogue, and code completion remains unsolved.Besides, while migrated to NLG models, existing privacy-preserving methodsperform poorly in terms of inference speed, and suffer from the convergenceproblem during the training stage. To address these issues, we propose MERGE, afast private text generation framework for Transformer-based language models.Specifically, MERGE reuse the output hidden state as the word embedding tobypass the embedding computation, and reorganize the linear operations in theTransformer module to accelerate the forward procedure. Based on these twooptimizations, extensive experiments show that MERGE can achieve a 26.5xspeedup under the sequence length 512, and reduce 80% communication bytes,with an up to 10x speedup to existing state-of-art models.", "output": "MERGE: Fast Private Text Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Time series prediction is a prevalent issue across various disciplines, suchas meteorology, traffic surveillance, investment, and energy production andconsumption. Many statistical and machine-learning strategies have beendeveloped to tackle this problem. However, these approaches either lackexplainability or exhibit less satisfactory performance when the predictionhorizon increases. To this end, we propose a novel plan for the designing ofnetworks' architecture based on transformations, possessing the potential toachieve an enhanced receptive field in learning which brings benefits to fusefeatures across scales. In this context, we introduce four differenttransformation mechanisms as bases to construct the learning model includingFourier Transform (FT), Singular Value Decomposition (SVD), matrixmultiplication and Conv block. Hence, we develop four learning models based onthe above building blocks, namely, FT-Matrix, FT-SVD, FT-Conv, and Conv-SVD.Note that the FT and SVD blocks are capable of learning global information,while the Conv blocks focus on learning local information. The matrix block issparsely designed to learn both global and local information simultaneously.The above Transformation Learning Networks (TLNets) have been extensivelytested and compared with multiple baseline models based on several real-worlddatasets and showed clear potential in long-range time-series forecasting.", "output": "TLNets: Transformation Learning Networks for long-range time-series prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Intrigued by the claims of emergent reasoning capabilities in LLMs trained ongeneral web corpora, in this paper, we set out to investigate their planningcapabilities. We aim to evaluate (1) the effectiveness of LLMs in generatingplans autonomously in commonsense planning tasks and (2) the potential of LLMsas a source of heuristic guidance for other agents (AI planners) in theirplanning tasks. We conduct a systematic study by generating a suite ofinstances on domains similar to the ones employed in the International PlanningCompetition and evaluate LLMs in two distinct modes: autonomous and heuristic.Our findings reveal that LLMs' ability to generate executable plansautonomously is rather limited, with the best model (GPT-4) having an averagesuccess rate of ~12% across the domains. However, the results in the heuristicmode show more promise. In the heuristic mode, we demonstrate thatLLM-generated plans can improve the search process for underlying soundplanners and additionally show that external verifiers can help providefeedback on the generated plans and back-prompt the LLM for better plangeneration.", "output": "On the Planning Abilities of Large Language Models -- A Critical Investigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The paper speculates about how ChatGPT-like systems can support the field ofautomated service composition and identifies new research areas to explore inorder to take advantage of such tools in the field of service-orientedcomposition.", "output": "Service Composition in the ChatGPT Era."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, numerous screening methods have been published forultrahigh-dimensional data that contain hundreds of thousands of features;however, most of these features cannot handle data with thousands of classes.Prediction models built to authenticate users based on multichannel biometricdata result in this type of problem. In this study, we present a novel methodknown as random forest-based multiround screening (RFMS) that can beeffectively applied under such circumstances. The proposed algorithm dividesthe feature space into small subsets and executes a series of partial modelbuilds. These partial models are used to implement tournament-based sorting andthe selection of features based on their importance. To benchmark RFMS, asynthetic biometric feature space generator known as BiometricBlender isemployed. Based on the results, the RFMS is on par with industry-standardfeature screening methods while simultaneously possessing many advantages overthese methods.", "output": "Feature space reduction method for ultrahigh-dimensional, multiclass data: Random forest-based multiround screening (RFMS)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A successful tactic that is followed by the scientific community foradvancing AI is to treat games as problems, which has been proven to lead tovarious breakthroughs. We adapt this strategy in order to study Rocket League,a widely popular but rather under-explored 3D multiplayer video game with adistinct physics engine and complex dynamics that pose a significant challengein developing efficient and high-performance game-playing agents. In thispaper, we present Lucy-SKG, a Reinforcement Learning-based model that learnedhow to play Rocket League in a sample-efficient manner, outperforming by anotable margin the two highest-ranking bots in this game, namely Necto (2022bot champion) and its successor Nexto, thus becoming a state-of-the-art agent.Our contributions include: a) the development of a reward analysis andvisualization library, b) novel parameterizable reward shape functions thatcapture the utility of complex reward types via our proposed Kinesthetic RewardCombination (KRC) technique, and c) design of auxiliary neural architecturesfor training on reward prediction and state representation tasks in anon-policy fashion for enhanced efficiency in learning speed and performance. Byperforming thorough ablation studies for each component of Lucy-SKG, we showedtheir independent effectiveness in overall performance. In doing so, wedemonstrate the prospects and challenges of using sample-efficientReinforcement Learning techniques for controlling complex dynamical systemsunder competitive team-based multiplayer conditions.", "output": "Lucy-SKG: Learning to Play Rocket League Efficiently Using Deep Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (LLMs) providing generative AI have become popular tosupport software engineers in creating, summarizing, optimizing, anddocumenting source code. It is still unknown how LLMs can support controlengineers using typical control programming languages in programming tasks.Researchers have explored GitHub CoPilot or DeepMind AlphaCode for source codegeneration but did not yet tackle control logic programming. The contributionof this paper is an exploratory study, for which we created 100 LLM prompts in10 representative categories to analyze control logic generation for of PLCsand DCS from natural language. We tested the prompts by generating answers withChatGPT using the GPT-4 LLM. It generated syntactically correct IEC 61131-3Structured Text code in many cases and demonstrated useful reasoning skillsthat could boost control engineer productivity. Our prompt collection is thebasis for a more formal LLM benchmark to test and compare such models forcontrol logic generation.", "output": "ChatGPT for PLC/DCS Control Logic Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Heterogeneous Graph Neural Networks (HGNNs) are a class of powerful deeplearning methods widely used to learn representations of heterogeneous graphs.Despite the fast development of HGNNs, they still face some challenges such asover-smoothing, and non-robustness. Previous studies have shown that theseproblems can be reduced by using gradient regularization methods. However, theexisting gradient regularization methods focus on either graph topology or nodefeatures. There is no universal approach to integrate these features, whichseverely affects the efficiency of regularization. In addition, the inclusionof gradient regularization into HGNNs sometimes leads to some problems, such asan unstable training process, increased complexity and insufficient coverageregularized information. Furthermore, there is still short of a completetheoretical analysis of the effects of gradient regularization on HGNNs. Inthis paper, we propose a novel gradient regularization method called Grug,which iteratively applies regularization to the gradients generated by bothpropagated messages and the node features during the message-passing process.Grug provides a unified framework integrating graph topology and node features,based on which we conduct a detailed theoretical analysis of theireffectiveness. Specifically, the theoretical analyses elaborate the advantagesof Grug: 1) Decreasing sample variance during the training process (Stability);2) Enhancing the generalization of the model (Universality); 3) Reducing thecomplexity of the model (Simplicity); 4) Improving the integrity and diversityof graph information utilization (Diversity). As a result, Grug has thepotential to surpass the theoretical upper bounds set by DropMessage (AAAI-23Distinguished Papers). In addition, we evaluate Grug on five public real-worlddatasets with two downstream tasks.", "output": "Unifying gradient regularization for Heterogeneous Graph Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion-based generative models have exhibited powerful generativeperformance in recent years. However, as many attributes exist in the datadistribution and owing to several limitations of sharing the model parametersacross all levels of the generation process, it remains challenging to controlspecific styles for each attribute. To address the above problem, this paperpresents decoupled denoising diffusion models (DDDMs) with disentangledrepresentations, which can control the style for each attribute in generativemodels. We apply DDDMs to voice conversion (VC) tasks to address the challengesof disentangling and controlling each speech attribute (e.g., linguisticinformation, intonation, and timbre). First, we use a self-supervisedrepresentation to disentangle the speech representation. Subsequently, theDDDMs are applied to resynthesize the speech from the disentangledrepresentations for denoising with respect to each attribute. Moreover, we alsopropose the prior mixup for robust voice style transfer, which uses theconverted representation of the mixed style as a prior distribution for thediffusion models. The experimental results reveal that our method outperformspublicly available VC models. Furthermore, we show that our method providesrobust generative performance regardless of the model size. Audio samples areavailable ", "output": "DDDM-VC: Decoupled Denoising Diffusion Models with Disentangled Representation and Prior Mixup for Verified Robust Voice Conversion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The generalization of neural networks is a central challenge in machinelearning, especially concerning the performance under distributions that differfrom training ones. Current methods, mainly based on the data-driven paradigmsuch as data augmentation, adversarial training, and noise injection, mayencounter limited generalization due to model non-smoothness. In this paper, wepropose to investigate generalization from a Partial Differential Equation(PDE) perspective, aiming to enhance it directly through the underlyingfunction of neural networks, rather than focusing on adjusting input data.Specifically, we first establish the connection between neural networkgeneralization and the smoothness of the solution to a specific PDE, namely``transport equation''. Building upon this, we propose a general framework thatintroduces adaptive distributional diffusion into transport equation to enhancethe smoothness of its solution, thereby improving generalization. In thecontext of neural networks, we put this theoretical framework into practice asPDE+ (textbf{PDE} with textbf{A}daptive textbf{D}istributionaltextbf{D}iffusion) which diffuses each sample into a distribution coveringsemantically similar inputs. This enables better coverage of potentiallyunobserved distributions in training, thus improving generalization beyondmerely data-driven methods. The effectiveness of PDE+ is validated in extensivesettings, including clean samples and various corruptions, demonstrating itssuperior performance compared to SOTA methods.", "output": "PDE+: Enhancing Generalization via PDE with Adaptive Distributional Diffusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Architectures that first convert point clouds to a grid representation andthen apply convolutional neural networks achieve good performance forradar-based object detection. However, the transfer from irregular point clouddata to a dense grid structure is often associated with a loss of information,due to the discretization and aggregation of points. In this paper, we proposea novel architecture, multi-scale KPPillarsBEV, that aims to mitigate thenegative effects of grid rendering. Specifically, we propose a novel gridrendering method, KPBEV, which leverages the descriptive power of kernel pointconvolutions to improve the encoding of local point cloud contexts during gridrendering. In addition, we propose a general multi-scale grid renderingformulation to incorporate multi-scale feature maps into convolutionalbackbones of detection networks with arbitrary grid rendering methods. Weperform extensive experiments on the nuScenes dataset and evaluate the methodsin terms of detection performance and computational complexity. The proposedmulti-scale KPPillarsBEV architecture outperforms the baseline by 5.37% and theprevious state of the art by 2.88% in Car AP4.0 (average precision for amatching threshold of 4 meters) on the nuScenes validation set. Moreover, theproposed single-scale KPBEV grid rendering improves the Car AP4.0 by 2.90% overthe baseline while maintaining the same inference speed.", "output": "Improved Multi-Scale Grid Rendering of Point Clouds for Radar Object Detection Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (large LMs) are susceptible to producing text withhallucinated content. Self-contradiction, where the LM generates twocontradictory sentences within the same context, is an important form ofhallucination. In this work, we present a comprehensive analysis onself-contradiction for state-of-the-art, instruction-tuned LMs, includingevaluation, detection, and mitigation. To effectively triggerself-contradictions, we design a framework that constrains LMs to generateappropriate sentence pairs. Our evaluation on these sentence pairs reveals thatself-contradictions occur frequently across different LMs for both famous andlesser-known topics. Next, we prompt the LMs to detect self-contradictions. Ourresults indicate that ChatGPT and GPT-4 are able to accurately identifyself-contradictions, while Vicuna-13B struggles to do so. For example, with ourbest prompting method, ChatGPT achieves 91.0% precision and 80.5% recall on thesentence pairs generated by itself. To automatically mitigateself-contradictions, we develop an iterative algorithm that prompts the LMs toremove the detected self-contradictions from the generated text. Our algorithmsuccessfully revises the text such that self-contradictions are significantlyreduced, while maintaining its fluency and informativeness. Importantly, ourentire pipeline of triggering, detecting, and mitigating self-contradictions isapplicable to black-box LMs and does not require any external groundedknowledge.", "output": "Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Several explanation methods such as Integrated Gradients (IG) can becharacterised as path-based methods, as they rely on a straight line betweenthe data and an uninformative baseline. However, when applied to languagemodels, these methods produce a path for each word of a sentencesimultaneously, which could lead to creating sentences from interpolated wordseither having no clear meaning, or having a significantly different meaningcompared to the original sentence. In order to keep the meaning of thesesentences as close as possible to the original one, we propose SequentialIntegrated Gradients (SIG), which computes the importance of each word in asentence by keeping fixed every other words, only creating interpolationsbetween the baseline and the word of interest. Moreover, inspired by thetraining procedure of several language models, we also propose to replace thebaseline token \"pad\" with the trained token \"mask\". While being a simpleimprovement over the original IG method, we show on various models and datasetsthat SIG proves to be a very effective method for explaining language models.", "output": "Sequential Integrated Gradients: a simple but effective method for explaining language models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Extracting dense representations for terms and phrases is a task of greatimportance for knowledge discovery platforms targeting highly-technical fields.Dense representations are used as features for downstream components and havemultiple applications ranging from ranking results in search to summarization.Common approaches to create dense representations include trainingdomain-specific embeddings with self-supervised setups or using sentenceencoder models trained over similarity tasks. In contrast to static embeddings,sentence encoders do not suffer from the out-of-vocabulary (OOV) problem, butimpose significant computational costs. In this paper, we propose a fullyunsupervised approach to text encoding that consists of training smallcharacter-based models with the objective of reconstructing large pre-trainedembedding matrices. Models trained with this approach can not only match thequality of sentence encoders in technical domains, but are 5 times smaller andup to 10 times faster, even on high-end GPUs.", "output": "Extracting Text Representations for Terms and Phrases in Technical Domains."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semi-supervised learning has been an important approach to address challengesin extracting entities and relations from limited data. However, currentsemi-supervised works handle the two tasks (i.e., Named Entity Recognition andRelation Extraction) separately and ignore the cross-correlation of entity andrelation instances as well as the existence of similar instances acrossunlabeled data. To alleviate the issues, we propose Jointprop, a HeterogeneousGraph-based Propagation framework for joint semi-supervised entity and relationextraction, which captures the global structure information between individualtasks and exploits interactions within unlabeled data. Specifically, weconstruct a unified span-based heterogeneous graph from entity and relationcandidates and propagate class labels based on confidence scores. We thenemploy a propagation learning scheme to leverage the affinities betweenlabelled and unlabeled samples. Experiments on benchmark datasets show that ourframework outperforms the state-of-the-art semi-supervised approaches on NERand RE tasks. We show that the joint semi-supervised learning of the two tasksbenefits from their codependency and validates the importance of utilizing theshared information between unlabeled data.", "output": "Jointprop: Joint Semi-supervised Learning for Entity and Relation Extraction with Heterogeneous Graph-based Propagation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate the phenomenon of an LLM's untruthful response using a largeset of 220 handcrafted linguistic features. We focus on GPT-3 models and findthat the linguistic profiles of responses are similar across model sizes. Thatis, how varying-sized LLMs respond to given prompts stays similar on thelinguistic properties level. We expand upon this finding by training supportvector machines that rely only upon the stylistic components of model responsesto classify the truthfulness of statements. Though the dataset size limits ourcurrent findings, we present promising evidence that truthfulness detection ispossible without evaluating the content itself.", "output": "Linguistic Properties of Truthful Response."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Off-policy learning (OPL) aims at finding improved policies from loggedbandit data, often by minimizing the inverse propensity scoring (IPS) estimatorof the risk. In this work, we investigate a smooth regularization for IPS, forwhich we derive a two-sided PAC-Bayes generalization bound. The bound istractable, scalable, interpretable and provides learning certificates. Inparticular, it is also valid for standard IPS without making the assumptionthat the importance weights are bounded. We demonstrate the relevance of ourapproach and its favorable performance through a set of learning tasks. Sinceour bound holds for standard IPS, we are able to provide insight into whenregularizing IPS is useful. Namely, we identify cases where regularizationmight not be needed. This goes against the belief that, in practice, clippedIPS often enjoys favorable performance than standard IPS in OPL.", "output": "Exponential Smoothing for Off-Policy Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Efficient utilisation of both intra- and extra-textual context remains one ofthe critical gaps between machine and human translation. Existing research hasprimarily focused on providing individual, well-defined types of context intranslation, such as the surrounding text or discrete external variables likethe speaker's gender. This work introduces MTCue, a novel neural machinetranslation (NMT) framework that interprets all context (including discretevariables) as text. MTCue learns an abstract representation of context,enabling transferability across different data settings and leveraging similarattributes in low-resource scenarios. With a focus on a dialogue domain withaccess to document and metadata context, we extensively evaluate MTCue in fourlanguage pairs in both translation directions. Our framework demonstratessignificant improvements in translation quality over a parameter-matchednon-contextual baseline, as measured by BLEU (+0.88) and Comet (+1.58).Moreover, MTCue significantly outperforms a \"tagging\" baseline at translatingEnglish text. Analysis reveals that the context encoder of MTCue learns arepresentation space that organises context based on specific attributes, suchas formality, enabling effective zero-shot control. Pre-training on contextembeddings also improves MTCue's few-shot performance compared to the \"tagging\"baseline. Finally, an ablation study conducted on model components andcontextual variables further supports the robustness of MTCue for context-basedNMT.", "output": "MTCue: Learning Zero-Shot Control of Extra-Textual Attributes by Leveraging Unstructured Context in Neural Machine Translation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Partially ordered models of time occur naturally in applications where agentsor processes cannot perfectly communicate with each other, and can be tracedback to the seminal work of Lamport. In this paper we consider the problem ofdeciding if a (likely incomplete) description of a system of events isconsistent, the network consistency problem for the point algebra of partiallyordered time (POT). While the classical complexity of this problem has beenfully settled, comparably little is known of the fine-grained complexity of POTexcept that it can be solved in $O^*((0.368n)^n)$ time by enumerating orderedpartitions. We construct a much faster algorithm with a run-time bounded by$O^*((0.26n)^n)$. This is achieved by a sophisticated enumeration of structuressimilar to total orders, which are then greedily expanded toward a solution.While similar ideas have been explored earlier for related problems it turnsout that the analysis for POT is non-trivial and requires significant newideas.", "output": "A Fast Algorithm for Consistency Checking Partially Ordered Time."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a novel approach to logic-based learning which generatesassumption-based argumentation (ABA) frameworks from positive and negativeexamples, using a given background knowledge. These ABA frameworks can bemapped onto logic programs with negation as failure that may be non-stratified.Whereas existing argumentation-based methods learn exceptions to general rulesby interpreting the exceptions as rebuttal attacks, our approach interpretsthem as undercutting attacks. Our learning technique is based on the use oftransformation rules, including some adapted from logic program transformationrules (notably folding) as well as others, such as rote learning and assumptionintroduction. We present a general strategy that applies the transformationrules in a suitable order to learn stratified frameworks, and we also propose avariant that handles the non-stratified case. We illustrate the benefits of ourapproach with a number of examples, which show that, on one hand, we are ableto easily reconstruct other logic-based learning approaches and, on the otherhand, we can work out in a very simple and natural way problems that seem to behard for existing techniques.", "output": "Learning Assumption-based Argumentation Frameworks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The comprehension and adoption of Artificial Intelligence (AI) are beset withpractical and ethical problems. This article presents a 5-level AI CapabilityAssessment Model (AI-CAM) and a related AI Capabilities Matrix (AI-CM) toassist practitioners in AI comprehension and adoption. These practical toolswere developed with business executives, technologists, and otherorganisational stakeholders in mind. They are founded on a comprehensiveconception of AI compared to those in other AI adoption models and are alsoopen-source artefacts. Thus, the AI-CAM and AI-CM present an accessibleresource to help inform organisational decision-makers on the capabilityrequirements for (1) AI-based data analytics use cases based on machinelearning technologies; (2) Knowledge representation to engineer and representdata, information and knowledge using semantic technologies; and (3) AI-basedsolutions that seek to emulate human reasoning and decision-making. The AI-CAMcovers the core capability dimensions (business, data, technology,organisation, AI skills, risks, and ethical considerations) required at thefive capability maturity levels to achieve optimal use of AI in organisations.", "output": "Towards a Capability Assessment Model for the Comprehension and Adoption of AI in Organisations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Rotary Indexing Machines (RIMs) are widely used in manufacturing due to theirability to perform multiple production steps on a single product without manualrepositioning, reducing production time and improving accuracy and consistency.Despite their advantages, little research has been done on diagnosing faults inRIMs, especially from the perspective of the actual production steps carriedout on these machines. Long downtimes due to failures are problematic,especially for smaller companies employing these machines. To address this gap,we propose a diagnosis algorithm based on the product perspective, whichfocuses on the product being processed by RIMs. The algorithm traces the stepsthat a product takes through the machine and is able to diagnose possiblecauses in case of failure. We also analyze the properties of RIMs and how theseinfluence the diagnosis of faults in these machines. Our contributions arethree-fold. Firstly, we provide an analysis of the properties of RIMs and howthey influence the diagnosis of faults in these machines. Secondly, we suggesta diagnosis algorithm based on the product perspective capable of diagnosingfaults in such a machine. Finally, we test this algorithm on a model of arotary indexing machine, demonstrating its effectiveness in identifying faultsand their root causes.", "output": "A Diagnosis Algorithms for a Rotary Indexing Machine."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a novel perspective and algorithm for learning directed acyclicgraphs (DAGs) from data generated by a linear structural equation model (SEM).First, we show that a linear SEM can be viewed as a linear transform that, inprior work, computes the data from a dense input vector of random valued rootcauses (as we will call them) associated with the nodes. Instead, we considerthe case of (approximately) few root causes and also introduce noise in themeasurement of the data. Intuitively, this means that the DAG data is producedby few data-generating events whose effect percolates through the DAG. We proveidentifiability in this new setting and show that the true DAG is the globalminimizer of the $L^0$-norm of the vector of root causes. For data with fewroot causes, with and without noise, we show superior performance compared toprior DAG learning methods.", "output": "Learning DAGs from Data with Few Root Causes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Some of the most successful knowledge graph embedding (KGE) models for linkprediction -- CP, RESCAL, TuckER, ComplEx -- can be interpreted as energy-basedmodels. Under this perspective they are not amenable for exactmaximum-likelihood estimation (MLE), sampling and struggle to integrate logicalconstraints. This work re-interprets the score functions of these KGEs ascircuits -- constrained computational graphs allowing efficientmarginalisation. Then, we design two recipes to obtain efficient generativecircuit models by either restricting their activations to be non-negative orsquaring their outputs. Our interpretation comes with little or no loss ofperformance for link prediction, while the circuits framework unlocks exactlearning by MLE, efficient sampling of new triples, and guarantee that logicalconstraints are satisfied by design. Furthermore, our models scale moregracefully than the original KGEs on graphs with millions of entities.", "output": "How to Turn Your Knowledge Graph Embeddings into Generative Models via Probabilistic Circuits."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Biological nervous systems consist of networks of diverse, sophisticatedinformation processors in the form of neurons of different classes. In mostartificial neural networks (ANNs), neural computation is abstracted to anactivation function that is usually shared between all neurons within a layeror even the whole network; training of ANNs focuses on synaptic optimization.In this paper, we propose the optimization of neuro-centric parameters toattain a set of diverse neurons that can perform complex computations.Demonstrating the promise of the approach, we show that evolving neuralparameters alone allows agents to solve various reinforcement learning taskswithout optimizing any synaptic weights. While not aiming to be an accuratebiological model, parameterizing neurons to a larger degree than the currentcommon practice, allows us to ask questions about the computational abilitiesafforded by neural diversity in random neural networks. The presented resultsopen up interesting future research directions, such as combining evolvedneural diversity with activity-dependent plasticity.", "output": "Learning to Act through Evolution of Neural Diversity in Random Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Allen's interval algebra is one of the most well-known calculi in qualitativetemporal reasoning with numerous applications in artificial intelligence.Recently, there has been a surge of improvements in the fine-grained complexityof NP-hard reasoning tasks, improving the running time from the naive$2^{O(n^2)}$ to $O^*((1.0615n)^{n})$, with even faster algorithms for unitintervals a bounded number of overlapping intervals (the $O^*(cdot)$ notationsuppresses polynomial factors). Despite these improvements the best known lowerbound is still only $2^{o(n)}$ (under the exponential-time hypothesis) andmajor improvements in either direction seemingly require fundamental advancesin computational complexity. In this paper we propose a novel framework forsolving NP-hard qualitative reasoning problems which we refer to as dynamicprogramming with sublinear partitioning. Using this technique we obtain a majorimprovement of $O^*((frac{cn}{log{n}})^{n})$ for Allen's interval algebra. Todemonstrate that the technique is applicable to more domains we apply it to aproblem in qualitative spatial reasoning, the cardinal direction point algebra,and solve it in $O^*((frac{cn}{log{n}})^{2n/3})$ time. Hence, not only do wesignificantly advance the state-of-the-art for NP-hard qualitative reasoningproblems, but obtain a novel algorithmic technique that is likely applicable tomany problems where $2^{O(n)}$ time algorithms are unlikely.", "output": "Improved Algorithms for Allen's Interval Algebra by Dynamic Programming with Sublinear Partitioning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the increasing relevance of explainable AI, assessing the quality ofexplanations remains a challenging issue. Due to the high costs associated withhuman-subject experiments, various proxy metrics are often used toapproximately quantify explanation quality. Generally, one possibleinterpretation of the quality of an explanation is its inherent value forteaching a related concept to a student. In this work, we extend artificialsimulatability studies to the domain of graph neural networks. Instead ofcostly human trials, we use explanation-supervisable graph neural networks toperform simulatability studies to quantify the inherent usefulness ofattributional graph explanations. We perform an extensive ablation study toinvestigate the conditions under which the proposed analyses are mostmeaningful. We additionally validate our methods applicability on real-worldgraph classification and regression datasets. We find that relevantexplanations can significantly boost the sample efficiency of graph neuralnetworks and analyze the robustness towards noise and bias in the explanations.We believe that the notion of usefulness obtained from our proposedsimulatability analysis provides a dimension of explanation quality that islargely orthogonal to the common practice of faithfulness and has greatpotential to expand the toolbox of explanation quality assessments,specifically for graph explanations.", "output": "Quantifying the Intrinsic Usefulness of Attributional Explanations for Graph Neural Networks with Artificial Simulatability Studies."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine-learned systems are in widespread use for making decisions abouthumans, and it is important that they are fair, i.e., not biased againstindividuals based on sensitive attributes. We present runtime verification ofalgorithmic fairness for systems whose models are unknown, but are assumed tohave a Markov chain structure. We introduce a specification language that canmodel many common algorithmic fairness properties, such as demographic parity,equal opportunity, and social burden. We build monitors that observe a longsequence of events as generated by a given system, and output, after eachobservation, a quantitative estimate of how fair or biased the system was onthat run until that point in time. The estimate is proven to be correct moduloa variable error bound and a given confidence level, where the error bound getstighter as the observed sequence gets longer. Our monitors are of two types,and use, respectively, frequentist and Bayesian statistical inferencetechniques. While the frequentist monitors compute estimates that areobjectively correct with respect to the ground truth, the Bayesian monitorscompute estimates that are correct subject to a given prior belief about thesystem's model. Using a prototype implementation, we show how we can monitor ifa bank is fair in giving loans to applicants from different social backgrounds,and if a college is fair in admitting students while maintaining a reasonablefinancial burden on the society. Although they exhibit different theoreticalcomplexities in certain cases, in our experiments, both frequentist andBayesian monitors took less than a millisecond to update their verdicts aftereach observation.", "output": "Monitoring Algorithmic Fairness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large pre-trained language models based on transformer architecture havedrastically changed the natural language processing (NLP) landscape. However,deploying those models for on-device applications in constrained devices suchas smart watches is completely impractical due to their size and inferencecost. As an alternative to transformer-based architectures, recent work onefficient NLP has shown that weight-efficient models can attain competitiveperformance for simple tasks, such as slot filling and intent classification,with model sizes in the order of the megabyte. This work introduces thepNLP-Mixer architecture, an embedding-free MLP-Mixer model for on-device NLPthat achieves high weight-efficiency thanks to a novel projection layer. Weevaluate a pNLP-Mixer model of only one megabyte in size on two multi-lingualsemantic parsing datasets, MTOP and multiATIS. Our quantized model achieves99.4% and 97.8% the performance of mBERT on MTOP and multi-ATIS, while using170x fewer parameters. Our model consistently beats the state-of-the-art oftiny models (pQRNN), which is twice as large, by a margin up to 7.8% on MTOP.", "output": "pNLP-Mixer: an Efficient all-MLP Architecture for Language."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transformer-based architectures are the model of choice for natural languageunderstanding, but they come at a significant cost, as they have quadraticcomplexity in the input length, require a lot of training data, and can bedifficult to tune. In the pursuit of lower costs, we investigate simpleMLP-based architectures. We find that existing architectures such as MLPMixer,which achieves token mixing through a static MLP applied to each featureindependently, are too detached from the inductive biases required for naturallanguage understanding. In this paper, we propose a simple variant, HyperMixer,which forms the token mixing MLP dynamically using hypernetworks. Empirically,we demonstrate that our model performs better than alternative MLP-basedmodels, and on par with Transformers. In contrast to Transformers, HyperMixerachieves these results at substantially lower costs in terms of processingtime, training data, and hyperparameter tuning.", "output": "HyperMixer: An MLP-based Low Cost Alternative to Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent progress in representation and contrastive learning in NLP has notwidely considered the class of textit{sociopragmatic meaning} (i.e., meaningin interaction within different language communities). To bridge this gap, wepropose a novel framework for learning task-agnostic representationstransferable to a wide range of sociopragmatic tasks (e.g., emotion, hatespeech, humor, sarcasm). Our framework outperforms other contrastive learningframeworks for both in-domain and out-of-domain data, across both the generaland few-shot settings. For example, compared to two popular pre-trainedlanguage models, our method obtains an improvement of $11.66$ average $F_1$ on$16$ datasets when fine-tuned on only $20$ training samples per dataset.Ourcode is available at: ", "output": "Contrastive Learning of Sociopragmatic Meaning in Social Media."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Navigating to destinations using human speech instructions is essential forautonomous mobile robots operating in the real world. Although robots can takedifferent paths toward the same goal, the shortest path is not always optimal.A desired approach is to flexibly accommodate waypoint specifications, planninga better alternative path, even with detours. Furthermore, robots requirereal-time inference capabilities. Spatial representations include semantic,topological, and metric levels, each capturing different aspects of theenvironment. This study aims to realize a hierarchical spatial representationby a topometric semantic map and path planning with speech instructions,including waypoints. We propose SpCoTMHP, a hierarchical path-planning methodthat utilizes multimodal spatial concepts, incorporating place connectivity.This approach provides a novel integrated probabilistic generative model andfast approximate inference, with interaction among the hierarchy levels. Aformulation based on control as probabilistic inference theoretically supportsthe proposed path planning. Navigation experiments using speech instructionwith a waypoint demonstrated the performance improvement of path planning,WN-SPL by 0.589, and reduced computation time by 7.14 sec compared toconventional methods. Hierarchical spatial representations offer a mutuallyunderstandable form for humans and robots, enabling language-based navigationtasks.", "output": "Hierarchical Path-planning from Speech Instructions with Spatial Concept-based Topometric Semantic Mapping."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous exploration has many important applications. However, classicinformation gain-based or frontier-based exploration only relies on the robotcurrent state to determine the immediate exploration goal, which lacks thecapability of predicting the value of future states and thus leads toinefficient exploration decisions. This paper presents a method to learn how\"good\" states are, measured by the state value function, to provide a guidancefor robot exploration in real-world challenging environments. We formulate ourwork as an off-policy evaluation (OPE) problem for robot exploration (OPERE).It consists of offline Monte-Carlo training on real-world data and performsTemporal Difference (TD) online adaptation to optimize the trained valueestimator. We also design an intrinsic reward function based on sensorinformation coverage to enable the robot to gain more information with sparseextrinsic rewards. Results show that our method enables the robot to predictthe value of future states so as to better guide robot exploration. Theproposed algorithm achieves better prediction and exploration performancecompared with the state-of-the-arts. To the best of our knowledge, this workfor the first time demonstrates value function prediction on real-world datasetfor robot exploration in challenging subterranean and urban environments. Moredetails and demo videos can be found at ", "output": "Off-Policy Evaluation with Online Adaptation for Robot Exploration in Challenging Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Interest in understanding and factorizing learned embedding spaces throughconceptual explanations is steadily growing. When no human concept labels areavailable, concept discovery methods search trained embedding spaces forinterpretable concepts like object shape or color that can be used to providepost-hoc explanations for decisions. Unlike previous work, we argue thatconcept discovery should be identifiable, meaning that a number of knownconcepts can be provably recovered to guarantee reliability of theexplanations. As a starting point, we explicitly make the connection betweenconcept discovery and classical methods like Principal Component Analysis andIndependent Component Analysis by showing that they can recover independentconcepts with non-Gaussian distributions. For dependent concepts, we proposetwo novel approaches that exploit functional compositionality properties ofimage-generating processes. Our provably identifiable concept discovery methodssubstantially outperform competitors on a battery of experiments includinghundreds of trained models and dependent concepts, where they exhibit up to 29% better alignment with the ground truth. Our results provide a rigorousfoundation for reliable concept discovery without human labels.", "output": "When are Post-hoc Conceptual Explanations Identifiable?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph Anomaly Detection (GAD) has recently become a hot research spot due toits practicability and theoretical value. Since GAD emphasizes the applicationand the rarity of anomalous samples, enriching the varieties of its datasets isfundamental work. Thus, this paper present DGraph, a real-world dynamic graphin the finance domain. DGraph overcomes many limitations of current GADdatasets. It contains about 3M nodes, 4M dynamic edges, and 1M ground-truthnodes. We provide a comprehensive observation of DGraph, revealing thatanomalous nodes and normal nodes generally have different structures, neighbordistribution, and temporal dynamics. Moreover, it suggests that unlabeled nodesare also essential for detecting fraudsters. Furthermore, we conduct extensiveexperiments on DGraph. Observation and experiments demonstrate that DGraph ispropulsive to advance GAD research and enable in-depth exploration of anomalousnodes.", "output": "DGraph: A Large-Scale Financial Dataset for Graph Anomaly Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing approaches for generating multitrack music with transformer modelshave been limited in terms of the number of instruments, the length of themusic segments and slow inference. This is partly due to the memoryrequirements of the lengthy input sequences necessitated by existingrepresentations. In this work, we propose a new multitrack music representationthat allows a diverse set of instruments while keeping a short sequence length.Our proposed Multitrack Music Transformer (MMT) achieves comparable performancewith state-of-the-art systems, landing in between two recently proposed modelsin a subjective listening test, while achieving substantial speedups and memoryreductions over both, making the method attractive for real time improvisationor near real time creative applications. Further, we propose a new measure foranalyzing musical self-attention and show that the trained model attends moreto notes that form a consonant interval with the current note and to notes thatare 4N beats away from the current step.", "output": "Multitrack Music Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Claim-Dissector: a novel latent variable model for fact-checkingand analysis, which given a claim and a set of retrieved evidences jointlylearns to identify: (i) the relevant evidences to the given claim, (ii) theveracity of the claim. We propose to disentangle the per-evidence relevanceprobability and its contribution to the final veracity probability in aninterpretable way -- the final veracity probability is proportional to a linearensemble of per-evidence relevance probabilities. In this way, the individualcontributions of evidences towards the final predicted probability can beidentified. In per-evidence relevance probability, our model can furtherdistinguish whether each relevant evidence is supporting (S) or refuting (R)the claim. This allows to quantify how much the S/R probability contributes tothe final verdict or to detect disagreeing evidence.Despite its interpretable nature, our system achieves results competitivewith state-of-the-art on the FEVER dataset, as compared to typical two-stagesystem pipelines, while using significantly fewer parameters. It also sets newstate-of-the-art on FAVIQ and RealFC datasets. Furthermore, our analysis showsthat our model can learn fine-grained relevance cues while using coarse-grainedsupervision, and we demonstrate it in 2 ways. (i) We show that our model canachieve competitive sentence recall while using only paragraph-level relevancesupervision. (ii) Traversing towards the finest granularity of relevance, weshow that our model is capable of identifying relevance at the token level. Todo this, we present a new benchmark TLR-FEVER focusing on token-levelinterpretability -- humans annotate tokens in relevant evidences theyconsidered essential when making their judgment. Then we measure how similarare these annotations to the tokens our model is focusing on.", "output": "Claim-Dissector: An Interpretable Fact-Checking System with Joint Re-ranking and Veracity Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Event extraction for the clinical domain is an under-explored research area.The lack of training data along with the high volume of domain-specificterminologies with vague entity boundaries makes the task especiallychallenging. In this paper, we introduce DICE, a robust and data-efficientgenerative model for clinical event extraction. DICE frames event extraction asa conditional generation problem and introduces a contrastive learningobjective to accurately decide the boundaries of biomedical mentions. DICE alsotrains an auxiliary mention identification task jointly with event extractiontasks to better identify entity mention boundaries, and further introducesspecial markers to incorporate identified entity mentions as trigger andargument candidates for their respective tasks. To benchmark clinical eventextraction, we compose MACCROBAT-EE, the first clinical event extractiondataset with argument annotation, based on an existing clinical informationextraction dataset MACCROBAT. Our experiments demonstrate state-of-the-artperformances of DICE for clinical and news domain event extraction, especiallyunder low data settings.", "output": "DICE: Data-Efficient Clinical Event Extraction with Generative Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Controlling artificial agents from visual sensory data is an arduous task.Reinforcement learning (RL) algorithms can succeed but require large amounts ofinteractions between the agent and the environment. To alleviate the issue,unsupervised RL proposes to employ self-supervised interaction and learning,for adapting faster to future tasks. Yet, as shown in the Unsupervised RLBenchmark (URLB; Laskin et al. 2021), whether current unsupervised strategiescan improve generalization capabilities is still unclear, especially in visualcontrol settings. In this work, we study the URLB and propose a new method tosolve it, using unsupervised model-based RL, for pre-training the agent, and atask-aware fine-tuning strategy combined with a new proposed hybrid planner,Dyna-MPC, to adapt the agent for downstream tasks. On URLB, our method obtains93.59% overall normalized performance, surpassing previous baselines by astaggering margin. The approach is empirically evaluated through a large-scaleempirical study, which we use to validate our design choices and analyze ourmodels. We also show robust performance on the Real-Word RL benchmark, hintingat resiliency to environment perturbations during adaptation. Project website:", "output": "Mastering the Unsupervised Reinforcement Learning Benchmark from Pixels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Adversarial training is widely acknowledged as the most effective defenseagainst adversarial attacks. However, it is also well established thatachieving both robustness and generalization in adversarially trained modelsinvolves a trade-off. The goal of this work is to provide an in depthcomparison of different approaches for adversarial training in language models.Specifically, we study the effect of pre-training data augmentation as well astraining time input perturbations vs. embedding space perturbations on therobustness and generalization of transformer-based language models. Ourfindings suggest that better robustness can be achieved by pre-training dataaugmentation or by training with input space perturbation. However, trainingwith embedding space perturbation significantly improves generalization. Alinguistic correlation analysis of neurons of the learned models reveals thatthe improved generalization is due to 'more specialized' neurons. To the bestof our knowledge, this is the first work to carry out a deep qualitativeanalysis of different methods of generating adversarial examples in adversarialtraining of language models.", "output": "Impact of Adversarial Training on Robustness and Generalizability of Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The spread of misinformation is a prominent problem in today's society, andmany researchers in academia and industry are trying to combat it. Due to thevast amount of misinformation that is created every day, it is unrealistic toleave this task to human fact-checkers. Data scientists and researchers havebeen working on automated misinformation detection for years, and it is still achallenging problem today. The goal of our research is to add a new level toautomated misinformation detection; classifying segments of text withpersuasive writing techniques in order to produce interpretable reasoning forwhy an article can be marked as misinformation. To accomplish this, we presenta novel annotation scheme containing many common persuasive writing tactics,along with a dataset with human annotations accordingly. For this task, we makeuse of a RoBERTa model for text classification, due to its high performance inNLP. We develop several language model-based baselines and present the resultsof our persuasive strategy label predictions as well as the improvements theseintermediate labels make in detecting misinformation and producinginterpretable results.", "output": "Using Persuasive Writing Strategies to Explain and Detect Health Misinformation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Just like weights, bias terms are the learnable parameters of many popularmachine learning models, including neural networks. Biases are believed toeffectively increase the representational power of neural networks to solve awide range of tasks in computer vision. However, we argue that if we considerthe intrinsic distribution of images in the input space as well as some desiredproperties a model should have from the first principles, biases can becompletely ignored in addressing many image-related tasks, such as imageclassification. Our observation indicates that zero-bias neural networks couldperform comparably to neural networks with bias at least on practical imageclassification tasks. In addition, we prove that zero-bias neural networkspossess a nice property called scalar (multiplication) invariance, which allowsthe prediction of neural networks remains the same when altering the contrastof the input image. We then extend scalar invariance to more general cases thatallow us to formally verify certain convex regions of the input space. Besidesthat, we show the fairness of zero-bias neural networks in predicting the zeroimage. In contrast to the state-of-art models which lean towards certainlabels, zero-bias neural networks have a uniform belief in all labels. Based onthose merits, we believe dropping bias terms can be considered as a prior indesigning neural network architecture for some CV tasks, which shares thespirit of adapting convolutions as the transnational invariance prior.", "output": "Scalar Invariant Networks with Zero Bias."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a multi-explanation graph attention network (MEGAN). Unlikeexisting graph explainability methods, our network can produce node and edgeattributional explanations along multiple channels, the number of which isindependent of task specifications. This proves crucial to improve theinterpretability of graph regression predictions, as explanations can be splitinto positive and negative evidence w.r.t to a reference value. Additionally,our attention-based network is fully differentiable and explanations canactively be trained in an explanation-supervised manner. We first validate ourmodel on a synthetic graph regression dataset with known ground-truthexplanations. Our network outperforms existing baseline explainability methodsfor the single- as well as the multi-explanation case, achieving near-perfectexplanation accuracy during explanation supervision. Finally, we demonstrateour model's capabilities on multiple real-world datasets. We find that ourmodel produces sparse high-fidelity explanations consistent with humanintuition about those tasks.", "output": "MEGAN: Multi-Explanation Graph Attention Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, RNN-Transducers have achieved remarkable results on variousautomatic speech recognition tasks. However, lattice-free sequencediscriminative training methods, which obtain superior performance in hybridmodels, are rarely investigated in RNN-Transducers. In this work, we proposethree lattice-free training objectives, namely lattice-free maximum mutualinformation, lattice-free segment-level minimum Bayes risk, and lattice-freeminimum Bayes risk, which are used for the final posterior output of thephoneme-based neural transducer with a limited context dependency. Compared tocriteria using N-best lists, lattice-free methods eliminate the decoding stepfor hypotheses generation during training, which leads to more efficienttraining. Experimental results show that lattice-free methods gain up to 6.5%relative improvement in word error rate compared to a sequence-levelcross-entropy trained model. Compared to the N-best-list based minimum Bayesrisk objectives, lattice-free methods gain 40% - 70% relative training timespeedup with a small degradation in performance.", "output": "Lattice-Free Sequence Discriminative Training for Phoneme-Based Neural Transducers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A significant drawback of eXplainable Artificial Intelligence (XAI)approaches is the assumption of feature independence. This paper focuses onintegrating causal knowledge in XAI methods to increase trust and help usersassess explanations' quality. We propose a novel extension to a widely usedlocal and model-agnostic explainer that explicitly encodes causal relationshipsin the data generated around the input instance to explain. Extensiveexperiments show that our method achieves superior performance comparing theinitial one for both the fidelity in mimicking the black-box and the stabilityof the explanations.", "output": "CALIME: Causality-Aware Local Interpretable Model-Agnostic Explanations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The BLOOM model is a large publicly available multilingual language model,but its pretraining was limited to 46 languages. To extend the benefits ofBLOOM to other languages without incurring prohibitively large costs, it isdesirable to adapt BLOOM to new languages not seen during pretraining. In thiswork, we apply existing language adaptation strategies to BLOOM and benchmarkits zero-shot prompting performance on eight new languages in aresource-constrained setting. We find language adaptation to be effective atimproving zero-shot performance in new languages. Surprisingly, we find thatadapter-based finetuning is more effective than continued pretraining for largemodels. In addition, we discover that prompting performance is notsignificantly affected by language specifics, such as the writing system. It isprimarily determined by the size of the language adaptation data. We also addnew languages to BLOOMZ, which is a multitask finetuned version of BLOOMcapable of following task instructions zero-shot. We find including a newlanguage in the multitask fine-tuning mixture to be the most effective methodto teach BLOOMZ a new language. We conclude that with sufficient training datalanguage adaptation can generalize well to diverse languages. Our code isavailable at ", "output": "BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "When people think of everyday things like an \"egg,\" they typically have amental image associated with it. This commonsense knowledge helps us understandhow these everyday things work and how to interact with them. For example, whensomeone tries to make a fried egg, they know that it has a shell and that itcan be cracked open to reveal the egg white and yolk inside. However, if asystem does not have a coherent picture of such everyday things, thinking thatthe egg yolk surrounds the shell, then it might have to resort to ridiculousapproaches such as trying to scrape the egg yolk off the shell into the pan. Dolanguage models have a coherent picture of such everyday things? To investigatethis, we propose a benchmark dataset consisting of 100 everyday things, theirparts, and the relationships between these parts. We observe thatstate-of-the-art pre-trained language models (LMs) like GPT-3 and Macaw havefragments of knowledge about these entities, but they fail to produceconsistent parts mental models. We propose a simple extension to these LMswhere we apply a constraint satisfaction layer on top of raw predictions fromLMs to produce more consistent and accurate parts mental models of everydaythings.", "output": "Do language models have coherent mental models of everyday things?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graphs have a superior ability to represent relational data, like chemicalcompounds, proteins, and social networks. Hence, graph-level learning, whichtakes a set of graphs as input, has been applied to many tasks includingcomparison, regression, classification, and more. Traditional approaches tolearning a set of graphs heavily rely on hand-crafted features, such assubstructures. But while these methods benefit from good interpretability, theyoften suffer from computational bottlenecks as they cannot skirt the graphisomorphism problem. Conversely, deep learning has helped graph-level learningadapt to the growing scale of graphs by extracting features automatically andencoding graphs into low-dimensional representations. As a result, these deepgraph learning methods have been responsible for many successes. Yet, there isno comprehensive survey that reviews graph-level learning starting withtraditional learning and moving through to the deep learning approaches. Thisarticle fills this gap and frames the representative algorithms into asystematic taxonomy covering traditional learning, graph-level deep neuralnetworks, graph-level graph neural networks, and graph pooling. To ensure athoroughly comprehensive survey, the evolutions, interactions, andcommunications between methods from four different branches of development arealso examined. This is followed by a brief review of the benchmark data sets,evaluation metrics, and common downstream applications. The survey concludeswith a broad overview of 12 current and future directions in this boomingfield.", "output": "State of the Art and Potentialities of Graph-level Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While a vast collection of explainable AI (XAI) algorithms have beendeveloped in recent years, they are often criticized for significant gaps withhow humans produce and consume explanations. As a result, current XAItechniques are often found to be hard to use and lack effectiveness. In thiswork, we attempt to close these gaps by making AI explanations selective -- afundamental property of human explanations -- by selectively presenting asubset from a large set of model reasons based on what aligns with therecipient's preferences. We propose a general framework for generatingselective explanations by leveraging human input on a small sample. Thisframework opens up a rich design space that accounts for different selectivitygoals, types of input, and more. As a showcase, we use a decision-support taskto explore selective explanations based on what the decision-maker wouldconsider relevant to the decision task. We conducted two experimental studiesto examine three out of a broader possible set of paradigms based on ourproposed framework: in Study 1, we ask the participants to provide their owninput to generate selective explanations, with either open-ended orcritique-based input. In Study 2, we show participants selective explanationsbased on input from a panel of similar users (annotators). Our experimentsdemonstrate the promise of selective explanations in reducing over-reliance onAI and improving decision outcomes and subjective perceptions of the AI, butalso paint a nuanced picture that attributes some of these positive effects tothe opportunity to provide one's own input to augment AI explanations. Overall,our work proposes a novel XAI framework inspired by human communicationbehaviors and demonstrates its potentials to encourage future work to betteralign AI explanations with human production and consumption of explanations.", "output": "Selective Explanations: Leveraging Human Input to Align Explainable AI."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent ODE/SDE-based generative models, such as diffusion models, rectifiedflows, and flow matching, define a generative process as a time reversal of afixed forward process. Even though these models show impressive performance onlarge-scale datasets, numerical simulation requires multiple evaluations of aneural network, leading to a slow sampling speed. We attribute the reason tothe high curvature of the learned generative trajectories, as it is directlyrelated to the truncation error of a numerical solver. Based on therelationship between the forward process and the curvature, here we present anefficient method of training the forward process to minimize the curvature ofgenerative trajectories without any ODE/SDE simulation. Experiments show thatour method achieves a lower curvature than previous models and, therefore,decreased sampling costs while maintaining competitive performance. Code isavailable at ", "output": "Minimizing Trajectory Curvature of ODE-based Generative Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Universality is a key hypothesis in mechanistic interpretability -- thatdifferent models learn similar features and circuits when trained on similartasks. In this work, we study the universality hypothesis by examining howsmall neural networks learn to implement group composition. We present a novelalgorithm by which neural networks may implement composition for any finitegroup via mathematical representation theory. We then show that networksconsistently learn this algorithm by reverse engineering model logits andweights, and confirm our understanding using ablations. By studying networks ofdiffering architectures trained on various groups, we find mixed evidence foruniversality: using our algorithm, we can completely characterize the family ofcircuits and features that networks learn on this task, but for a given networkthe precise circuits learned -- as well as the order they develop -- arearbitrary.", "output": "A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Memory-based meta-learning is a technique for approximating Bayes-optimalpredictors. Under fairly general conditions, minimizing sequential predictionerror, measured by the log loss, leads to implicit meta-learning. The goal ofthis work is to investigate how far this interpretation can be realized bycurrent sequence prediction models and training regimes. The focus is onpiecewise stationary sources with unobserved switching-points, which arguablycapture an important characteristic of natural language and action-observationsequences in partially observable environments. We show that various types ofmemory-based neural models, including Transformers, LSTMs, and RNNs can learnto accurately approximate known Bayes-optimal algorithms and behave as ifperforming Bayesian inference over the latent switching-points and the latentparameters governing the data distribution within each segment.", "output": "Memory-Based Meta-Learning on Non-Stationary Distributions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transfer learning with a small amount of target data is an effective andcommon approach to adapting a pre-trained model to distribution shifts. In somesituations, target data labels may be expensive to obtain, so we may only haveaccess to a limited number of target data points. To make the most of a verysmall target dataset, we propose a lightweight, sample-efficient approach thatlearns a diverse set of features and adapts to a target distribution byinterpolating these features. Our approach, Project and Probe (Pro$^2$), firstlearns a linear projection that maps a pre-trained embedding onto orthogonaldirections while being predictive of labels in the source dataset. The goal ofthis step is to learn a variety of predictive features, so that at least someof them remain useful after distribution shift. Pro$^2$ then learns a linearclassifier on top of these projected features using a small target dataset.Theoretically, we find that Pro$^2$ results in more sample-efficientgeneralization by inducing a favorable bias-variance tradeoff. Our experimentson four datasets, with multiple distribution shift settings for each, show thatPro$^2$ improves performance by 5-15% when given limited target data comparedto prior methods such as standard linear probing.", "output": "Project and Probe: Sample-Efficient Domain Adaptation by Interpolating Orthogonal Features."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Although we have witnessed great success of pre-trained models in naturallanguage processing (NLP) and computer vision (CV), limited progress has beenmade for general time series analysis. Unlike NLP and CV where a unified modelcan be used to perform different tasks, specially designed approach stilldominates in each time series analysis task such as classification, anomalydetection, forecasting, and few-shot learning. The main challenge that blocksthe development of pre-trained model for time series analysis is the lack of alarge amount of data for training. In this work, we address this challenge byleveraging language or CV models, pre-trained from billions of tokens, for timeseries analysis. Specifically, we refrain from altering the self-attention andfeedforward layers of the residual blocks in the pre-trained language or imagemodel. This model, known as the Frozen Pretrained Transformer (FPT), isevaluated through fine-tuning on all major types of tasks involving timeseries. Our results demonstrate that pre-trained models on natural language orimages can lead to a comparable or state-of-the-art performance in all maintime series analysis tasks, as illustrated in Figure 1. We also found boththeoretically and empirically that the self-attention module behaviorssimilarly to principle component analysis (PCA), an observation that helpsexplains how transformer bridges the domain gap and a crucial step towardsunderstanding the universality of a pre-trained transformer.", "output": "One Fits All:Power General Time Series Analysis by Pretrained LM."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Energy markets can provide incentives for undesired behavior of marketparticipants. Multi-agent Reinforcement learning (MARL) is a promising newapproach to predicting the expected behavior of energy market participants.However, reinforcement learning requires many interactions with the system toconverge, and the power system environment often consists of extensivecomputations, e.g., optimal power flow (OPF) calculation for market clearing.To tackle this complexity, we provide a model of the energy market to a basicMARL algorithm in the form of a learned OPF approximation and explicit marketrules. The learned OPF surrogate model makes an explicit solving of the OPFcompletely unnecessary. Our experiments demonstrate that the model additionallyreduces training time by about one order of magnitude but at the cost of aslightly worse approximation of the Nash equilibrium. Potential applications ofour method are market design, more realistic modeling of market participants,and analysis of manipulative behavior.", "output": "Approximating Energy Market Clearing and Bidding With Model-Based Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Efficient packing of items into bins is a common daily task. Known as BinPacking Problem, it has been intensively studied in the field of artificialintelligence, thanks to the wide interest from industry and logistics. Sincedecades, many variants have been proposed, with the three-dimensional BinPacking Problem as the closest one to real-world use cases. We introduce ahybrid quantum-classical framework for solving real-world three-dimensional BinPacking Problems (Q4RealBPP), considering different realistic characteristics,such as: i) package and bin dimensions, ii) overweight restrictions, iii)affinities among item categories and iv) preferences for item ordering.Q4RealBPP permits the solving of real-world oriented instances of 3dBPP,contemplating restrictions well appreciated by industrial and logisticssectors.", "output": "Hybrid Approach for Solving Real-World Bin Packing Problem Instances Using Quantum Annealers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep Learning (DL) systems have proliferated in many applications, requiringspecialized hardware accelerators and chips. In the nano-era, devices havebecome increasingly more susceptible to permanent and transient faults.Therefore, we need an efficient methodology for analyzing the resilience ofadvanced DL systems against such faults, and understand how the faults inneural accelerator chips manifest as errors at the DL application level, wherefaults can lead to undetectable and unrecoverable errors. Using faultinjection, we can perform resilience investigations of the DL system bymodifying neuron weights and outputs at the software-level, as if the hardwarehad been affected by a transient fault. Existing fault models reduce the searchspace, allowing faster analysis, but requiring a-priori knowledge on the model,and not allowing further analysis of the filtered-out search space. Therefore,we propose ISimDL, a novel methodology that employs neuron sensitivity togenerate importance sampling-based fault-scenarios. Without any a-prioriknowledge of the model-under-test, ISimDL provides an equivalent reduction ofthe search space as existing works, while allowing long simulations to coverall the possible faults, improving on existing model requirements. Ourexperiments show that the importance sampling provides up to 15x higherprecision in selecting critical faults than the random uniform sampling,reaching such precision in less than 100 faults. Additionally, we showcaseanother practical use-case for importance sampling for reliable DNN design,namely Fault Aware Training (FAT). By using ISimDL to select the faults leadingto errors, we can insert the faults during the DNN training process to hardenthe DNN against such faults. Using importance sampling in FAT reduces theoverhead required for finding faults that lead to a predetermined drop inaccuracy by more than 12x.", "output": "ISimDL: Importance Sampling-Driven Acceleration of Fault Injection Simulations for Evaluating the Robustness of Deep Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Solving complicated AI tasks with different domains and modalities is a keystep toward artificial general intelligence. While there are abundant AI modelsavailable for different domains and modalities, they cannot handle complicatedAI tasks. Considering large language models (LLMs) have exhibited exceptionalability in language understanding, generation, interaction, and reasoning, weadvocate that LLMs could act as a controller to manage existing AI models tosolve complicated AI tasks and language could be a generic interface to empowerthis. Based on this philosophy, we present HuggingGPT, a framework thatleverages LLMs (e.g., ChatGPT) to connect various AI models in machine learningcommunities (e.g., Hugging Face) to solve AI tasks. Specifically, we useChatGPT to conduct task planning when receiving a user request, select modelsaccording to their function descriptions available in Hugging Face, executeeach subtask with the selected AI model, and summarize the response accordingto the execution results. By leveraging the strong language capability ofChatGPT and abundant AI models in Hugging Face, HuggingGPT is able to covernumerous sophisticated AI tasks in different modalities and domains and achieveimpressive results in language, vision, speech, and other challenging tasks,which paves a new way towards artificial general intelligence.", "output": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large Language Models (LLMs) have revolutionized natural language processingand demonstrated impressive capabilities in various tasks. Unfortunately, theyare prone to hallucinations, where the model exposes incorrect or falseinformation in its responses, which renders diligent evaluation approachesmandatory. While LLM performance in specific knowledge fields is oftenevaluated based on question and answer (Q&amp;A) datasets, such evaluations usuallyreport only a single accuracy number for the entire field, a procedure which isproblematic with respect to transparency and model improvement. A stratifiedevaluation could instead reveal subfields, where hallucinations are more likelyto occur and thus help to better assess LLMs' risks and guide their furtherdevelopment. To support such stratified evaluations, we propose LLMMaps as anovel visualization technique that enables users to evaluate LLMs' performancewith respect to Q&amp;A datasets. LLMMaps provide detailed insights into LLMs'knowledge capabilities in different subfields, by transforming Q&amp;A datasets aswell as LLM responses into our internal knowledge structure. An extension forcomparative visualization furthermore, allows for the detailed comparison ofmultiple LLMs. To assess LLMMaps we use them to conduct a comparative analysisof several state-of-the-art LLMs, such as BLOOM, GPT-2, GPT-3, ChatGPT andLLaMa-13B, as well as two qualitative user evaluations. All necessary sourcecode and data for generating LLMMaps to be used in scientific publications andelsewhere will be available on GitHub.", "output": "LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative foundation models are susceptible to implicit biases that canarise from extensive unsupervised training data. Such biases can producesuboptimal samples, skewed outcomes, and unfairness, with potentiallysignificant repercussions. Consequently, aligning these models with humanethics and preferences is an essential step toward ensuring their responsibleand effective deployment in real-world applications. Prior research hasprimarily employed Reinforcement Learning from Human Feedback (RLHF) as a meansof addressing this problem, wherein generative models are fine-tuned using RLalgorithms guided by a human-feedback-informed reward model. However, theinefficiencies and instabilities associated with RL algorithms frequentlypresent substantial obstacles to the successful alignment of generative models,necessitating the development of a more robust and streamlined approach. Tothis end, we introduce a new framework, Reward rAnked FineTuning (RAFT),designed to align generative models more effectively. Utilizing a reward modeland a sufficient number of samples, our approach selects the high-qualitysamples, discarding those that exhibit undesired behavior, and subsequentlyassembles a streaming dataset. This dataset serves as the basis for aligningthe generative model and can be employed under both offline and onlinesettings. Notably, the sample generation process within RAFT is gradient-free,rendering it compatible with black-box generators. Through extensiveexperiments, we demonstrate that our proposed algorithm exhibits strongperformance in the context of both large language models and diffusion models.", "output": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A key challenge for a reinforcement learning (RL) agent is to incorporateexternal/expert1 advice in its learning. The desired goals of an algorithm thatcan shape the learning of an RL agent with external advice include (a)maintaining policy invariance; (b) accelerating the learning of the agent; and(c) learning from arbitrary advice [3]. To address this challenge this paperformulates the problem of incorporating external advice in RL as a multi-armedbandit called shaping-bandits. The reward of each arm of shaping banditscorresponds to the return obtained by following the expert or by following adefault RL algorithm learning on the true environment reward.We show thatdirectly applying existing bandit and shaping algorithms that do not reasonabout the non-stationary nature of the underlying returns can lead to poorresults. Thus we propose UCB-PIES (UPIES), Racing-PIES (RPIES), and Lazy PIES(LPIES) three different shaping algorithms built on different assumptions thatreason about the long-term consequences of following the expert policy or thedefault RL algorithm. Our experiments in four different settings show thatthese proposed algorithms achieve the above-mentioned goals whereas the otheralgorithms fail to do so.", "output": "Bandit-Based Policy Invariant Explicit Shaping for Incorporating External Advice in Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantum walks (QWs) have a property that classical random walks (RWs) do notpossess -- the coexistence of linear spreading and localization -- and thisproperty is utilized to implement various kinds of applications. This paperproposes RW- and QW-based algorithms for multi-armed-bandit (MAB) problems. Weshow that, under some settings, the QW-based model realizes higher performancethan the corresponding RW-based one by associating the two operations that makeMAB problems difficult -- exploration and exploitation -- with these twobehaviors of QWs.", "output": "Bandit Algorithm Driven by a Classical Random Walk and a Quantum Walk."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Games have been the perfect test-beds for artificial intelligence researchfor the characteristics that widely exist in real-world scenarios. Learning andoptimisation, decision making in dynamic and uncertain environments, gametheory, planning and scheduling, design and education are common research areasshared between games and real-world problems. Numerous open-source games orgame-based environments have been implemented for studying artificialintelligence. In addition to single- or multi-player, collaborative oradversarial games, there has also been growing interest in implementingplatforms for creative design in recent years. Those platforms provide idealbenchmarks for exploring and comparing artificial intelligence ideas andtechniques. This paper reviews the game-based platforms for artificialintelligence research, discusses the research trend induced by the evolution ofthose platforms, and gives an outlook.", "output": "Game-based Platforms for Artificial Intelligence Research."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reasoning over knowledge graphs (KGs) is a challenging task that requires adeep understanding of the complex relationships between entities and theunderlying logic of their relations. Current approaches rely on learninggeometries to embed entities in vector space for logical query operations, butthey suffer from subpar performance on complex queries and dataset-specificrepresentations. In this paper, we propose a novel decoupled approach,Language-guided Abstract Reasoning over Knowledge graphs (LARK), thatformulates complex KG reasoning as a combination of contextual KG search andlogical query reasoning, to leverage the strengths of graph extractionalgorithms and large language models (LLM), respectively. Our experimentsdemonstrate that the proposed approach outperforms state-of-the-art KGreasoning methods on standard benchmark datasets across several logical queryconstructs, with significant performance gain for queries of higher complexity.Furthermore, we show that the performance of our approach improvesproportionally to the increase in size of the underlying LLM, enabling theintegration of the latest advancements in LLMs for logical reasoning over KGs.Our work presents a new direction for addressing the challenges of complex KGreasoning and paves the way for future research in this area.", "output": "Complex Logical Reasoning over Knowledge Graphs using Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Few-shot event detection (ED) has been widely studied, while this bringsnoticeable discrepancies, e.g., various motivations, tasks, and experimentalsettings, that hinder the understanding of models for future progress.Thispaper presents a thorough empirical study, a unified view of ED models, and abetter unified baseline. For fair evaluation, we compare 12 representativemethods on three datasets, which are roughly grouped into prompt-based andprototype-based models for detailed analysis. Experiments consistentlydemonstrate that prompt-based methods, including ChatGPT, still significantlytrail prototype-based methods in terms of overall performance. To investigatetheir superior performance, we break down their design elements along severaldimensions and build a unified framework on prototype-based methods. Under suchunified view, each prototype-method can be viewed a combination of differentmodules from these design elements. We further combine all advantageous modulesand propose a simple yet effective baseline, which outperforms existing methodsby a large margin (e.g., 2.7% F1 gains under low-resource setting).", "output": "Few-shot Event Detection: An Empirical Study and a Unified View."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As the field of automated machine learning (AutoML) advances, it becomesincreasingly important to incorporate domain knowledge into these systems. Wepresent an approach for doing so by harnessing the power of large languagemodels (LLMs). Specifically, we introduce Context-Aware Automated FeatureEngineering (CAAFE), a feature engineering method for tabular datasets thatutilizes an LLM to iteratively generate additional semantically meaningfulfeatures for tabular datasets based on the description of the dataset. Themethod produces both Python code for creating new features and explanations forthe utility of the generated features.Despite being methodologically simple, CAAFE improves performance on 11 outof 14 datasets - boosting mean ROC AUC performance from 0.798 to 0.822 acrossall dataset - similar to the improvement achieved by using a random forestinstead of logistic regression on our datasets.Furthermore, CAAFE is interpretable by providing a textual explanation foreach generated feature. CAAFE paves the way for more extensive semi-automationin data science tasks and emphasizes the significance of context-awaresolutions that can extend the scope of AutoML systems to semantic AutoML. Werelease our $href{ a simple$href{ a $href{ package}$.", "output": "LLMs for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Compositional generalization--understanding unseen combinations of seenprimitives--is an essential reasoning capability in human intelligence. The AIcommunity mainly studies this capability by fine-tuning neural networks on lotsof training samples, while it is still unclear whether and how in-contextlearning--the prevailing few-shot paradigm based on large languagemodels--exhibits compositional generalization. In this paper, we present CoFe,a test suite to investigate in-context compositional generalization. We findthat the compositional generalization performance can be easily affected by theselection of in-context examples, thus raising the research question what thekey factors are to make good in-context examples for compositionalgeneralization. We study three potential factors: similarity, diversity andcomplexity. Our systematic experiments indicate that in-context examples shouldbe structurally similar to the test case, diverse from each other, andindividually simple. Furthermore, two strong limitations are observed:in-context compositional generalization on fictional words is much weaker thanthat on commonly used ones; it is still critical that the in-context examplesshould cover required linguistic structures, even though the backbone model hasbeen pre-trained on large corpus. We hope our analysis would facilitate theunderstanding and utilization of in-context learning paradigm.", "output": "How Do In-Context Examples Affect Compositional Generalization?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, soft prompt learning methods have been proposed to fine-tunelarge-scale vision-language pre-trained models for various downstream tasks.These methods typically combine learnable textual tokens with class tokens asinput for models with frozen parameters. However, they often employ a singleprompt to describe class contexts, failing to capture categories' diverseattributes adequately. This study introduces the Partitioned Multi-modal Prompt(PMPO), a multi-modal prompting technique that extends the soft prompt from asingle learnable prompt to multiple prompts. Our method divides the visualencoder depths and connects learnable prompts to the separated visual depths,enabling different prompts to capture the hierarchical contextual depths ofvisual representations. Furthermore, to maximize the advantages of multi-promptlearning, we incorporate prior information from manually designed templates andlearnable multi-prompts, thus improving the generalization capabilities of ourapproach. We evaluate the effectiveness of our approach on three challengingtasks: new class generalization, cross-dataset evaluation, and domaingeneralization. For instance, our method achieves a $79.28$ harmonic mean,averaged over 11 diverse image recognition datasets ($+7.62$ compared to CoOp),demonstrating significant competitiveness compared to state-of-the-artprompting methods.", "output": "Multi-Prompt with Depth Partitioned Cross-Modal Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present the findings of SemEval-2023 Task 2 on Fine-grained MultilingualNamed Entity Recognition (MultiCoNER 2). Divided into 13 tracks, the taskfocused on methods to identify complex fine-grained named entities (likeWRITTENWORK, VEHICLE, MUSICALGRP) across 12 languages, in both monolingual andmultilingual scenarios, as well as noisy settings. The task used the MultiCoNERV2 dataset, composed of 2.2 million instances in Bangla, Chinese, English,Farsi, French, German, Hindi, Italian., Portuguese, Spanish, Swedish, andUkrainian. MultiCoNER 2 was one of the most popular tasks of SemEval-2023. Itattracted 842 submissions from 47 teams, and 34 teams submitted system papers.Results showed that complex entity types such as media titles and product nameswere the most challenging. Methods fusing external knowledge into transformermodels achieved the best performance, and the largest gains were on theCreative Work and Group classes, which are still challenging even with externalknowledge. Some fine-grained classes proved to be more challenging than others,such as SCIENTIST, ARTWORK, and PRIVATECORP. We also observed that noisy datahas a significant impact on model performance, with an average drop of 10% onthe noisy subset. The task highlights the need for future research on improvingNER robustness on noisy data containing complex entities.", "output": "SemEval-2023 Task 2: Fine-grained Multilingual Named Entity Recognition (MultiCoNER 2)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Language models (LMs) are powerful tools for natural language processing, butthey often struggle to produce coherent and fluent text when they are small.Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) canrarely generate coherent and consistent English text beyond a few words evenafter extensive training. This raises the question of whether the emergence ofthe ability to produce coherent English text only occurs at larger scales (withhundreds of millions of parameters or more) and complex architectures (withmany layers of global attention).In this work, we introduce TinyStories, a synthetic dataset of short storiesthat only contain words that a typical 3 to 4-year-olds usually understand,generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to trainand evaluate LMs that are much smaller than the state-of-the-art models (below10 million total parameters), or have much simpler architectures (with only onetransformer block), yet still produce fluent and consistent stories withseveral paragraphs that are diverse and have almost perfect grammar, anddemonstrate reasoning capabilities.We also introduce a new paradigm for the evaluation of language models: Wesuggest a framework which uses GPT-4 to grade the content generated by thesemodels as if those were stories written by students and graded by a (human)teacher. This new paradigm overcomes the flaws of standard benchmarks whichoften requires the model's output to be very structures, and moreover providesa multidimensional score for the model, providing scores for differentcapabilities such as grammar, creativity and consistency.We hope that TinyStories can facilitate the development, analysis andresearch of LMs, especially for low-resource or specialized domains, and shedlight on the emergence of language capabilities in LMs.", "output": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Chatbots based on Large Language Models (LLMs) have shown strong capabilitiesin language understanding. In this study, we explore the potential of LLMs inassisting corpus-based linguistic studies through automatic annotation of textswith specific categories of linguistic information. Specifically, we examinedto what extent LLMs understand the functional elements constituting the speechact of apology from a local grammar perspective, by comparing the performanceof ChatGPT (powered by GPT-3.5), the Bing chatbot (powered by GPT-4), and ahuman coder in the annotation task. The results demonstrate that the Bingchatbot significantly outperformed ChatGPT in the task. Compared to humanannotator, the overall performance of the Bing chatbot was slightly lesssatisfactory. However, it already achieved high F1 scores: 99.95% for the tagof APOLOGISING, 91.91% for REASON, 95.35% for APOLOGISER, 89.74% forAPOLOGISEE, and 96.47% for INTENSIFIER. This suggests that it is feasible touse LLM-assisted annotation for local grammar analysis, together with humanintervention on tags that are less accurately recognized by machine. Westrongly advocate conducting future studies to evaluate the performance of LLMsin annotating other linguistic phenomena. These studies have the potential tooffer valuable insights into the advancement of theories developed in corpuslinguistics, as well into the linguistic capabilities of LLMs..", "output": "Using LLM-assisted Annotation for Corpus Linguistics: A Case Study of Local Grammar Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present $text{sustain.AI}$, an intelligent, context-aware recommendersystem that assists auditors and financial investors as well as the generalpublic to efficiently analyze companies' sustainability reports. The toolleverages an end-to-end trainable architecture that couples a BERT-basedencoding module with a multi-label classification head to match relevant textpassages from sustainability reports to their respective law regulations fromthe Global Reporting Initiative (GRI) standards. We evaluate our model on twonovel German sustainability reporting data sets and consistently achieve asignificantly higher recommendation performance compared to multiple strongbaselines. Furthermore, $text{sustain.AI}$ is publicly available for everyoneat ", "output": "sustain.AI: a Recommender System to analyze Sustainability Reports."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Attempts to import dual-system descriptions of System-1 and System-2 into AIhave been hindered by a lack of clarity over their distinction. We address thisand other issues by situating System-1 and System-2 within the Common Model ofCognition. Results show that what are thought to be distinctive characteristicsof System-1 and 2 instead form a spectrum of cognitive properties. The CommonModel provides a comprehensive vision of the computational units involved inSystem-1 and System-2, their underlying mechanisms, and the implications forlearning, metacognition, and emotion.", "output": "AAAI 2022 Fall Symposium: System-1 and System-2 realized within the Common Model of Cognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent popularity of large language models (LLMs) has brought asignificant impact to boundless fields, particularly through their open-endedecosystem such as the APIs, open-sourced models, and plugins. However, withtheir widespread deployment, there is a general lack of research thatthoroughly discusses and analyzes the potential risks concealed. In that case,we intend to conduct a preliminary but pioneering study covering therobustness, consistency, and credibility of LLMs systems. With most of therelated literature in the era of LLM uncharted, we propose an automatedworkflow that copes with an upscaled number of queries/responses. Overall, weconduct over a million queries to the mainstream LLMs including ChatGPT, LLaMA,and OPT. Core to our workflow consists of a data primitive, followed by anautomated interpreter that evaluates these LLMs under different adversarialmetrical systems. As a result, we draw several, and perhaps unfortunate,conclusions that are quite uncommon from this trendy community. Briefly, theyare: (i)-the minor but inevitable error occurrence in the user-generated queryinput may, by chance, cause the LLM to respond unexpectedly; (ii)-LLMs possesspoor consistency when processing semantically similar query input. In addition,as a side finding, we find that ChatGPT is still capable to yield the correctanswer even when the input is polluted at an extreme level. While thisphenomenon demonstrates the powerful memorization of the LLMs, it raisesserious concerns about using such data for LLM-involved evaluation in academicdevelopment. To deal with it, we propose a novel index associated with adataset that roughly decides the feasibility of using such data forLLM-involved evaluation. Extensive empirical studies are tagged to support theaforementioned claims.", "output": "Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Sequential Recommender Systems (SRSs) are a popular type of recommendersystem that learns from a user's history to predict the next item they arelikely to interact with. However, user interactions can be affected by noisestemming from account sharing, inconsistent preferences, or accidental clicks.To address this issue, we (i) propose a new evaluation protocol that takesmultiple future items into account and (ii) introduce a novel relevance-awareloss function to train a SRS with multiple future items to make it more robustto noise. Our relevance-aware models obtain an improvement of ~1.2% of NDCG@10and 0.88% in the traditional evaluation protocol, while in the new evaluationprotocol, the improvement is ~1.63% of NDCG@10 and ~1.5% of HR w.r.t the bestperforming models.", "output": "Integrating Item Relevance in Training Loss for Sequential Recommender Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Achieving machine autonomy and human control often represent divergentobjectives in the design of interactive AI systems. Visual generativefoundation models such as Stable Diffusion show promise in navigating thesegoals, especially when prompted with arbitrary languages. However, they oftenfall short in generating images with spatial, structural, or geometriccontrols. The integration of such controls, which can accommodate variousvisual conditions in a single unified model, remains an unaddressed challenge.In response, we introduce UniControl, a new generative foundation model thatconsolidates a wide array of controllable condition-to-image (C2I) tasks withina singular framework, while still allowing for arbitrary language prompts.UniControl enables pixel-level-precise image generation, where visualconditions primarily influence the generated structures and language promptsguide the style and context. To equip UniControl with the capacity to handlediverse visual conditions, we augment pretrained text-to-image diffusion modelsand introduce a task-aware HyperNet to modulate the diffusion models, enablingthe adaptation to different C2I tasks simultaneously. Trained on nine uniqueC2I tasks, UniControl demonstrates impressive zero-shot generation abilitieswith unseen visual conditions. Experimental results show that UniControl oftensurpasses the performance of single-task-controlled methods of comparable modelsizes. This control versatility positions UniControl as a significantadvancement in the realm of controllable visual generation.", "output": "UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing solutions to zero-shot text classification either conduct promptingwith pre-trained language models, which is sensitive to the choices oftemplates, or rely on large-scale annotated data of relevant tasks formeta-tuning. In this work, we propose a new paradigm based on self-supervisedlearning to solve zero-shot text classification tasks by tuning the languagemodels with unlabeled data, called self-supervised tuning. By exploring theinherent structure of free texts, we propose a new learning objective calledfirst sentence prediction to bridge the gap between unlabeled data and textclassification tasks. After tuning the model to learn to predict the firstsentence in a paragraph based on the rest, the model is able to conductzero-shot inference on unseen tasks such as topic classification and sentimentanalysis. Experimental results show that our model outperforms thestate-of-the-art baselines on 7 out of 10 tasks. Moreover, the analysis revealsthat our model is less sensitive to the prompt design. Our code and pre-trainedmodels are publicly available at  .", "output": "Zero-Shot Text Classification via Self-Supervised Tuning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a lightweight system for stereo matching through embedded GPUs. Itbreaks the trade-off between accuracy and processing speed in stereo matching,enabling our embedded system to further improve the matching accuracy whileensuring real-time processing. The main idea of our method is to construct atiny neural network based on variational auto-encoder (VAE) to upsample andrefinement a small size of coarse disparity map, which is first generated by atraditional matching method. The proposed hybrid structure cannot only bringthe advantage of traditional methods in terms of computational complexity, butalso ensure the matching accuracy under the impact of neural network. Extensiveexperiments on the KITTI 2015 benchmark demonstrate that our tiny systemexhibits high robustness in improving the accuracy of the coarse disparity mapsgenerated by different algorithms, while also running in real-time on embeddedGPUs.", "output": "StereoVAE: A lightweight stereo matching system through embedded GPUs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The advent of automation in particular Software Engineering (SE) tasks hastransitioned from theory to reality. Numerous scholarly articles havedocumented the successful application of Artificial Intelligence to addressissues in areas such as project management, modeling, testing, and development.A recent innovation is the introduction of ChatGPT, an ML-infused chatbot,touted as a resource proficient in generating programming codes and formulatingsoftware testing strategies for developers and testers respectively. Althoughthere is speculation that AI-based computation can increase productivity andeven substitute software engineers in software development, there is currentlya lack of empirical evidence to verify this. Moreover, despite the primaryfocus on enhancing the accuracy of AI systems, non-functional requirementsincluding energy efficiency, vulnerability, fairness (i.e., human bias), andsafety frequently receive insufficient attention. This paper posits that acomprehensive comparison of software engineers and AI-based solutions,considering various evaluation criteria, is pivotal in fostering human-machinecollaboration, enhancing the reliability of AI-based methods, and understandingtask suitability for humans or AI. Furthermore, it facilitates the effectiveimplementation of cooperative work structures and human-in-the-loop processes.This paper conducts an empirical investigation, contrasting the performance ofsoftware engineers and AI systems, like ChatGPT, across different evaluationmetrics. The empirical study includes a case of assessing ChatGPT-generatedcode versus code produced by developers and uploaded in Leetcode.", "output": "Comparing Software Developers with ChatGPT: An Empirical Investigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents a dataset containing recordings of theelectroencephalogram (EEG) and the electromyogram (EMG) from eight subjects whowere assisted in moving their right arm by an active orthosis device. Thesupported movements were elbow joint movements, i.e., flexion and extension ofthe right arm. While the orthosis was actively moving the subject's arm, someerrors were deliberately introduced for a short duration of time. During thistime, the orthosis moved in the opposite direction. In this paper, we explainthe experimental setup and present some behavioral analyses across allsubjects. Additionally, we present an average event-related potential analysisfor one subject to offer insights into the data quality and the EEG activitycaused by the error introduction. The dataset described herein is openlyaccessible. The aim of this study was to provide a dataset to the researchcommunity, particularly for the development of new methods in the asynchronousdetection of erroneous events from the EEG. We are especially interested in thetactile and haptic-mediated recognition of errors, which has not yet beensufficiently investigated in the literature. We hope that the detaileddescription of the orthosis and the experiment will enable its reproduction andfacilitate a systematic investigation of the influencing factors in thedetection of erroneous behavior of assistive systems by a large community.", "output": "EEG and EMG dataset for the detection of errors introduced by an active orthosis device."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite substantial advancements, Natural Language Processing (NLP) modelsoften require post-training adjustments to enforce business rules, rectifyundesired behavior, and align with user values. These adjustments involveoperationalizing \"concepts\"--dictating desired model responses to certaininputs. However, it's difficult for a single entity to enumerate and define allpossible concepts, indicating a need for a multi-user, collaborative modelalignment framework. Moreover, the exhaustive delineation of a concept ischallenging, and an improper approach can create shortcuts or interfere withoriginal data or other concepts.To address these challenges, we introduce CoDev, a framework that enablesmulti-user interaction with the model, thereby mitigating individuallimitations. CoDev aids users in operationalizing their concepts using LargeLanguage Models, and relying on the principle that NLP models exhibit simplerbehaviors in local regions. Our main insight is learning a emph{local} modelfor each concept, and a emph{global} model to integrate the original data withall concepts. We then steer a large language model to generate instances withinconcept boundaries where local and global disagree. Our experiments show CoDevis effective at helping multiple users operationalize concepts and avoidinterference for a variety of scenarios, tasks, and models.", "output": "Collaborative Development of NLP models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To implement autonomous driving, one essential step is to model the vehicleenvironment based on the sensor inputs. Radars, with their well-knownadvantages, became a popular option to infer the occupancy state of grid cellssurrounding the vehicle. To tackle data sparsity and noise of radar detections,we propose a deep learning-based Inverse Sensor Model (ISM) to learn themapping from sparse radar detections to polar measurement grids. Improvedlidar-based measurement grids are used as reference. The learned radarmeasurement grids, combined with radar Doppler velocity measurements, arefurther used to generate a Dynamic Grid Map (DGM). Experiments in real-worldhighway scenarios show that our approach outperforms the hand-crafted geometricISMs. In comparison to state-of-the-art deep learning methods, our approach isthe first one to learn a single-frame measurement grid in the polar scheme fromradars with a limited Field Of View (FOV). The learning framework makes thelearned ISM independent of the radar mounting. This enables us to flexibly useone or more radar sensors without network retraining and without requirementson 360{deg} sensor coverage.", "output": "Deep Radar Inverse Sensor Models for Dynamic Occupancy Grid Maps."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper proposes a new framework to study multi-agent interaction inMarkov games: Markov $alpha$-potential games. Markov potential games arespecial cases of Markov $alpha$-potential games, so are two important andpractically significant classes of games: Markov congestion games and perturbedMarkov team games. In this paper, {$alpha$-potential} functions for both gamesare provided and the gap $alpha$ is characterized with respect to gameparameters. Two algorithms -- the projected gradient-ascent algorithm and thesequential maximum improvement smoothed best response dynamics -- areintroduced for approximating the stationary Nash equilibrium in Markov$alpha$-potential games. The Nash-regret for each algorithm is shown to scalesub-linearly in time horizon. Our analysis and numerical experimentsdemonstrates that simple algorithms are capable of finding approximateequilibrium in Markov $alpha$-potential games.", "output": "Markov $\\alpha$-Potential Games: Equilibrium Approximation and Regret Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present FIT: a transformer-based architecture with efficientself-attention and adaptive computation. Unlike original transformers, whichoperate on a single sequence of data tokens, we divide the data tokens intogroups, with each group being a shorter sequence of tokens. We employ two typesof transformer layers: local layers operate on data tokens within each group,while global layers operate on a smaller set of introduced latent tokens. Theselayers, comprising the same set of self-attention and feed-forward layers asstandard transformers, are interleaved, and cross-attention is used tofacilitate information exchange between data and latent tokens within the samegroup. The attention complexity is $O(n^2)$ locally within each group of size$n$, but can reach $O(L^{{4}/{3}})$ globally for sequence length of $L$. Theefficiency can be further enhanced by relying more on global layers thatperform adaptive computation using a smaller set of latent tokens. FIT is aversatile architecture and can function as an encoder, diffusion decoder, orautoregressive decoder. We provide initial evidence demonstrating itseffectiveness in high-resolution image understanding and generation tasks.Notably, FIT exhibits potential in performing end-to-end training ongigabit-scale data, such as 6400$times$6400 images, or 160K tokens (afterpatch tokenization), within a memory capacity of 16GB, without requiringspecific optimizations or model parallelism.", "output": "FIT: Far-reaching Interleaved Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Privacy-Preserving machine learning (PPML) can help us train and deploymodels that utilize private information. In particular, on-device MachineLearning allows us to completely avoid sharing information with a third-partyserver during inference. However, on-device models are typically less accuratewhen compared to the server counterparts due to the fact that (1) theytypically only rely on a small set of on-device features and (2) they need tobe small enough to run efficiently on end-user devices. Split Learning (SL) isa promising approach that can overcome these limitations. In SL, a largemachine learning model is divided into two parts, with the bigger part residingon the server-side and a smaller part executing on-device, aiming toincorporate the private features. However, end-to-end training of such modelsrequires exchanging gradients at the cut layer, which might encode privatefeatures or labels. In this paper, we provide insights into potential privacyrisks associated with SL and introduce a novel attack method, EXACT, toreconstruct private information. Furthermore, we also investigate theeffectiveness of various mitigation strategies. Our results indicate that thegradients significantly improve the attacker's effectiveness in all threedatasets reaching almost 100% reconstruction accuracy for some features.However, a small amount of differential privacy (DP) is quite effective inmitigating this risk without causing significant training degradation.", "output": "EXACT: Extensive Attack for Split Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The superconducting linear accelerator is a highly flexiable facility formodern scientific discoveries, necessitating weekly reconfiguration and tuning.Accordingly, minimizing setup time proves essential in affording users withample experimental time. We propose a trend-based soft actor-critic(TBSAC) beamcontrol method with strong robustness, allowing the agents to be trained in asimulated environment and applied to the real accelerator directly withzero-shot. To validate the effectiveness of our method, two different typicalbeam control tasks were performed on China Accelerator Facility for SuperheavyElements (CAFe II) and a light particle injector(LPI) respectively. The orbitcorrection tasks were performed in three cryomodules in CAFe II seperately, thetime required for tuning has been reduced to one-tenth of that needed by humanexperts, and the RMS values of the corrected orbit were all less than 1mm. Theother transmission efficiency optimization task was conducted in the LPI, ouragent successfully optimized the transmission efficiency of radio-frequencyquadrupole(RFQ) to over $85%$ within 2 minutes. The outcomes of these twoexperiments offer substantiation that our proposed TBSAC approach canefficiently and effectively accomplish beam commissioning tasks while upholdingthe same standard as skilled human experts. As such, our method exhibitspotential for future applications in other accelerator commissioning fields.", "output": "Trend-Based SAC Beam Control Method with Zero-Shot in Superconducting Linear Accelerator."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Concerns regarding unfairness and discrimination in the context of artificialintelligence (AI) systems have recently received increased attention from bothlegal and computer science scholars. Yet, the degree of overlap between notionsof algorithmic bias and fairness on the one hand, and legal notions ofdiscrimination and equality on the other, is often unclear, leading tomisunderstandings between computer science and law. What types of bias andunfairness does the law address when it prohibits discrimination? What role canfairness metrics play in establishing legal compliance? In this paper, we aimto illustrate to what extent European Union (EU) non-discrimination lawcoincides with notions of algorithmic fairness proposed in computer scienceliterature and where they differ. The contributions of this paper are asfollows. First, we analyse seminal examples of algorithmic unfairness throughthe lens of EU non-discrimination law, drawing parallels with EU case law.Second, we set out the normative underpinnings of fairness metrics andtechnical interventions and compare these to the legal reasoning of the Courtof Justice of the EU. Specifically, we show how normative assumptions oftenremain implicit in both disciplinary approaches and explain the ensuinglimitations of current AI practice and non-discrimination law. We conclude withimplications for AI practitioners and regulators.", "output": "Algorithmic Unfairness through the Lens of EU Non-Discrimination Law: Or Why the Law is not a Decision Tree."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, methods for learning diverse skills to generate various behaviorswithout external rewards have been actively studied as a form of unsupervisedreinforcement learning. However, most of the existing methods learn a finitenumber of discrete skills, and thus the variety of behaviors that can beexhibited with the learned skills is limited. In this paper, we propose a novelmethod for learning potentially an infinite number of different skills, whichis named discovery of continuous skills on a sphere (DISCS). In DISCS, skillsare learned by maximizing mutual information between skills and states, andeach skill corresponds to a continuous value on a sphere. Because therepresentations of skills in DISCS are continuous, infinitely diverse skillscould be learned. We examine existing methods and DISCS in the MuJoCo Ant robotcontrol environments and show that DISCS can learn much more diverse skillsthan the other methods.", "output": "Unsupervised Discovery of Continuous Skills on a Sphere."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "End-to-end speech translation (ST) is the task of translating speech signalsin the source language into text in the target language. As a cross-modal task,end-to-end ST is difficult to train with limited data. Existing methods oftentry to transfer knowledge from machine translation (MT), but their performancesare restricted by the modality gap between speech and text. In this paper, wepropose Cross-modal Mixup via Optimal Transport CMOT to overcome the modalitygap. We find the alignment between speech and text sequences via optimaltransport and then mix up the sequences from different modalities at a tokenlevel using the alignment. Experiments on the MuST-C ST benchmark demonstratethat CMOT achieves an average BLEU of 30.0 in 8 translation directions,outperforming previous methods. Further analysis shows CMOT can adaptively findthe alignment between modalities, which helps alleviate the modality gapbetween speech and text. Code is publicly available at", "output": "CMOT: Cross-modal Mixup via Optimal Transport for Speech Translation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Current state-of-the-art object-centric models use slots and attention-basedrouting for binding. However, this class of models has several conceptuallimitations: the number of slots is hardwired; all slots have equal capacity;training has high computational cost; there are no object-level relationalfactors within slots. Synchrony-based models in principle can address theselimitations by using complex-valued activations which store binding informationin their phase components. However, working examples of such synchrony-basedmodels have been developed only very recently, and are still limited to toygrayscale datasets and simultaneous storage of less than three objects inpractice. Here we introduce architectural modifications and a novel contrastivelearning method that greatly improve the state-of-the-art synchrony-basedmodel. For the first time, we obtain a class of synchrony-based models capableof discovering objects in an unsupervised manner in multi-object color datasetsand simultaneously representing more than three objects", "output": "Contrastive Training of Complex-Valued Autoencoders for Object Discovery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent studies have shown competitive performance in protein design that aimsto find the amino acid sequence folding into the desired structure. However,most of them disregard the importance of predictive confidence, fail to coverthe vast protein space, and do not incorporate common protein knowledge. Afterwitnessing the great success of pretrained models on diverse protein-relatedtasks and the fact that recovery is highly correlated with confidence, wewonder whether this knowledge can push the limits of protein design further. Asa solution, we propose a knowledge-aware module that refines low-qualityresidues. We also introduce a memory-retrieval mechanism to save more than 50%of the training time. We extensively evaluate our proposed method on the CATH,TS50, and TS500 datasets and our results show that our Knowledge-Design methodoutperforms the previous PiFold method by approximately 9% on the CATHdataset. Specifically, Knowledge-Design is the first method that achieves 60+%recovery on CATH, TS50 and TS500 benchmarks. We also provide additionalanalysis to demonstrate the effectiveness of our proposed method. The code willbe publicly available.", "output": "Knowledge-Design: Pushing the Limit of Protein Deign via Knowledge Refinement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised representation learning has proved to be a valuable componentfor out-of-distribution (OoD) detection with only the texts of in-distribution(ID) examples. These approaches either train a language model from scratch orfine-tune a pre-trained language model using ID examples, and then take theperplexity output by the language model as OoD scores. In this paper, weanalyze the complementary characteristics of both OoD detection methods andpropose a multi-level knowledge distillation approach that integrates theirstrengths while mitigating their limitations. Specifically, we use a fine-tunedmodel as the teacher to teach a randomly initialized student model on the IDexamples. Besides the prediction layer distillation, we present asimilarity-based intermediate layer distillation method to thoroughly explorethe representation space of the teacher model. In this way, the learned studentcan better represent the ID data manifold while gaining a stronger ability tomap OoD examples outside the ID data manifold with the regularization inheritedfrom pre-training. Besides, the student model sees only ID examples duringparameter learning, further promoting more distinguishable features for OoDdetection. We conduct extensive experiments over multiple benchmark datasets,i.e., CLINC150, SST, ROSTD, 20 NewsGroups, and AG News; showing that theproposed method yields new state-of-the-art performance. We also explore itsapplication as an AIGC detector to distinguish between answers generated byChatGPT and human experts. It is observed that our model exceeds humanevaluators in the pair-expert task on the Human ChatGPT Comparison Corpus.", "output": "Multi-Level Knowledge Distillation for Out-of-Distribution Detection in Text."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Relation extraction (RE), which has relied on structurally annotated corporafor model training, has been particularly challenging in low-resource scenariosand domains. Recent literature has tackled low-resource RE by self-supervisedlearning, where the solution involves pretraining the relation embedding byRE-based objective and finetuning on labeled data by classification-basedobjective. However, a critical challenge to this approach is the gap inobjectives, which prevents the RE model from fully utilizing the knowledge inpretrained representations. In this paper, we aim at bridging the gap andpropose to pretrain and finetune the RE model using consistent objectives ofcontrastive learning. Since in this kind of representation learning paradigm,one relation may easily form multiple clusters in the representation space, wefurther propose a multi-center contrastive loss that allows one relation toform multiple clusters to better align with pretraining. Experiments on twodocument-level RE datasets, BioRED and Re-DocRED, demonstrate the effectivenessof our method. Particularly, when using 1% end-task training data, our methodoutperforms PLM-based RE classifier by 10.5% and 5.8% on the two datasets,respectively.", "output": "Continual Contrastive Finetuning Improves Low-Resource Relation Extraction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In single-objective optimization, it is well known that evolutionaryalgorithms also without further adjustments can tolerate a certain amount ofnoise in the evaluation of the objective function. In contrast, this questionis not at all understood for multi-objective optimization.In this work, we conduct the first mathematical runtime analysis of a simplemulti-objective evolutionary algorithm (MOEA) on a classic benchmark in thepresence of noise in the objective functions. We prove that when bit-wise priornoise with rate $p le alpha/n$, $alpha$ a suitable constant, is present, theemph{simple evolutionary multi-objective optimizer} (SEMO) without anyadjustments to cope with noise finds the Pareto front of the OneMinMaxbenchmark in time $O(n^2log n)$, just as in the case without noise. Given thatthe problem here is to arrive at a population consisting of $n+1$ individualswitnessing the Pareto front, this is a surprisingly strong robustness to noise(comparably simple evolutionary algorithms cannot optimize the single-objectiveOneMax problem in polynomial time when $p = omega(log(n)/n)$). Our proofssuggest that the strong robustness of the MOEA stems from its implicitdiversity mechanism designed to enable it to compute a population covering thewhole Pareto front.Interestingly this result only holds when the objective value of a solutionis determined only once and the algorithm from that point on works with this,possibly noisy, objective value. We prove that when all solutions arereevaluated in each iteration, then any noise rate $p = omega(log(n)/n^2)$leads to a super-polynomial runtime. This is very different fromsingle-objective optimization, where it is generally preferred to reevaluatesolutions whenever their fitness is important and where examples are known suchthat not reevaluating solutions can lead to catastrophic performance losses.", "output": "Runtime Analyses of Multi-Objective Evolutionary Algorithms in the Presence of Noise."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Since acquiring perfect supervision is usually difficult, real-world machinelearning tasks often confront inaccurate, incomplete, or inexact supervision,collectively referred to as weak supervision. In this work, we present WSAUC, aunified framework for weakly supervised AUC optimization problems, which coversnoisy label learning, positive-unlabeled learning, multi-instance learning, andsemi-supervised learning scenarios. Within the WSAUC framework, we first framethe AUC optimization problems in various weakly supervised scenarios as acommon formulation of minimizing the AUC risk on contaminated sets, anddemonstrate that the empirical risk minimization problems are consistent withthe true AUC. Then, we introduce a new type of partial AUC, specifically, thereversed partial AUC (rpAUC), which serves as a robust training objective forAUC maximization in the presence of contaminated labels. WSAUC offers auniversal solution for AUC optimization in various weakly supervised scenariosby maximizing the empirical rpAUC. Theoretical and experimental results undermultiple settings support the effectiveness of WSAUC on a range of weaklysupervised AUC optimization tasks.", "output": "Weakly Supervised AUC Optimization: A Unified Partial AUC Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "An important topic in medical research is the process of improving the imagesobtained from medical devices. As a consequence, there is also a need toimprove medical image resolution and analysis. Another issue in this field isthe large amount of stored medical data [16]. Human brain databases at medicalinstitutes, for example, can accumulate tens of Terabytes of data per year. Inthis paper, we propose a novel medical image format representation based onmultiple data structures that improve the information maintained in the medicalimages. The new representation keeps additional metadata information, such asthe image class or tags for the objects found in the image. We defined our ownontology to help us classify the objects found in medical images using amultilayer neural network. As we generally deal with large data sets, we usedthe MapReduce paradigm in the Cloud environment to speed up the imageprocessing. To optimize the transfer between Cloud nodes and to reduce thepreprocessing time, we also propose a data compression method based ondeduplication. We test our solution for image representation and efficient datatransfer in a multisite cloud environment. Our proposed solution optimizes thedata transfer with a time improvement of 27% on average.", "output": "Advanced Medical Image Representation for Efficient Processing and Transfer in Multisite Clouds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Clustering data is an unsupervised learning approach that aims to divide aset of data points into multiple groups. It is a crucial yet demanding subjectin machine learning and data mining. Its successful applications span variousfields. However, conventional clustering techniques necessitate theconsideration of balance significance in specific applications. Therefore, thispaper addresses the challenge of imbalanced clustering problems and presents anew method for balanced clustering by utilizing entropy-aware similarity, whichcan be defined as the degree of balances. We have coined the term,entropy-aware similarity for balanced clustering (EASB), which maximizesbalance during clustering by complementary clustering of unbalanced data andincorporating entropy in a novel similarity formula that accounts for bothangular differences and distances. The effectiveness of the proposed approachis evaluated on actual melanoma medial data, specifically the InternationalSkin Imaging Collaboration (ISIC) 2019 and 2020 challenge datasets, todemonstrate how it can successfully cluster the data while preserving balance.Lastly, we can confirm that the proposed method exhibited outstandingperformance in detecting melanoma, comparing to classical methods.", "output": "Entropy-Aware Similarity for Balanced Clustering: A Case Study with Melanoma Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Building Information Modeling (BIM) technology is a key component of modernconstruction engineering and project management workflows. As-is BIM modelsthat represent the spatial reality of a project site can offer crucialinformation to stakeholders for construction progress monitoring, errorchecking, and building maintenance purposes. Geometric methods forautomatically converting raw scan data into BIM models (Scan-to-BIM) often failto make use of higher-level semantic information in the data. Whereas, semanticsegmentation methods only output labels at the point level without creatingobject level models that is necessary for BIM. To address these issues, thisresearch proposes a hybrid semantic-geometric approach for clutter-resistantfloorplan generation from laser-scanned building point clouds. The input pointclouds are first pre-processed by normalizing the coordinate system andremoving outliers. Then, a semantic segmentation network based on PointNet++ isused to label each point as ceiling, floor, wall, door, stair, and clutter. Theclutter points are removed whereas the wall, door, and stair points are usedfor 2D floorplan generation. A region-growing segmentation algorithm pairedwith geometric reasoning rules is applied to group the points together intoindividual building elements. Finally, a 2-fold Random Sample Consensus(RANSAC) algorithm is applied to parameterize the building elements into 2Dlines which are used to create the output floorplan. The proposed method isevaluated using the metrics of precision, recall, Intersection-over-Union(IOU), Betti error, and warping error.", "output": "A Hybrid Semantic-Geometric Approach for Clutter-Resistant Floorplan Generation from Building Point Clouds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In medical imaging, image synthesis is the estimation process of one image(sequence, modality) from another image (sequence, modality). Since images withdifferent modalities provide diverse biomarkers and capture various features,multi-modality imaging is crucial in medicine. While multi-screening isexpensive, costly, and time-consuming to report by radiologists, imagesynthesis methods are capable of artificially generating missing modalities.Deep learning models can automatically capture and extract the high dimensionalfeatures. Especially, generative adversarial network (GAN) as one of the mostpopular generative-based deep learning methods, uses convolutional networks asgenerators, and estimated images are discriminated as true or false based on adiscriminator network. This review provides brain image synthesis via GANs. Wesummarized the recent developments of GANs for cross-modality brain imagesynthesis including CT to PET, CT to MRI, MRI to PET, and vice versa.", "output": "Generative Adversarial Networks for Brain Images Synthesis: A Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Facial Expression Recognition (FER) plays an important role in human-computerinteractions and is used in a wide range of applications. Convolutional NeuralNetworks (CNN) have shown promise in their ability to classify human facialexpressions, however, large CNNs are not well-suited to be implemented onresource- and energy-constrained IoT devices. In this work, we present ahierarchical framework for developing and optimizing hardware-aware CNNs tunedfor deployment at the edge. We perform a comprehensive analysis across variousedge AI accelerators including NVIDIA Jetson Nano, Intel Neural Compute Stick,and Coral TPU. Using the proposed strategy, we achieved a peak accuracy of99.49% when testing on the CK+ facial expression recognition dataset.Additionally, we achieved a minimum inference latency of 0.39 milliseconds anda minimum power consumption of 0.52 Watts.", "output": "Facial Expression Recognition at the Edge: CPU vs GPU vs VPU vs TPU."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In healthcare, accurately classifying medical images is vital, butconventional methods often hinge on medical data with a consistent gridstructure, which may restrict their overall performance. Recent medicalresearch has been focused on tweaking the architectures to attain betterperformance without giving due consideration to the representation of data. Inthis paper, we present a novel approach for transforming grid based data intoits higher dimensional representations, leveraging unstructured point clouddata structures. We first generate a sparse point cloud from an image byintegrating pixel color information as spatial coordinates. Next, we constructa hypersurface composed of points based on the image dimensions, with eachsmooth section within this hypersurface symbolizing a specific pixel location.Polygonal face construction is achieved using an adjacency tensor. Finally, adense point cloud is generated by densely sampling the constructedhypersurface, with a focus on regions of higher detail. The effectiveness ofour approach is demonstrated on a publicly accessible brain tumor dataset,achieving significant improvements over existing classification techniques.This methodology allows the extraction of intricate details from the originalimage, opening up new possibilities for advanced image analysis and processingtasks.", "output": "Transcending Grids: Point Clouds and Surface Representations Powering Neurological Processing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Weakly supervised vision-and-language pre-training (WVLP), which learnscross-modal representations with limited cross-modal supervision, has beenshown to effectively reduce the data cost of pre-training while maintainingdecent performance on downstream tasks. However, current WVLP methods use onlylocal descriptions of images, i.e., object tags, as cross-modal anchors toconstruct weakly-aligned image-text pairs for pre-training. This affects thedata quality and thus the effectiveness of pre-training. In this paper, wepropose to directly take a small number of aligned image-text pairs as anchors,and represent each unaligned image and text by its similarities to theseanchors, i.e., relative representations. We build a WVLP framework based on therelative representations, namely RELIT, which collects high-qualityweakly-aligned image-text pairs from large-scale image-only and text-only datafor pre-training through relative representation-based retrieval andgeneration. Experiments on four downstream tasks show that RELIT achieves newstate-of-the-art results under the weakly supervised setting.", "output": "Weakly Supervised Vision-and-Language Pre-training with Relative Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Efficient compression of correlated data is essential to minimizecommunication overload in multi-sensor networks. In such networks, each sensorindependently compresses the data and transmits them to a central node due tolimited communication bandwidth. A decoder at the central node decompresses andpasses the data to a pre-trained machine learning-based task to generate thefinal output. Thus, it is important to compress the features that are relevantto the task. Additionally, the final performance depends heavily on the totalavailable bandwidth. In practice, it is common to encounter varyingavailability in bandwidth, and higher bandwidth results in better performanceof the task. We design a novel distributed compression framework composed ofindependent encoders and a joint decoder, which we call neural distributedprincipal component analysis (NDPCA). NDPCA flexibly compresses data frommultiple sources to any available bandwidth with a single model, reducingcomputing and storage overhead. NDPCA achieves this by learning low-rank taskrepresentations and efficiently distributing bandwidth among sensors, thusproviding a graceful trade-off between performance and bandwidth. Experimentsshow that NDPCA improves the success rate of multi-view robotic armmanipulation by 9% and the accuracy of object detection tasks on satelliteimagery by 14% compared to an autoencoder with uniform bandwidth allocation.", "output": "Task-aware Distributed Source Coding under Dynamic Bandwidth."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transfer learning involves adapting a pre-trained model to novel downstreamtasks. However, we observe that current transfer learning methods often fail tofocus on task-relevant features. In this work, we emphasize the importance ofrefocusing the attention in transfer learning. We introduce Top-Down AttentionSteering (TOAST), a novel transfer learning algorithm that keeps thepre-trained backbone frozen, while selecting the task-relevant elements in theoutput and feeding them back to the model to steer its attention to thetask-specific features. By refocusing the attention only, TOAST achievesstate-of-the-art results on a number of transfer learning benchmarks, whilehaving a small portion of tunable parameters. Compared to fully fine-tuning,LoRA, and prompt tuning, TOAST substantially improves performance across arange of fine-grained visual classification datasets (e.g., 81.1% -&gt; 86.2% onFGVC). TOAST also outperforms the fully fine-tuned Alpaca model oninstruction-following language generation. Code is available at", "output": "Refocusing Is Key to Transfer Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modern neural-network-based no-reference image- and video-quality metricsexhibit performance as high as full-reference metrics. These metrics are widelyused to improve visual quality in computer vision methods and compare videoprocessing methods. However, these metrics are not stable to traditionaladversarial attacks, which can cause incorrect results. Our goal is toinvestigate the boundaries of no-reference metrics applicability, and in thispaper, we propose a fast adversarial perturbation attack on no-referencequality metrics. The proposed attack (FACPA) can be exploited as apreprocessing step in real-time video processing and compression algorithms.This research can yield insights to further aid in designing of stableneural-network-based no-reference quality metrics.", "output": "Fast Adversarial CNN-based Perturbation Attack on No-Reference Image- and Video-Quality Metrics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Online video platforms receive hundreds of hours of uploads every minute,making manual content moderation impossible. Unfortunately, the most vulnerableconsumers of malicious video content are children from ages 1-5 whose attentionis easily captured by bursts of color and sound. Scammers attempting tomonetize their content may craft malicious children's videos that aresuperficially similar to educational videos, but include scary and disgustingcharacters, violent motions, loud music, and disturbing noises. Prominent videohosting platforms like YouTube have taken measures to mitigate maliciouscontent on their platform, but these videos often go undetected by currentcontent moderation tools that are focused on removing pornographic orcopyrighted content. This paper introduces our toolkit Malicious or Benign forpromoting research on automated content moderation of children's videos. Wepresent 1) a customizable annotation tool for videos, 2) a new dataset withdifficult to detect test cases of malicious content and 3) a benchmark suite ofstate-of-the-art video classification models.", "output": "Malicious or Benign? Towards Effective Content Moderation for Children's Videos."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generating differentially private (DP) synthetic data that closely resemblesthe original private data without leaking sensitive user information is ascalable way to mitigate privacy concerns in the current data-driven world. Incontrast to current practices that train customized models for this task, weaim to generate DP Synthetic Data via APIs (DPSDA), where we treat foundationmodels as blackboxes and only utilize their inference APIs. Such API-based,training-free approaches are easier to deploy as exemplified by the recentsurge in the number of API-based apps. These approaches can also leverage thepower of large foundation models which are accessible via their inference APIswhile the model weights are unreleased. However, this comes with greaterchallenges due to strictly more restrictive model access and the additionalneed to protect privacy from the API provider.In this paper, we present a new framework called Private Evolution (PE) tosolve this problem and show its initial promise on synthetic images.Surprisingly, PE can match or even outperform state-of-the-art (SOTA) methodswithout any model training. For example, on CIFAR10 (with ImageNet as thepublic data), we achieve FID&lt;=7.9 with privacy cost epsilon=0.67, significantlyimproving the previous SOTA from epsilon=32. We further demonstrate the promiseof applying PE on large foundation models such as Stable Diffusion to tacklechallenging private datasets with a small number of high-resolution images.", "output": "Differentially Private Synthetic Data via Foundation Model APIs 1: Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Conditional graph generation tasks involve training a model to generate agraph given a set of input conditions. Many previous studies employautoregressive models to incrementally generate graph components such as nodesand edges. However, as graphs typically lack a natural ordering among theircomponents, converting a graph into a sequence of tokens is notstraightforward. While prior works mostly rely on conventional heuristics orgraph traversal methods like breadth-first search (BFS) or depth-first search(DFS) to convert graphs to sequences, the impact of ordering on graphgeneration has largely been unexplored. This paper contributes to this problemby: (1) highlighting the crucial role of ordering in autoregressive graphgeneration models, (2) proposing a novel theoretical framework that perceivesordering as a dimensionality reduction problem, thereby facilitating a deeperunderstanding of the relationship between orderings and generated graphaccuracy, and (3) introducing \"latent sort,\" a learning-based ordering schemeto perform dimensionality reduction of graph tokens. Our experimental resultsshowcase the effectiveness of latent sort across a wide range of graphgeneration tasks, encouraging future works to further explore and developlearning-based ordering schemes for autoregressive graph generation.", "output": "Let There Be Order: Rethinking Ordering in Autoregressive Graph Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Text-to-image diffusion models are now capable of generating images that areoften indistinguishable from real images. To generate such images, these modelsmust understand the semantics of the objects they are asked to generate. Inthis work we show that, without any training, one can leverage this semanticknowledge within diffusion models to find semantic correspondences -- locationsin multiple images that have the same semantic meaning. Specifically, given animage, we optimize the prompt embeddings of these models for maximum attentionon the regions of interest. These optimized embeddings capture semanticinformation about the location, which can then be transferred to another image.By doing so we obtain results on par with the strongly supervised state of theart on the PF-Willow dataset and significantly outperform (20.9% relative forthe SPair-71k dataset) any existing weakly or unsupervised method on PF-Willow,CUB-200 and SPair-71k datasets.", "output": "Unsupervised Semantic Correspondence Using Stable Diffusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Denoising Diffusion Probabilistic Models (DDPM) have shown remarkableefficacy in the synthesis of high-quality images. However, their inferenceprocess characteristically requires numerous, potentially hundreds, ofiterative steps, which could lead to the problem of exposure bias due to theaccumulation of prediction errors over iterations. Previous work has attemptedto mitigate this issue by perturbing inputs during training, which consequentlymandates the retraining of the DDPM. In this work, we conduct a systematicstudy of exposure bias in diffusion models and, intriguingly, we find that theexposure bias could be alleviated with a new sampling method, withoutretraining the model. We empirically and theoretically show that, duringinference, for each backward time step $t$ and corresponding state $hat{x}_t$,there might exist another time step $t_s$ which exhibits superior coupling with$hat{x}_t$. Based on this finding, we introduce an inference method namedTime-Shift Sampler. Our framework can be seamlessly integrated with existingsampling algorithms, such as DDIM or DDPM, inducing merely minimal additionalcomputations. Experimental results show that our proposed framework caneffectively enhance the quality of images generated by existing samplingalgorithms.", "output": "Alleviating Exposure Bias in Diffusion Models through Sampling with Shifted Time Steps."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Annotating data for multi-label classification is prohibitively expensivebecause every category of interest must be confirmed to be present or absent.Recent work on single positive multi-label (SPML) learning shows that it ispossible to train effective multi-label classifiers using only one positivelabel per image. However, the standard benchmarks for SPML are derived fromtraditional multi-label classification datasets by retaining one positive labelfor each training example (chosen uniformly at random) and discarding all otherlabels. In realistic settings it is not likely that positive labels are chosenuniformly at random. This work introduces protocols for studying label bias inSPML and provides new empirical results.", "output": "Understanding Label Bias in Single Positive Multi-Label Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Semantic segmentation is a critical task in computer vision that aims toidentify and classify individual pixels in an image, with numerous applicationsfor example autonomous driving and medical image analysis. However, semanticsegmentation can be super challenging particularly due to the need for largeamounts of annotated data. Annotating images is a time-consuming and costlyprocess, often requiring expert knowledge and significant effort. In thispaper, we propose a novel approach for semantic segmentation by eliminating theneed of ground-truth segmentation maps. Instead, our approach requires only therough information of individual semantic class proportions, shortened assemantic proportions. It greatly simplifies the data annotation process andthus will significantly reduce the annotation time and cost, making it morefeasible for large-scale applications. Moreover, it opens up new possibilitiesfor semantic segmentation tasks where obtaining the full ground-truthsegmentation maps may not be feasible or practical. Extensive experimentalresults demonstrate that our approach can achieve comparable and sometimes evenbetter performance against the benchmark method that relies on the ground-truthsegmentation maps. Utilising semantic proportions suggested in this work offersa promising direction for future research in the field of semanticsegmentation.", "output": "Semantic Segmentation by Semantic Proportions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As the adoption of AI systems within the clinical setup grows, limitations inbandwidth could create communication bottlenecks when streaming imaging data,leading to delays in patient diagnosis and treatment. As such, healthcareproviders and AI vendors will require greater computational infrastructure,therefore dramatically increasing costs. To that end, we developed intelligentstreaming, a state-of-the-art framework to enable accelerated, cost-effective,bandwidth-optimized, and computationally efficient AI inference for clinicaldecision making at scale. For classification, intelligent streaming reduced thedata transmission by 99.01% and decoding time by 98.58%, while increasingthroughput by 27.43x. For segmentation, our framework reduced data transmissionby 90.32%, decoding time by 90.26%, while increasing throughput by 4.20x. Ourwork demonstrates that intelligent streaming results in faster turnaroundtimes, and reduced overall cost of data and transmission, without negativelyimpacting clinical decision making using AI systems.", "output": "High-Throughput AI Inference for Medical Image Classification and Segmentation using Intelligent Streaming."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Out-of-distribution (OOD) data poses serious challenges in deployed machinelearning models, so methods of predicting a model's performance on OOD datawithout labels are important for machine learning safety. While a number ofmethods have been proposed by prior work, they often underestimate the actualerror, sometimes by a large margin, which greatly impacts their applicabilityto real tasks. In this work, we identify pseudo-label shift, or the differencebetween the predicted and true OOD label distributions, as a key indicator tothis underestimation. Based on this observation, we introduce a novel methodfor estimating model performance by leveraging optimal transport theory,Confidence Optimal Transport (COT), and show that it provably provides morerobust error estimates in the presence of pseudo-label shift. Additionally, weintroduce an empirically-motivated variant of COT, Confidence Optimal Transportwith Thresholding (COTT), which applies thresholding to the individualtransport costs and further improves the accuracy of COT's error estimates. Weevaluate COT and COTT on a variety of standard benchmarks that induce varioustypes of distribution shift -- synthetic, novel subpopulation, and natural --and show that our approaches significantly outperform existing state-of-the-artmethods with an up to 3x lower prediction error.", "output": "Characterizing Out-of-Distribution Error via Optimal Transport."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To ensure the out-of-distribution (OOD) generalization performance,traditional domain generalization (DG) methods resort to training on data frommultiple sources with different underlying distributions. And the success ofthose DG methods largely depends on the fact that there are diverse trainingdistributions. However, it usually needs great efforts to obtain enoughheterogeneous data due to the high expenses, privacy issues or the scarcity ofdata. Thus an interesting yet seldom investigated problem arises: how toimprove the OOD generalization performance when the perceived heterogeneity islimited. In this paper, we instantiate a new framework called few-domaingeneralization (FDG), which aims to learn a generalizable model from very fewdomains of novel tasks with the knowledge acquired from previous learningexperiences on base tasks. Moreover, we propose a Meta Adaptive Task Sampling(MATS) procedure to differentiate base tasks according to their semantic anddomain-shift similarity to the novel task. Empirically, we show that the newlyintroduced FDG framework can substantially improve the OOD generalizationperformance on the novel task and further combining MATS with episodic trainingcould outperform several state-of-the-art DG baselines on widely usedbenchmarks like PACS and DomainNet.", "output": "Meta Adaptive Task Sampling for Few-Domain Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Although existing image anomaly detection methods yield impressive results,they are mostly an offline learning paradigm that requires excessive datapre-collection, limiting their adaptability in industrial scenarios with onlinestreaming data. Online learning-based image anomaly detection methods are morecompatible with industrial online streaming data but are rarely noticed. Forthe first time, this paper presents a fully online learning image anomalydetection method, namely LeMO, learning memory for online image anomalydetection. LeMO leverages learnable memory initialized with orthogonal randomnoise, eliminating the need for excessive data in memory initialization andcircumventing the inefficiencies of offline data collection. Moreover, acontrastive learning-based loss function for anomaly detection is designed toenable online joint optimization of memory and image target-oriented features.The presented method is simple and highly effective. Extensive experimentsdemonstrate the superior performance of LeMO in the online setting.Additionally, in the offline setting, LeMO is also competitive with the currentstate-of-the-art methods and achieves excellent performance in few-shotscenarios.", "output": "Towards Total Online Unsupervised Anomaly Detection and Localization in Industrial Vision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "There are more than 80,000 character categories in Chinese while most of themare rarely used. To build a high performance handwritten Chinese characterrecognition (HCCR) system supporting the full character set with a traditionalapproach, many training samples need be collected for each character category,which is both time-consuming and expensive. In this paper, we propose a novelapproach to transforming Chinese character glyph images generated from fontlibraries to handwritten ones with a denoising diffusion probabilistic model(DDPM). Training from handwritten samples of a small character set, the DDPM iscapable of mapping printed strokes to handwritten ones, which makes it possibleto generate photo-realistic and diverse style handwritten samples of unseencharacter categories. Combining DDPM-synthesized samples of unseen categorieswith real samples of other categories, we can build an HCCR system to supportthe full character set. Experimental results on CASIA-HWDB dataset with 3,755character categories show that the HCCR systems trained with synthetic samplesperform similarly with the one trained with real samples in terms ofrecognition accuracy. The proposed method has the potential to address HCCRwith a larger vocabulary.", "output": "Zero-shot Generation of Training Data with Denoising Diffusion Probabilistic Model for Handwritten Chinese Character Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, a bipartite output regulation problem is solved for a class ofnonlinear multi-agent systems subject to static signed communication networks.A nonlinear distributed observer is proposed for a nonlinear exosystem withcooperation-competition interactions to address the problem. Sufficientconditions are provided to guarantee its existence and stability. Theexponential stability of the observer is established. As a practicalapplication, a leader-following bipartite consensus problem is solved for aclass of nonlinear multi-agent systems based on the observer. Finally, anetwork of multiple pendulum systems is treated to support the feasibility ofthe proposed design. The possible application of the approach to generatespecific Turing patterns is also presented.", "output": "Nonlinear Bipartite Output Regulation with Application to Turing Pattern."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the development of multimedia technology, Video Copy Detection has beena crucial problem for social media platforms. Meta AI hold Video SimilarityChallenge on CVPR 2023 to push the technology forward. In this report, we shareour winner solutions on Matching Track. We propose a Similarity AlignmentModel(SAM) for video copy segment matching. Our SAM exhibits superiorperformance compared to other competitors, with a 0.108 / 0.144 absoluteimprovement over the second-place competitor in Phase 1 / Phase 2. Code isavailable at", "output": "A Similarity Alignment Model for Video Copy Segment Matching."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most existing RGB-based trackers target low frame rate benchmarks of around30 frames per second. This setting restricts the tracker's functionality in thereal world, especially for fast motion. Event-based cameras as bioinspiredsensors provide considerable potential for high frame rate tracking due totheir high temporal resolution. However, event-based cameras cannot offerfine-grained texture information like conventional cameras. This uniquecomplementarity motivates us to combine conventional frames and events for highframe rate object tracking under various challenging conditions. Inthispaper,we propose an end-to-end network consisting of multi-modality alignment andfusion modules to effectively combine meaningful information from bothmodalities at different measurement rates. The alignment module is responsiblefor cross-style and cross-frame-rate alignment between frame and eventmodalities under the guidance of the moving cues furnished by events. While thefusion module is accountable for emphasizing valuable features and suppressingnoise information by the mutual complement between the two modalities.Extensive experiments show that the proposed approach outperformsstate-of-the-art trackers by a significant margin in high frame rate tracking.With the FE240hz dataset, our approach achieves high frame rate tracking up to240Hz.", "output": "Frame-Event Alignment and Fusion Network for High Frame Rate Tracking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Currently, video behavior recognition is one of the most foundational tasksof computer vision. The 2D neural networks of deep learning are built forrecognizing pixel-level information such as images with RGB, RGB-D, or opticalflow formats, with the current increasingly wide usage of surveillance videoand more tasks related to human action recognition. There are increasing tasksrequiring temporal information for frames dependency analysis. The researchershave widely studied video-based recognition rather thanimage-based(pixel-based) only to extract more informative elements fromgeometry tasks. Our current related research addresses multiple novel proposedresearch works and compares their advantages and disadvantages between thederived deep learning frameworks rather than machine learning frameworks. Thecomparison happened between existing frameworks and datasets, which are videoformat data only. Due to the specific properties of human actions and theincreasingly wide usage of deep neural networks, we collected all researchworks within the last three years between 2020 to 2022. In our article, theperformance of deep neural networks surpassed most of the techniques in thefeature learning and extraction tasks, especially video action recognition.", "output": "Deep Neural Networks in Video Human Action Recognition: A Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Monocular 3D detection is a challenging task due to the lack of accurate 3Dinformation. Existing approaches typically rely on geometry constraints anddense depth estimates to facilitate the learning, but often fail to fullyexploit the benefits of three-dimensional feature extraction in frustum and 3Dspace. In this paper, we propose textbf{OccupancyM3D}, a method of learningoccupancy for monocular 3D detection. It directly learns occupancy in frustumand 3D space, leading to more discriminative and informative 3D features andrepresentations. Specifically, by using synchronized raw sparse LiDAR pointclouds, we define the space status and generate voxel-based occupancy labels.We formulate occupancy prediction as a simple classification problem and designassociated occupancy losses. Resulting occupancy estimates are employed toenhance original frustum/3D features. As a result, experiments on KITTI andWaymo open datasets demonstrate that the proposed method achieves a new stateof the art and surpasses other methods by a significant margin. Codes andpre-trained models will be available aturl{", "output": "Learning Occupancy for Monocular 3D Object Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Understanding action recognition in egocentric videos has emerged as a vitalresearch topic with numerous practical applications. With the limitation in thescale of egocentric data collection, learning robust deep learning-based actionrecognition models remains difficult. Transferring knowledge learned from thelarge-scale exocentric data to the egocentric data is challenging due to thedifference in videos across views. Our work introduces a novel cross-viewlearning approach to action recognition (CVAR) that effectively transfersknowledge from the exocentric to the egocentric view. First, we introduce anovel geometric-based constraint into the self-attention mechanism inTransformer based on analyzing the camera positions between two views. Then, wepropose a new cross-view self-attention loss learned on unpaired cross-viewdata to enforce the self-attention mechanism learning to transfer knowledgeacross views. Finally, to further improve the performance of our cross-viewlearning approach, we present the metrics to measure the correlations in videosand attention maps effectively. Experimental results on standard egocentricaction recognition benchmarks, i.e., Charades-Ego, EPIC-Kitchens-55, andEPIC-Kitchens-100, have shown our approach's effectiveness and state-of-the-artperformance.", "output": "Cross-view Action Recognition Understanding From Exocentric to Egocentric Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continual semantic segmentation aims to learn new classes while maintainingthe information from the previous classes. Although prior studies have shownimpressive progress in recent years, the fairness concern in the continualsemantic segmentation needs to be better addressed. Meanwhile, fairness is oneof the most vital factors in deploying the deep learning model, especially inhuman-related or safety applications. In this paper, we present a novelFairness Continual Learning approach to the semantic segmentation problem. Inparticular, under the fairness objective, a new fairness continual learningframework is proposed based on class distributions. Then, a novel PrototypicalContrastive Clustering loss is proposed to address the significant challengesin continual learning, i.e., catastrophic forgetting and background shift. Ourproposed loss has also been proven as a novel, generalized learning paradigm ofknowledge distillation commonly used in continual learning. Moreover, theproposed Conditional Structural Consistency loss further regularized thestructural constraint of the predicted segmentation. Our proposed approach hasachieved State-of-the-Art performance on three standard scene understandingbenchmarks, i.e., ADE20K, Cityscapes, and Pascal VOC, and promoted the fairnessof the segmentation model.", "output": "Fairness Continual Learning Approach to Semantic Scene Understanding in Open-World Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Temporal action localization (TAL), which involves recognizing and locatingaction instances, is a challenging task in video understanding. Most existingapproaches directly predict action classes and regress offsets to boundaries,while overlooking the discrepant importance of each frame. In this paper, wepropose an Action Sensitivity Learning framework (ASL) to tackle this task,which aims to assess the value of each frame and then leverage the generatedaction sensitivity to recalibrate the training procedure. We first introduce alightweight Action Sensitivity Evaluator to learn the action sensitivity at theclass level and instance level, respectively. The outputs of the two branchesare combined to reweight the gradient of the two sub-tasks. Moreover, based onthe action sensitivity of each frame, we design an Action Sensitive ContrastiveLoss to enhance features, where the action-aware frames are sampled as positivepairs to push away the action-irrelevant frames. The extensive studies onvarious action localization benchmarks (i.e., MultiThumos, Charades,Ego4D-Moment Queries v1.0, Epic-Kitchens 100, Thumos14 and ActivityNet1.3) showthat ASL surpasses the state-of-the-art in terms of average-mAP under multipletypes of scenarios, e.g., single-labeled, densely-labeled and egocentric.", "output": "Action Sensitivity Learning for Temporal Action Localization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multimodal Variational Autoencoders (VAEs) represent a promising group ofgenerative models that facilitate the construction of a tractable posteriorwithin the latent space, given multiple modalities. Daunhawer et al. (2022)demonstrate that as the number of modalities increases, the generative qualityof each modality declines. In this study, we explore an alternative approach toenhance the generative performance of multimodal VAEs by jointly modeling thelatent space of unimodal VAEs using score-based models (SBMs). The role of theSBM is to enforce multimodal coherence by learning the correlation among thelatent variables. Consequently, our model combines the superior generativequality of unimodal VAEs with coherent integration across different modalities.", "output": "Score-Based Multimodal Autoencoders."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In light of the significant progress made in the development and applicationof semantic segmentation tasks, there has been increasing attention towardsimproving the robustness of segmentation models against natural degradationfactors (e.g., rain streaks) or artificially attack factors (e.g., adversarialattack). Whereas, most existing methods are designed to address a singledegradation factor and are tailored to specific application scenarios. In thiswork, we present the first attempt to improve the robustness of semanticsegmentation tasks by simultaneously handling different types of degradationfactors. Specifically, we introduce the Preprocessing Enhanced AdversarialRobust Learning (PEARL) framework based on the analysis of our proposed NaiveAdversarial Training (NAT) framework. Our approach effectively handles bothrain streaks and adversarial perturbation by transferring the robustness of thesegmentation model to the image derain model. Furthermore, as opposed to thecommonly used Negative Adversarial Attack (NAA), we design the Auxiliary MirrorAttack (AMA) to introduce positive information prior to the training of thePEARL framework, which improves defense capability and segmentationperformance. Our extensive experiments and ablation studies based on differentderain methods and segmentation models have demonstrated the significantperformance improvement of PEARL with AMA in defense against variousadversarial attacks and rain streaks while maintaining high generalizationperformance across different datasets.", "output": "PEARL: Preprocessing Enhanced Adversarial Robust Learning of Image Deraining for Semantic Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent analysis of incidents involving Autonomous Driving Systems (ADS) hasshown that the decision-making process of ADS can be significantly differentfrom that of human drivers. To improve the performance of ADS, it may behelpful to incorporate the human decision-making process, particularly thesignals provided by the human gaze. There are many existing works to createhuman gaze datasets and predict the human gaze using deep learning models.However, current datasets of human gaze are noisy and include irrelevantobjects that can hinder model training. Additionally, existing CNN-based modelsfor predicting human gaze lack generalizability across different datasets anddriving conditions, and many models have a centre bias in their prediction suchthat the gaze tends to be generated in the centre of the gaze map. To addressthese gaps, we propose an adaptive method for cleansing existing human gazedatasets and a robust convolutional self-attention gaze prediction model. Ourquantitative metrics show that our cleansing method improves models'performance by up to 7.38% and generalizability by up to 8.24% compared tothose trained on the original datasets. Furthermore, our model demonstrates animprovement of up to 12.13% in terms of generalizability compared to thestate-of-the-art (SOTA) models. Notably, it achieves these gains whileconserving up to 98.12% of computation resources.", "output": "CUEING: A pioneer work of encoding human gaze for autonomous driving."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The representation gap between teacher and student is an emerging topic inknowledge distillation (KD). To reduce the gap and improve the performance,current methods often resort to complicated training schemes, loss functions,and feature alignments, which are task-specific and feature-specific. In thispaper, we state that the essence of these methods is to discard the noisyinformation and distill the valuable information in the feature, and propose anovel KD method dubbed DiffKD, to explicitly denoise and match features usingdiffusion models. Our approach is based on the observation that studentfeatures typically contain more noises than teacher features due to the smallercapacity of student model. To address this, we propose to denoise studentfeatures using a diffusion model trained by teacher features. This allows us toperform better distillation between the refined clean feature and teacherfeature. Additionally, we introduce a light-weight diffusion model with alinear autoencoder to reduce the computation cost and an adpative noisematching module to improve the denoising performance. Extensive experimentsdemonstrate that DiffKD is effective across various types of features andachieves state-of-the-art performance consistently on image classification,object detection, and semantic segmentation tasks. Code will be available at", "output": "Knowledge Diffusion for Distillation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the significant progress in six degrees-of-freedom (6DoF) object poseestimation, existing methods have limited applicability in real-world scenariosinvolving embodied agents and downstream 3D vision tasks. These limitationsmainly come from the necessity of 3D models, closed-category detection, and alarge number of densely annotated support views. To mitigate this issue, wepropose a general paradigm for object pose estimation, called Promptable ObjectPose Estimation (POPE). The proposed approach POPE enables zero-shot 6DoFobject pose estimation for any target object in any scene, while only a singlereference is adopted as the support view. To achieve this, POPE leverages thepower of the pre-trained large-scale 2D foundation model, employs a frameworkwith hierarchical feature representation and 3D geometry principles. Moreover,it estimates the relative camera pose between object prompts and the targetobject in new views, enabling both two-view and multi-view 6DoF pose estimationtasks. Comprehensive experimental results demonstrate that POPE exhibitsunrivaled robust performance in zero-shot settings, by achieving a significantreduction in the averaged Median Pose Error by 52.38% and 50.47% on the LINEMODand OnePose datasets, respectively. We also conduct more challenging testingsin causally captured images (see Figure 1), which further demonstrates therobustness of POPE. Project page can be found with", "output": "POPE: 6-DoF Promptable Pose Estimation of Any Object, in Any Scene, with One Reference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose a novel language-guided 3D arbitrary neural styletransfer method (CLIP3Dstyler). We aim at stylizing any 3D scene with anarbitrary style from a text description, and synthesizing the novel stylizedview, which is more flexible than the image-conditioned style transfer.Compared with the previous 2D method CLIPStyler, we are able to stylize a 3Dscene and generalize to novel scenes without re-train our model. Astraightforward solution is to combine previous image-conditioned 3D styletransfer and text-conditioned 2D style transfer bigskip methods. However, sucha solution cannot achieve our goal due to two main challenges. First, there isno multi-modal model matching point clouds and language at different featurescales (eg low-level, high-level). Second, we observe a style mixing issuewhen we stylize the content with different style conditions from text prompts.To address the first issue, we propose a 3D stylization framework to match thepoint cloud features with text features in local and global views. For thesecond issue, we propose an improved directional divergence loss to makearbitrary text styles more distinguishable as a complement to our framework. Weconduct extensive experiments to show the effectiveness of our model ontext-guided 3D scene style transfer.", "output": "CLIP3Dstyler: Language Guided 3D Arbitrary Neural Style Transfer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Several recent studies have elucidated why knowledge distillation (KD)improves model performance. However, few have researched the other advantagesof KD in addition to its improving model performance. In this study, we haveattempted to show that KD enhances the interpretability as well as the accuracyof models. We measured the number of concept detectors identified in networkdissection for a quantitative comparison of model interpretability. Weattributed the improvement in interpretability to the class-similarityinformation transferred from the teacher to student models. First, we confirmedthe transfer of class-similarity information from the teacher to student modelvia logit distillation. Then, we analyzed how class-similarity informationaffects model interpretability in terms of its presence or absence and degreeof similarity information. We conducted various quantitative and qualitativeexperiments and examined the results on different datasets, different KDmethods, and according to different measures of interpretability. Our researchshowed that KD models by large models could be used more reliably in variousfields.", "output": "On the Impact of Knowledge Distillation for Model Interpretability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "When virtual agents interact with humans, gestures are crucial to deliveringtheir intentions with speech. Previous multimodal co-speech gesture generationmodels required encoded features of all modalities to generate gestures. Ifsome input modalities are removed or contain noise, the model may not generatethe gestures properly. To acquire robust and generalized encodings, we proposea novel framework with a multimodal pre-trained encoder for co-speech gesturegeneration. In the proposed method, the multi-head-attention-based encoder istrained with self-supervised learning to contain the information on eachmodality. Moreover, we collect full-body gestures that consist of 3D jointrotations to improve visualization and apply gestures to the extensible bodymodel. Through the series of experiments and human evaluation, the proposedmethod renders realistic co-speech gestures not only when all input modalitiesare given but also when the input modalities are missing or noisy.", "output": "MPE4G: Multimodal Pretrained Encoder for Co-Speech Gesture Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In dyadic interaction, predicting the listener's facial reactions ischallenging as different reactions may be appropriate in response to the samespeaker's behaviour. This paper presents a novel framework called ReactFacethat learns an appropriate facial reaction distribution from a speaker'sbehaviour rather than replicating the real facial reaction of the listener.ReactFace generates multiple different but appropriate photo-realistic humanfacial reactions by (i) learning an appropriate facial reaction distributionrepresenting multiple appropriate facial reactions; and (ii) synchronizing thegenerated facial reactions with the speaker's verbal and non-verbal behavioursat each time stamp, resulting in realistic 2D facial reaction sequences.Experimental results demonstrate the effectiveness of our approach ingenerating multiple diverse, synchronized, and appropriate facial reactionsfrom each speaker's behaviour, with the quality of the generated reactionsbeing influenced by the speaker's speech and facial behaviours. Our code ismade publicly available at url{", "output": "ReactFace: Multiple Appropriate Facial Reaction Generation in Dyadic Interactions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Millimeter-wave (MMW) imaging is emerging as a promising technique for safesecurity inspection. It achieves a delicate balance between imaging resolution,penetrability and human safety, resulting in higher resolution compared tolow-frequency microwave, stronger penetrability compared to visible light, andstronger safety compared to X ray. Despite of recent advance in the lastdecades, the high cost of requisite large-scale antenna array hinderswidespread adoption of MMW imaging in practice. To tackle this challenge, wereport a large-scale single-shot MMW imaging framework using sparse antennaarray, achieving low-cost but high-fidelity security inspection under aninterpretable learning scheme. We first collected extensive full-sampled MMWechoes to study the statistical ranking of each element in the large-scalearray. These elements are then sampled based on the ranking, building theexperimentally optimal sparse sampling strategy that reduces the cost ofantenna array by up to one order of magnitude. Additionally, we derived anuntrained interpretable learning scheme, which realizes robust and accurateimage reconstruction from sparsely sampled echoes. Last, we developed a neuralnetwork for automatic object detection, and experimentally demonstratedsuccessful detection of concealed centimeter-sized targets using 10% sparsearray, whereas all the other contemporary approaches failed at the same samplesampling ratio. The performance of the reported technique presents higher than50% superiority over the existing MMW imaging schemes on various metricsincluding precision, recall, and mAP50. With such strong detection ability andorder-of-magnitude cost reduction, we anticipate that this technique provides apractical way for large-scale single-shot MMW imaging, and could advocate itsfurther practical applications.", "output": "Towards Large-scale Single-shot Millimeter-wave Imaging for Low-cost Security Inspection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, 3D models have been utilized in many applications, such asauto-driver, 3D reconstruction, VR, and AR. However, the scarcity of 3D modeldata does not meet its practical demands. Thus, generating high-quality 3Dmodels efficiently from textual descriptions is a promising but challenging wayto solve this problem. In this paper, inspired by the ability of human beingsto complement visual information details from ambiguous descriptions based ontheir own experience, we propose a novel text-3D generation model (T2TD), whichintroduces the related shapes or textual information as the prior knowledge toimprove the performance of the 3D generation model. In this process, we firstintroduce the text-3D knowledge graph to save the relationship between 3Dmodels and textual semantic information, which can provide the related shapesto guide the target 3D model generation. Second, we integrate an effectivecausal inference model to select useful feature information from these relatedshapes, which removes the unrelated shape information and only maintainsfeature information that is strongly relevant to the textual description.Meanwhile, to effectively integrate multi-modal prior knowledge into textualinformation, we adopt a novel multi-layer transformer structure toprogressively fuse related shape and textual information, which can effectivelycompensate for the lack of structural information in the text and enhance thefinal performance of the 3D generation model. The final experimental resultsdemonstrate that our approach significantly improves 3D model generationquality and outperforms the SOTA methods on the text2shape datasets.", "output": "T2TD: Text-3D Generation Model based on Prior Knowledge Guidance."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Many existing multi-modality studies are based on the assumption of modalityintegrity. However, the problem of missing arbitrary modalities is very commonin real life, and this problem is less studied, but actually important in thetask of multi-modality person re-identification (Re-ID). To this end, we designa novel dynamic enhancement network (DENet), which allows missing arbitrarymodalities while maintaining the representation ability of multiple modalities,for partial multi-modality person Re-ID. To be specific, the multi-modalrepresentation of the RGB, near-infrared (NIR) and thermal-infrared (TIR)images is learned by three branches, in which the information of missingmodalities is recovered by the feature transformation module. Since the missingstate might be changeable, we design a dynamic enhancement module, whichdynamically enhances modality features according to the missing state in anadaptive manner, to improve the multi-modality representation. Extensiveexperiments on multi-modality person Re-ID dataset RGBNT201 and vehicle Re-IDdataset RGBNT100 comparing to the state-of-the-art methods verify theeffectiveness of our method in complex and changeable environments.", "output": "Dynamic Enhancement Network for Partial Multi-modality Person Re-identification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing vehicle re-identification methods mainly rely on the single query,which has limited information for vehicle representation and thus significantlyhinders the performance of vehicle Re-ID in complicated surveillance networks.In this paper, we propose a more realistic and easily accessible task, calledmulti-query vehicle Re-ID, which leverages multiple queries to overcomeviewpoint limitation of single one. Based on this task, we make three majorcontributions. First, we design a novel viewpoint-conditioned network (VCNet),which adaptively combines the complementary information from different vehicleviewpoints, for multi-query vehicle Re-ID. Moreover, to deal with the problemof missing vehicle viewpoints, we propose a cross-view feature recovery modulewhich recovers the features of the missing viewpoints by learnt the correlationbetween the features of available and missing viewpoints. Second, we create aunified benchmark dataset, taken by 6142 cameras from a real-lifetransportation surveillance system, with comprehensive viewpoints and largenumber of crossed scenes of each vehicle for multi-query vehicle Re-IDevaluation. Finally, we design a new evaluation metric, called mean cross-sceneprecision (mCSP), which measures the ability of cross-scene recognition bysuppressing the positive samples with similar viewpoints from same camera.Comprehensive experiments validate the superiority of the proposed methodagainst other methods, as well as the effectiveness of the designed metric inthe evaluation of multi-query vehicle Re-ID.", "output": "Multi-query Vehicle Re-identification: Viewpoint-conditioned Network, Unified Dataset and New Metric."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper addresses the problem of 3D referring expression comprehension(REC) in autonomous driving scenario, which aims to ground a natural languageto the targeted region in LiDAR point clouds. Previous approaches for RECusually focus on the 2D or 3D-indoor domain, which is not suitable foraccurately predicting the location of the queried 3D region in an autonomousdriving scene. In addition, the upper-bound limitation and the heavycomputation cost motivate us to explore a better solution. In this work, wepropose a new multi-modal visual grounding task, termed LiDAR Grounding. Thenwe devise a Multi-modal Single Shot Grounding (MSSG) approach with an effectivetoken fusion strategy. It jointly learns the LiDAR-based object detector withthe language features and predicts the targeted region directly from thedetector without any post-processing. Moreover, the image feature can beflexibly integrated into our approach to provide rich texture and colorinformation. The cross-modal learning enforces the detector to concentrate onimportant regions in the point cloud by considering the informative languageexpressions, thus leading to much better accuracy and efficiency. Extensiveexperiments on the Talk2Car dataset demonstrate the effectiveness of theproposed methods. Our work offers a deeper insight into the LiDAR-basedgrounding task and we expect it presents a promising direction for theautonomous driving community.", "output": "Language-Guided 3D Object Detection in Point Cloud for Autonomous Driving."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent developments in the field of non-local attention (NLA) have led to arenewed interest in self-similarity-based single image super-resolution (SISR).Researchers usually used the NLA to explore non-local self-similarity (NSS) inSISR and achieve satisfactory reconstruction results. However, a surprisingphenomenon that the reconstruction performance of the standard NLA is similarto the NLA with randomly selected regions stimulated our interest to revisitNLA. In this paper, we first analyzed the attention map of the standard NLAfrom different perspectives and discovered that the resulting probabilitydistribution always has full support for every local feature, which implies astatistical waste of assigning values to irrelevant non-local features,especially for SISR which needs to model long-range dependence with a largenumber of redundant non-local features. Based on these findings, we introduceda concise yet effective soft thresholding operation to obtainhigh-similarity-pass attention (HSPA), which is beneficial for generating amore compact and interpretable distribution. Furthermore, we derived some keyproperties of the soft thresholding operation that enable training our HSPA inan end-to-end manner. The HSPA can be integrated into existing deep SISR modelsas an efficient general building block. In addition, to demonstrate theeffectiveness of the HSPA, we constructed a deep high-similarity-pass attentionnetwork (HSPAN) by integrating a few HSPAs in a simple backbone. Extensiveexperimental results demonstrate that HSPAN outperforms state-of-the-artapproaches on both quantitative and qualitative evaluations.", "output": "High-Similarity-Pass Attention for Single Image Super-Resolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The multi-scale information among the whole slide images (WSIs) is essentialfor cancer diagnosis. Although the existing multi-scale vision Transformer hasshown its effectiveness for learning multi-scale image representation, it stillcannot work well on the gigapixel WSIs due to their extremely large imagesizes. To this end, we propose a novel Multi-scale Efficient Graph-Transformer(MEGT) framework for WSI classification. The key idea of MEGT is to adopt twoindependent Efficient Graph-based Transformer (EGT) branches to process thelow-resolution and high-resolution patch embeddings (i.e., tokens in aTransformer) of WSIs, respectively, and then fuse these tokens via amulti-scale feature fusion module (MFFM). Specifically, we design an EGT toefficiently learn the local-global information of patch tokens, whichintegrates the graph representation into Transformer to capture spatial-relatedinformation of WSIs. Meanwhile, we propose a novel MFFM to alleviate thesemantic gap among different resolution patches during feature fusion, whichcreates a non-patch token for each branch as an agent to exchange informationwith another branch by cross-attention. In addition, to expedite networktraining, a novel token pruning module is developed in EGT to reduce theredundant tokens. Extensive experiments on TCGA-RCC and CAMELYON16 datasetsdemonstrate the effectiveness of the proposed MEGT.", "output": "Multi-scale Efficient Graph-Transformer for Whole Slide Image Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Attention mechanisms have greatly improved the performance of deep-learningmodels on visual, NLP, and multimodal tasks while also providing tools to aidin the model's interpretability. In particular, attention scores over inputregions or concrete image features can be used to measure how much the attendedelements contribute to the model inference. The recently proposed ConceptTransformer (CT) generalizes the Transformer attention mechanism from suchlow-level input features to more abstract, intermediate-level latent conceptsthat better allow human analysts to more directly assess an explanation for thereasoning of the model about any particular output classification. However, theconcept learning employed by CT implicitly assumes that across every image in aclass, each image patch makes the same contribution to concepts thatcharacterize membership in that class. Instead of using the CT'simage-patch-centric concepts, object-centric concepts could lead to betterclassification performance as well as better explainability. Thus, we proposeConcept-Centric Transformers (CCT), a new family of concept transformers thatprovides more robust explanations and performance by integrating a novelconcept-extraction module based on object-centric learning. We test ourproposed CCT against the CT and several other existing approaches onclassification problems for MNIST (odd/even), CIFAR100 (super-classes), andCUB-200-2011 (bird species). Our experiments demonstrate that CCT not onlyachieves significantly better classification accuracy than all selectedbenchmark classifiers across all three of our test problems, but it generatesmore consistent concept-based explanations of classification output whencompared to CT.", "output": "Concept-Centric Transformers: Concept Transformers with Object-Centric Concept Learning for Interpretability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Medical image data are often limited due to the expensive acquisition andannotation process. Hence, training a deep-learning model with only raw datacan easily lead to overfitting. One solution to this problem is to augment theraw data with various transformations, improving the model's ability togeneralize to new data. However, manually configuring a generic augmentationcombination and parameters for different datasets is non-trivial due toinconsistent acquisition approaches and data distributions. Therefore,automatic data augmentation is proposed to learn favorable augmentationstrategies for different datasets while incurring large GPU overhead. To thisend, we present a novel method, called Dynamic Data Augmentation (DDAug), whichis efficient and has negligible computation cost. Our DDAug develops ahierarchical tree structure to represent various augmentations and utilizes anefficient Monte-Carlo tree searching algorithm to update, prune, and sample thetree. As a result, the augmentation pipeline can be optimized for each datasetautomatically. Experiments on multiple Prostate MRI datasets show that ourmethod outperforms the current state-of-the-art data augmentation strategies.", "output": "Dynamic Data Augmentation via MCTS for Prostate MRI Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Text-to-image diffusion models can generate diverse, high-fidelity imagesbased on user-provided text prompts. Recent research has extended these modelsto support text-guided image editing. While text guidance is an intuitiveediting interface for users, it often fails to ensure the precise conceptconveyed by users. To address this issue, we propose Custom-Edit, in which we(i) customize a diffusion model with a few reference images and then (ii)perform text-guided editing. Our key discovery is that customizing onlylanguage-relevant parameters with augmented prompts improves referencesimilarity significantly while maintaining source similarity. Moreover, weprovide our recipe for each customization and editing process. We comparepopular customization methods and validate our findings on two editing methodsusing various datasets.", "output": "Custom-Edit: Text-Guided Image Editing with Customized Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The tremendous success of large models trained on extensive datasetsdemonstrates that scale is a key ingredient in achieving superior results.Therefore, the reflection on the rationality of designing knowledgedistillation (KD) approaches for limited-capacity architectures solely based onsmall-scale datasets is now deemed imperative. In this paper, we identify theemph{small data pitfall} that presents in previous KD methods, which resultsin the underestimation of the power of vanilla KD framework on large-scaledatasets such as ImageNet-1K. Specifically, we show that employing strongerdata augmentation techniques and using larger datasets can directly decreasethe gap between vanilla KD and other meticulously designed KD variants. Thishighlights the necessity of designing and evaluating KD approaches in thecontext of practical scenarios, casting off the limitations of small-scaledatasets. Our investigation of the vanilla KD and its variants in more complexschemes, including stronger training strategies and different model capacities,demonstrates that vanilla KD is elegantly simple but astonishingly effective inlarge-scale scenarios. Without bells and whistles, we obtain state-of-the-artResNet-50, ViT-S, and ConvNeXtV2-T models for ImageNet, which achieve 83.1%,84.3%, and 85.0% top-1 accuracy, respectively. PyTorch code and checkpointscan be found at ", "output": "VanillaKD: Revisit the Power of Vanilla Knowledge Distillation from Small Scale to Large Scale."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generating and editing a 3D scene guided by natural language poses achallenge, primarily due to the complexity of specifying the positionalrelations and volumetric changes within the 3D space. Recent advancements inLarge Language Models (LLMs) have demonstrated impressive reasoning,conversational, and zero-shot generation abilities across various domains.Surprisingly, these models also show great potential in realizing andinterpreting the 3D space. In light of this, we propose a novel language-guidedinteractive 3D generation system, dubbed LI3D, that integrates LLMs as a 3Dlayout interpreter into the off-the-shelf layout-to-3D generative models,allowing users to flexibly and interactively generate visual content.Specifically, we design a versatile layout structure base on the bounding boxesand semantics to prompt the LLMs to model the spatial generation and reasoningfrom language. Our system also incorporates LLaVA, a large language and visionassistant, to provide generative feedback from the visual aspect for improvingthe visual quality of generated content. We validate the effectiveness of LI3D,primarily in 3D generation and editing through multi-round interactions, whichcan be flexibly extended to 2D generation and editing. Various experimentsdemonstrate the potential benefits of incorporating LLMs in generative AI forapplications, e.g., metaverse. Moreover, we benchmark the layout reasoningperformance of LLMs with neural visual artist tasks, revealing their emergentability in the spatial layout domain.", "output": "Towards Language-guided Interactive 3D Generation: LLMs as Layout Interpreter with Generative Feedback."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Lung cancer poses a significant global public health challenge, emphasizingthe importance of early detection for improved patient outcomes. Recentadvancements in deep learning algorithms have shown promising results inmedical image analysis. This study aims to explore the application of objectdetection particularly YOLOv5, an advanced object identification system, inmedical imaging for lung cancer identification. To train and evaluate thealgorithm, a dataset comprising chest X-rays and corresponding annotations wasobtained from Kaggle. The YOLOv5 model was employed to train an algorithmcapable of detecting cancerous lung lesions. The training process involvedoptimizing hyperparameters and utilizing augmentation techniques to enhance themodel's performance. The trained YOLOv5 model exhibited exceptional proficiencyin identifying lung cancer lesions, displaying high accuracy and recall rates.It successfully pinpointed malignant areas in chest radiographs, as validatedby a separate test set where it outperformed previous techniques. Additionally,the YOLOv5 model demonstrated computational efficiency, enabling real-timedetection and making it suitable for integration into clinical procedures. Thisproposed approach holds promise in assisting radiologists in the earlydiscovery and diagnosis of lung cancer, ultimately leading to prompt treatmentand improved patient outcomes.", "output": "Leveraging object detection for the identification of lung cancer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pseudo-labels are widely employed in weakly supervised 3D segmentation taskswhere only sparse ground-truth labels are available for learning. Existingmethods often rely on empirical label selection strategies, such as confidencethresholding, to generate beneficial pseudo-labels for model training. Thisapproach may, however, hinder the comprehensive exploitation of unlabeled datapoints. We hypothesize that this selective usage arises from the noise inpseudo-labels generated on unlabeled data. The noise in pseudo-labels mayresult in significant discrepancies between pseudo-labels and modelpredictions, thus confusing and affecting the model training greatly. Toaddress this issue, we propose a novel learning strategy to regularize thegenerated pseudo-labels and effectively narrow the gaps between pseudo-labelsand model predictions. More specifically, our method introduces an EntropyRegularization loss and a Distribution Alignment loss for weakly supervisedlearning in 3D segmentation tasks, resulting in an ERDA learning strategy.Interestingly, by using KL distance to formulate the distribution alignmentloss, it reduces to a deceptively simple cross-entropy-based loss whichoptimizes both the pseudo-label generation network and the 3D segmentationnetwork simultaneously. Despite the simplicity, our method promisingly improvesthe performance. We validate the effectiveness through extensive experiments onvarious baselines and large-scale datasets. Results show that ERDA effectivelyenables the effective usage of all unlabeled data points for learning andachieves state-of-the-art performance under different settings. Remarkably, ourmethod can outperform fully-supervised baselines using only 1% of trueannotations. Code and model will be made publicly available.", "output": "All Points Matter: Entropy-Regularized Distribution Alignment for Weakly-supervised 3D Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Architectures that first convert point clouds to a grid representation andthen apply convolutional neural networks achieve good performance forradar-based object detection. However, the transfer from irregular point clouddata to a dense grid structure is often associated with a loss of information,due to the discretization and aggregation of points. In this paper, we proposea novel architecture, multi-scale KPPillarsBEV, that aims to mitigate thenegative effects of grid rendering. Specifically, we propose a novel gridrendering method, KPBEV, which leverages the descriptive power of kernel pointconvolutions to improve the encoding of local point cloud contexts during gridrendering. In addition, we propose a general multi-scale grid renderingformulation to incorporate multi-scale feature maps into convolutionalbackbones of detection networks with arbitrary grid rendering methods. Weperform extensive experiments on the nuScenes dataset and evaluate the methodsin terms of detection performance and computational complexity. The proposedmulti-scale KPPillarsBEV architecture outperforms the baseline by 5.37% and theprevious state of the art by 2.88% in Car AP4.0 (average precision for amatching threshold of 4 meters) on the nuScenes validation set. Moreover, theproposed single-scale KPBEV grid rendering improves the Car AP4.0 by 2.90% overthe baseline while maintaining the same inference speed.", "output": "Improved Multi-Scale Grid Rendering of Point Clouds for Radar Object Detection Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Due to recent advances in pose-estimation methods, human motion can beextracted from a common video in the form of 3D skeleton sequences. Despitewonderful application opportunities, effective and efficient content-basedaccess to large volumes of such spatio-temporal skeleton data still remains achallenging problem. In this paper, we propose a novel content-basedtext-to-motion retrieval task, which aims at retrieving relevant motions basedon a specified natural-language textual description. To define baselines forthis uncharted task, we employ the BERT and CLIP language representations toencode the text modality and successful spatio-temporal models to encode themotion modality. We additionally introduce our transformer-based approach,called Motion Transformer (MoT), which employs divided space-time attention toeffectively aggregate the different skeleton joints in space and time. Inspiredby the recent progress in text-to-image/video matching, we experiment with twowidely-adopted metric-learning loss functions. Finally, we set up a commonevaluation protocol by defining qualitative metrics for assessing the qualityof the retrieved motions, targeting the two recently-introduced KITMotion-Language and HumanML3D datasets. The code for reproducing our results isavailable at ", "output": "Text-to-Motion Retrieval: Towards Joint Understanding of Human Motion Data and Natural Language."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Image fusion plays a key role in a variety of multi-sensor-based visionsystems, especially for enhancing visual quality and/or extracting aggregatedfeatures for perception. However, most existing methods just consider imagefusion as an individual task, thus ignoring its underlying relationship withthese downstream vision problems. Furthermore, designing proper fusionarchitectures often requires huge engineering labor. It also lacks mechanismsto improve the flexibility and generalization ability of current fusionapproaches. To mitigate these issues, we establish a Task-guided,Implicit-searched and Meta-initialized (TIM) deep model to address the imagefusion problem in a challenging real-world scenario. Specifically, we firstpropose a constrained strategy to incorporate information from downstream tasksto guide the unsupervised learning process of image fusion. Within thisframework, we then design an implicit search scheme to automatically discovercompact architectures for our fusion model with high efficiency. In addition, apretext meta initialization technique is introduced to leverage divergencefusion data to support fast adaptation for different kinds of image fusiontasks. Qualitative and quantitative experimental results on differentcategories of image fusion problems and related downstream tasks (e.g., visualenhancement and semantic understanding) substantiate the flexibility andeffectiveness of our TIM. The source code will be available at", "output": "A Task-guided, Implicitly-searched and Meta-initialized Deep Model for Image Fusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Addressing accuracy limitations and pose ambiguity in 6D object poseestimation from single RGB images presents a significant challenge,particularly due to object symmetries or occlusions. In response, we introducea novel score-based diffusion method applied to the $SE(3)$ group, marking thefirst application of diffusion models to $SE(3)$ within the image domain,specifically tailored for pose estimation tasks. Extensive evaluationsdemonstrate the method's efficacy in handling pose ambiguity, mitigatingperspective-induced ambiguity, and showcasing the robustness of our surrogateStein score formulation on $SE(3)$. This formulation not only improves theconvergence of Langevin dynamics but also enhances computational efficiency.Thus, we pioneer a promising strategy for 6D object pose estimation.", "output": "Confronting Ambiguity in 6D Object Pose Estimation via Score-Based Diffusion on SE(3)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Radars and cameras belong to the most frequently used sensors for advanceddriver assistance systems and automated driving research. However, there hasbeen surprisingly little research on radar-camera fusion with neural networks.One of the reasons is a lack of large-scale automotive datasets with radar andunmasked camera data, with the exception of the nuScenes dataset. Anotherreason is the difficulty of effectively fusing the sparse radar point cloud onthe bird's eye view (BEV) plane with the dense images on the perspective plane.The recent trend of camera-based 3D object detection using BEV features hasenabled a new type of fusion, which is better suited for radars. In this work,we present RC-BEVFusion, a modular radar-camera fusion network on the BEVplane. We propose BEVFeatureNet, a novel radar encoder branch, and show that itcan be incorporated into several state-of-the-art camera-based architectures.We show significant performance gains of up to 28% increase in the nuScenesdetection score, which is an important step in radar-camera fusion research.Without tuning our model for the nuScenes benchmark, we achieve the best resultamong all published methods in the radar-camera fusion category.", "output": "RC-BEVFusion: A Plug-In Module for Radar-Camera Bird's Eye View Feature Fusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Low-dose computed tomography (CT) image denoising is crucial in medical imagecomputing. Recent years have been remarkable improvement in deep learning-basedmethods for this task. However, training deep denoising neural networksrequires low-dose and normal-dose CT image pairs, which are difficult to obtainin the clinic settings. To address this challenge, we propose a novel fullyunsupervised method for low-dose CT image denoising, which is based ondenoising diffusion probabilistic model -- a powerful generative model. First,we train an unconditional denoising diffusion probabilistic model capable ofgenerating high-quality normal-dose CT images from random noise. Subsequently,the probabilistic priors of the pre-trained diffusion model are incorporatedinto a Maximum A Posteriori (MAP) estimation framework for iteratively solvingthe image denoising problem. Our method ensures the diffusion model produceshigh-quality normal-dose CT images while keeping the image content consistentwith the input low-dose CT images. We evaluate our method on a widely usedlow-dose CT image denoising benchmark, and it outperforms several supervisedlow-dose CT image denoising methods in terms of both quantitative and visualperformance.", "output": "A Diffusion Probabilistic Prior for Low-Dose CT Image Denoising."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transformer-based trackers have achieved strong accuracy on the standardbenchmarks. However, their efficiency remains an obstacle to practicaldeployment on both GPU and CPU platforms. In this paper, to overcome thisissue, we propose a fully transformer tracking framework, coined asemph{MixFormerV2}, without any dense convolutional operation and complex scoreprediction module. Our key design is to introduce four special predictiontokens and concatenate them with the tokens from target template and searchareas. Then, we apply the unified transformer backbone on these mixed tokensequence. These prediction tokens are able to capture the complex correlationbetween target template and search area via mixed attentions. Based on them, wecan easily predict the tracking box and estimate its confidence score throughsimple MLP heads. To further improve the efficiency of MixFormerV2, we presenta new distillation-based model reduction paradigm, including dense-to-sparsedistillation and deep-to-shallow distillation. The former one aims to transferknowledge from the dense-head based MixViT to our fully transformer tracker,while the latter one is used to prune some layers of the backbone. Weinstantiate two types of MixForemrV2, where the MixFormerV2-B achieves an AUCof 70.6% on LaSOT and an AUC of 57.4% on TNL2k with a high GPU speed of 165FPS, and the MixFormerV2-S surpasses FEAR-L by 2.7% AUC on LaSOT with areal-time CPU speed.", "output": "MixFormerV2: Efficient Fully Transformer Tracking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Object Re-identification (ReID) aims to retrieve the probe object from manygallery images with the ReID model inferred based on a stationary camera-freedataset by associating and collecting the identities across all camera views.When deploying the ReID algorithm in real-world scenarios, the aspect ofstorage, privacy constraints, and dynamic changes of cameras would degrade itsgeneralizability and applicability. Treating each camera's data independently,we introduce a novel ReID task named Camera-Incremental ObjectRe-identification (CIOR) by continually optimizing the ReID mode from theincoming stream of the camera dataset. Since the identities under differentcamera views might describe the same object, associating and distilling theknowledge of common identities would boost the discrimination and benefit fromalleviating the catastrophic forgetting. In this paper, we propose a novelIdentity Knowledge Evolution (IKE) framework for CIOR, consisting of theIdentity Knowledge Association (IKA), Identity Knowledge Distillation (IKD),and Identity Knowledge Update (IKU). IKA is proposed to discover the commonidentities between the current identity and historical identities. IKD hasapplied to distillate historical identity knowledge from common identities andquickly adapt the historical model to the current camera view. After eachcamera has been trained, IKU is applied to continually expand the identityknowledge by combining the historical and current identity memories. Theevaluation of Market-CL and Veri-CL shows the Identity Knowledge Evolution(IKE) effectiveness for CIOR.code:", "output": "Camera-Incremental Object Re-Identification with Identity Knowledge Evolution."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Convolutional neural networks (CNN) and Transformer variants have emerged asthe leading medical image segmentation backbones. Nonetheless, due to theirlimitations in either preserving global image context or efficiently processingirregular shapes in visual objects, these backbones struggle to effectivelyintegrate information from diverse anatomical regions and reduceinter-individual variability, particularly for the vasculature. Motivated bythe successful breakthroughs of graph neural networks (GNN) in capturingtopological properties and non-Euclidean relationships across various fields,we propose NexToU, a novel hybrid architecture for medical image segmentation.NexToU comprises improved Pool GNN and Swin GNN modules from Vision GNN (ViG)for learning both global and local topological representations while minimizingcomputational costs. To address the containment and exclusion relationshipsamong various anatomical structures, we reformulate the topological interaction(TI) module based on the nature of binary trees, rapidly encoding thetopological constraints into NexToU. Extensive experiments conducted on threedatasets (including distinct imaging dimensions, disease types, and imagingmodalities) demonstrate that our method consistently outperforms otherstate-of-the-art (SOTA) architectures. All the code is publicly available at", "output": "NexToU: Efficient Topology-Aware U-Net for Medical Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Detecting 3D mask attacks to a face recognition system is challenging.Although genuine faces and 3D face masks show significantly different remotephotoplethysmography (rPPG) signals, rPPG-based face anti-spoofing methodsoften suffer from performance degradation due to unstable face alignment in thevideo sequence and weak rPPG signals. To enhance the rPPG signal in amotion-robust way, a landmark-anchored face stitching method is proposed toalign the faces robustly and precisely at the pixel-wise level by using bothSIFT keypoints and facial landmarks. To better encode the rPPG signal, aweighted spatial-temporal representation is proposed, which emphasizes the faceregions with rich blood vessels. In addition, characteristics of rPPG signalsin different color spaces are jointly utilized. To improve the generalizationcapability, a lightweight EfficientNet with a Gated Recurrent Unit (GRU) isdesigned to extract both spatial and temporal features from the rPPGspatial-temporal representation for classification. The proposed method iscompared with the state-of-the-art methods on five benchmark datasets underboth intra-dataset and cross-dataset evaluations. The proposed method shows asignificant and consistent improvement in performance over otherstate-of-the-art rPPG-based methods for face spoofing detection.", "output": "Mask Attack Detection Using Vascular-weighted Motion-robust rPPG Signals."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ability to anticipate pedestrian motion changes is a critical capabilityfor autonomous vehicles. In urban environments, pedestrians may enter the roadarea and create a high risk for driving, and it is important to identify thesecases. Typical predictors use the trajectory history to predict future motion,however in cases of motion initiation, motion in the trajectory may only beclearly visible after a delay, which can result in the pedestrian has enteredthe road area before an accurate prediction can be made. Appearance dataincludes useful information such as changes of gait, which are early indicatorsof motion changes, and can inform trajectory prediction. This work presents acomparative evaluation of trajectory-only and appearance-based methods forpedestrian prediction, and introduces a new dataset experiment for predictionusing appearance. We create two trajectory and image datasets based on thecombination of image and trajectory sequences from the popular NuScenesdataset, and examine prediction of trajectories using observed appearance toinfluence futures. This shows some advantages over trajectory prediction alone,although problems with the dataset prevent advantages of appearance-basedmodels from being shown. We describe methods for improving the dataset andexperiment to allow benefits of appearance-based models to be captured.", "output": "Comparison of Pedestrian Prediction Models from Trajectory and Appearance Data for Autonomous Driving."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reconstruction-based methods have struggled to achieve competitiveperformance on anomaly detection. In this paper, we introduce DenoisingDiffusion Anomaly Detection (DDAD). We propose a novel denoising process forimage reconstruction conditioned on a target image. This results in a coherentrestoration that closely resembles the target image. Subsequently, our anomalydetection framework leverages this conditioning where the target image is setas the input image to guide the denoising process, leading to defectlessreconstruction while maintaining nominal patterns. We localise anomalies via apixel-wise and feature-wise comparison of the input and reconstructed image.Finally, to enhance the effectiveness of feature comparison, we introduce adomain adaptation method that utilises generated examples from our conditioneddenoising process to fine-tune the feature extractor. The veracity of theapproach is demonstrated on various datasets including MVTec and VisAbenchmarks, achieving state-of-the-art results of 99.5% and 99.3% image-levelAUROC respectively.", "output": "Anomaly Detection with Conditioned Denoising Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large pre-trained models have had a significant impact on computer vision byenabling multi-modal learning, where the CLIP model has achieved impressiveresults in image classification, object detection, and semantic segmentation.However, the model's performance on 3D point cloud processing tasks is limiteddue to the domain gap between depth maps from 3D projection and training imagesof CLIP. This paper proposes DiffCLIP, a new pre-training framework thatincorporates stable diffusion with ControlNet to minimize the domain gap in thevisual branch. Additionally, a style-prompt generation module is introduced forfew-shot tasks in the textual branch. Extensive experiments on the ModelNet10,ModelNet40, and ScanObjectNN datasets show that DiffCLIP has strong abilitiesfor 3D understanding. By using stable diffusion and style-prompt generation,DiffCLIP achieves an accuracy of 43.2% for zero-shot classification on OBJ_BGof ScanObjectNN, which is state-of-the-art performance, and an accuracy of80.6% for zero-shot classification on ModelNet10, which is comparable tostate-of-the-art performance.", "output": "DiffCLIP: Leveraging Stable Diffusion for Language Grounded 3D Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The potential of integrating Computer-Assisted Diagnosis (CAD) with LargeLanguage Models (LLMs) in clinical applications, particularly in digital familydoctor and clinic assistant roles, shows promise. However, existing works havelimitations in terms of reliability, effectiveness, and their narrowapplicability to specific image domains, which restricts their overallprocessing capabilities. Moreover, the mismatch in writing style between LLMsand radiologists undermines their practical utility. To address thesechallenges, we present ChatCAD+, an interactive CAD system that is universal,reliable, and capable of handling medical images from diverse domains. ChatCAD+utilizes current information obtained from reputable medical websites to offerprecise medical advice. Additionally, it incorporates a template retrievalsystem that emulates real-world diagnostic reporting, thereby improving itsseamless integration into existing clinical workflows. The source code isavailable at href{ The onlinedemo will be available soon.", "output": "ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In Knowledge Distillation, the teacher is generally much larger than thestudent, making the solution of the teacher likely to be difficult for thestudent to learn. To ease the mimicking difficulty, we introduce a tripletknowledge distillation mechanism named TriKD. Besides teacher and student,TriKD employs a third role called anchor model. Before distillation begins, thepre-trained anchor model delimits a subspace within the full solution space ofthe target problem. Solutions within the subspace are expected to be easytargets that the student could mimic well. Distillation then begins in anonline manner, and the teacher is only allowed to express solutions within theaforementioned subspace. Surprisingly, benefiting from accurate buteasy-to-mimic hints, the student can finally perform well. After the student iswell trained, it can be used as the new anchor for new students, forming acurriculum learning strategy. Our experiments on image classification and facerecognition with various models clearly demonstrate the effectiveness of ourmethod. Furthermore, the proposed TriKD is also effective in dealing with theoverfitting issue. Moreover, our theoretical analysis supports the rationalityof our triplet distillation.", "output": "Triplet Knowledge Distillation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, there have been a plethora of classification and detection systemsfrom RGB as well as 3D images. In this work, we describe a new 3D objectdetection system from an RGB-D or depth-only point cloud. Our system firstdetects objects in 2D (either RGB or pseudo-RGB constructed from depth). Thenext step is to detect 3D objects within the 3D frustums these 2D detectionsdefine. This is achieved by voxelizing parts of the frustums (since frustumscan be really large), instead of using the whole frustums as done in earlierwork. The main novelty of our system has to do with determining which parts (3Dproposals) of the frustums to voxelize, thus allowing us to provide highresolution representations around the objects of interest. It also allows oursystem to have reduced memory requirements. These 3D proposals are fed to anefficient ResNet-based 3D Fully Convolutional Network (FCN). Our 3D detectionsystem is fast and can be integrated into a robotics platform. With respect tosystems that do not perform voxelization (such as PointNet), our methods canoperate without the requirement of subsampling of the datasets. We have alsointroduced a pipelining approach that further improves the efficiency of oursystem. Results on SUN RGB-D dataset show that our system, which is based on asmall network, can process 20 frames per second with comparable detectionresults to the state-of-the-art, achieving a 2 times speedup.", "output": "Frustum VoxNet for 3D object detection from RGB-D or Depth images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Domain generalization (DG) aims to learn from multiple source domains a modelthat can generalize well on unseen target domains. Existing DG methods mainlylearn the representations with invariant marginal distribution of the inputfeatures, however, the invariance of the conditional distribution of the labelsgiven the input features is more essential for unknown domain prediction.Meanwhile, the existing of unobserved confounders which affect the inputfeatures and labels simultaneously cause spurious correlation and hinder thelearning of the invariant relationship contained in the conditionaldistribution. Interestingly, with a causal view on the data generating process,we find that the input features of one domain are valid instrumental variablesfor other domains. Inspired by this finding, we propose an instrumentalvariable-driven DG method (IV-DG) by removing the bias of the unobservedconfounders with two-stage learning. In the first stage, it learns theconditional distribution of the input features of one domain given inputfeatures of another domain. In the second stage, it estimates the relationshipby predicting labels with the learned conditional distribution. Theoreticalanalyses and simulation experiments show that it accurately captures theinvariant relationship. Extensive experiments on real-world datasetsdemonstrate that IV-DG method yields state-of-the-art results.", "output": "Instrumental Variable-Driven Domain Generalization with Unobserved Confounders."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Domain generalization (DG) aims to learn from multiple known source domains amodel that can generalize well to unknown target domains. The existing DGmethods usually exploit the fusion of shared multi-source data to train ageneralizable model. However, tremendous data is distributed across lots ofplaces nowadays that can not be shared due to privacy policies. In this paper,we tackle the problem of federated domain generalization where the sourcedatasets can only be accessed and learned locally for privacy protection. Wepropose a novel framework called Collaborative Semantic Aggregation andCalibration (CSAC) to enable this challenging problem. To fully absorbmulti-source semantic information while avoiding unsafe data fusion, we conductdata-free semantic aggregation by fusing the models trained on the separateddomains layer-by-layer. To address the semantic dislocation problem caused bydomain shift, we further design cross-layer semantic calibration with anattention mechanism to align each semantic level and enhance domain invariance.We unify multi-source semantic learning and alignment in a collaborative way byrepeating the semantic aggregation and calibration alternately, keeping eachdataset localized, and the data privacy is carefully protected. Extensiveexperiments show the significant performance of our method in addressing thischallenging problem.", "output": "Collaborative Semantic Aggregation and Calibration for Federated Domain Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Arrays of quantum dots (QDs) are a promising candidate system to realizescalable, coupled qubit systems and serve as a fundamental building block forquantum computers. In such semiconductor quantum systems, devices now have tensof individual electrostatic and dynamical voltages that must be carefully setto localize the system into the single-electron regime and to realize goodqubit operational performance. The mapping of requisite QD locations andcharges to gate voltages presents a challenging classical control problem. Withan increasing number of QD qubits, the relevant parameter space growssufficiently to make heuristic control unfeasible. In recent years, there hasbeen considerable effort to automate device control that combines script-basedalgorithms with machine learning (ML) techniques. In this Colloquium, acomprehensive overview of the recent progress in the automation of QD devicecontrol is presented, with a particular emphasis on silicon- and GaAs-based QDsformed in two-dimensional electron gases. Combining physics-based modeling withmodern numerical optimization and ML has proven effective in yieldingefficient, scalable control. Further integration of theoretical, computational,and experimental efforts with computer science and ML holds vast potential inadvancing semiconductor and other platforms for quantum computing.", "output": "Colloquium: Advances in automation of quantum dot devices control."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowledge distillation aims to enhance the performance of a lightweightstudent model by exploiting the knowledge from a pre-trained cumbersome teachermodel. However, in the traditional knowledge distillation, teacher predictionsare only used to provide the supervisory signal for the last layer of thestudent model, which may result in those shallow student layers lackingaccurate training guidance in the layer-by-layer back propagation and thushinders effective knowledge transfer. To address this issue, we proposeDeeply-Supervised Knowledge Distillation (DSKD), which fully utilizes classpredictions and feature maps of the teacher model to supervise the training ofshallow student layers. A loss-based weight allocation strategy is developed inDSKD to adaptively balance the learning process of each shallow layer, so as tofurther improve the student performance. Extensive experiments on CIFAR-100 andTinyImageNet with various teacher-student models show significantlyperformance, confirming the effectiveness of our proposed method. Code isavailable at:$href{", "output": "Knowledge Distillation with Deep Supervision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Light field (LF) cameras record both intensity and directions of light rays,and encode 3D scenes into 4D LF images. Recently, many convolutional neuralnetworks (CNNs) have been proposed for various LF image processing tasks.However, it is challenging for CNNs to effectively process LF images since thespatial and angular information are highly inter-twined with varyingdisparities. In this paper, we propose a generic mechanism to disentangle thesecoupled information for LF image processing. Specifically, we first design aclass of domain-specific convolutions to disentangle LFs from differentdimensions, and then leverage these disentangled features by designingtask-specific modules. Our disentangling mechanism can well incorporate the LFstructure prior and effectively handle 4D LF data. Based on the proposedmechanism, we develop three networks (i.e., DistgSSR, DistgASR and DistgDisp)for spatial super-resolution, angular super-resolution and disparityestimation. Experimental results show that our networks achievestate-of-the-art performance on all these three tasks, which demonstrates theeffectiveness, efficiency, and generality of our disentangling mechanism.Project page: ", "output": "Disentangling Light Fields for Super-Resolution and Disparity Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Most modern imaging systems incorporate a computational pipeline to infer theimage of interest from acquired measurements. The Bayesian approach to solvesuch ill-posed inverse problems involves the characterization of the posteriordistribution of the image. It depends on the model of the imaging system and onprior knowledge on the image of interest. In this work, we present a Bayesianreconstruction framework for nonlinear imaging models where we specify theprior knowledge on the image through a deep generative model. We develop atractable posterior-sampling scheme based on the Metropolis-adjusted Langevinalgorithm for the class of nonlinear inverse problems where the forward modelhas a neural-network-like structure. This class includes most practical imagingmodalities. We introduce the notion of augmented deep generative priors inorder to suitably handle the recovery of quantitative images.We illustrate theadvantages of our framework by applying it to two nonlinear imagingmodalities-phase retrieval and optical diffraction tomography.", "output": "Bayesian Inversion for Nonlinear Imaging Models using Deep Generative Priors."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "State-of-the-art deep learning models have a parameter count that reachesinto the billions. Training, storing and transferring such models is energy andtime consuming, thus costly. A big part of these costs is caused by trainingthe network. Model compression lowers storage and transfer costs, and canfurther make training more efficient by decreasing the number of computationsin the forward and/or backward pass. Thus, compressing networks also attraining time while maintaining a high performance is an important researchtopic. This work is a survey on methods which reduce the number of trainedweights in deep learning models throughout the training. Most of the introducedmethods set network parameters to zero which is called pruning. The presentedpruning approaches are categorized into pruning at initialization, lotterytickets and dynamic sparse training. Moreover, we discuss methods that freezeparts of a network at its random initialization. By freezing weights, thenumber of trainable parameters is shrunken which reduces gradient computationsand the dimensionality of the model's optimization space. In this survey wefirst propose dimensionality reduced training as an underlying mathematicalmodel that covers pruning and freezing during training. Afterwards, we presentand discuss different dimensionality reduced training methods.", "output": "Dimensionality Reduced Training by Pruning and Freezing Parts of a Deep Neural Network, a Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "License plate recognition plays a critical role in many practicalapplications, but license plates of large vehicles are difficult to berecognized due to the factors of low resolution, contamination, lowillumination, and occlusion, to name a few. To overcome the above factors, thetransportation management department generally introduces the enlarged licenseplate behind the rear of a vehicle. However, enlarged license plates have highdiversity as they are non-standard in position, size, and style. Furthermore,the background regions contain a variety of noisy information which greatlydisturbs the recognition of license plate characters. Existing works have notstudied this challenging problem. In this work, we first address the enlargedlicense plate recognition problem and contribute a dataset containing 9342images, which cover most of the challenges of real scenes. However, the createddata are still insufficient to train deep methods of enlarged license platerecognition, and building large-scale training data is very time-consuming andhigh labor cost. To handle this problem, we propose a novel task-leveldisentanglement generation framework based on the Disentangled GenerationNetwork (DGNet), which disentangles the generation into the text generation andbackground generation in an end-to-end manner to effectively ensure diversityand integrity, for robust enlarged license plate recognition. Extensiveexperiments on the created dataset are conducted, and we demonstrate theeffectiveness of the proposed approach in three representative text recognitionframeworks.", "output": "Disentangled Generation Network for Enlarged License Plate Recognition and A Unified Dataset."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning is an emerging paradigm allowing large-scale decentralizedlearning without sharing data across different data owners, which helps addressthe concern of data privacy in medical image analysis. However, the requirementfor label consistency across clients by the existing methods largely narrowsits application scope. In practice, each clinical site may only annotatecertain organs of interest with partial or no overlap with other sites.Incorporating such partially labeled data into a unified federation is anunexplored problem with clinical significance and urgency. This work tacklesthe challenge by using a novel federated multi-encoding U-Net (Fed-MENU) methodfor multi-organ segmentation. In our method, a multi-encoding U-Net (MENU-Net)is proposed to extract organ-specific features through different encodingsub-networks. Each sub-network can be seen as an expert of a specific organ andtrained for that client. Moreover, to encourage the organ-specific featuresextracted by different sub-networks to be informative and distinctive, weregularize the training of the MENU-Net by designing an auxiliary genericdecoder (AGD). Extensive experiments on six public abdominal CT datasets showthat our Fed-MENU method can effectively obtain a federated learning modelusing the partially labeled datasets with superior performance to other modelstrained by either localized or centralized learning methods. Source code ispublicly available at ", "output": "Federated Multi-organ Segmentation with Inconsistent Labels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Predicting whether subjects with mild cognitive impairment (MCI) will convertto Alzheimer's disease is a significant clinical challenge. Longitudinalvariations and complementary information inherent in longitudinal andmultimodal data are crucial for MCI conversion prediction, but persistent issueof missing data in these data may hinder their effective application.Additionally, conversion prediction should be achieved in the early stages ofdisease progression in clinical practice, specifically at baseline visit (BL).Therefore, longitudinal data should only be incorporated during training tocapture disease progression information. To address these challenges, amulti-view imputation and cross-attention network (MCNet) was proposed tointegrate data imputation and MCI conversion prediction in a unified framework.First, a multi-view imputation method combined with adversarial learning waspresented to handle various missing data scenarios and reduce imputationerrors. Second, two cross-attention blocks were introduced to exploit thepotential associations in longitudinal and multimodal data. Finally, amulti-task learning model was established for data imputation, longitudinalclassification, and conversion prediction tasks. When the model wasappropriately trained, the disease progression information learned fromlongitudinal data can be leveraged by BL data to improve MCI conversionprediction at BL. MCNet was tested on two independent testing sets andsingle-modal BL data to verify its effectiveness and flexibility in MCIconversion prediction. Results showed that MCNet outperformed severalcompetitive methods. Moreover, the interpretability of MCNet was demonstrated.Thus, our MCNet may be a valuable tool in longitudinal and multimodal dataanalysis for MCI conversion prediction. Codes are available at", "output": "Multi-View Imputation and Cross-Attention Network Based on Incomplete Longitudinal and Multimodal Data for Conversion Prediction of Mild Cognitive Impairment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Interest in understanding and factorizing learned embedding spaces throughconceptual explanations is steadily growing. When no human concept labels areavailable, concept discovery methods search trained embedding spaces forinterpretable concepts like object shape or color that can be used to providepost-hoc explanations for decisions. Unlike previous work, we argue thatconcept discovery should be identifiable, meaning that a number of knownconcepts can be provably recovered to guarantee reliability of theexplanations. As a starting point, we explicitly make the connection betweenconcept discovery and classical methods like Principal Component Analysis andIndependent Component Analysis by showing that they can recover independentconcepts with non-Gaussian distributions. For dependent concepts, we proposetwo novel approaches that exploit functional compositionality properties ofimage-generating processes. Our provably identifiable concept discovery methodssubstantially outperform competitors on a battery of experiments includinghundreds of trained models and dependent concepts, where they exhibit up to 29% better alignment with the ground truth. Our results provide a rigorousfoundation for reliable concept discovery without human labels.", "output": "When are Post-hoc Conceptual Explanations Identifiable?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In 3D action recognition, there exists rich complementary information betweenskeleton modalities. Nevertheless, how to model and utilize this informationremains a challenging problem for self-supervised 3D action representationlearning. In this work, we formulate the cross-modal interaction as abidirectional knowledge distillation problem. Different from classicdistillation solutions that transfer the knowledge of a fixed and pre-trainedteacher to the student, in this work, the knowledge is continuously updated andbidirectionally distilled between modalities. To this end, we propose a newCross-modal Mutual Distillation (CMD) framework with the following designs. Onthe one hand, the neighboring similarity distribution is introduced to modelthe knowledge learned in each modality, where the relational information isnaturally suitable for the contrastive frameworks. On the other hand,asymmetrical configurations are used for teacher and student to stabilize thedistillation process and to transfer high-confidence information betweenmodalities. By derivation, we find that the cross-modal positive mining inprevious works can be regarded as a degenerated version of our CMD. We performextensive experiments on NTU RGB+D 60, NTU RGB+D 120, and PKU-MMD II datasets.Our approach outperforms existing self-supervised methods and sets a series ofnew records. The code is available at: ", "output": "CMD: Self-supervised 3D Action Representation Learning with Cross-modal Mutual Distillation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Affordance grounding aims to locate objects' \"action possibilities\" regions,which is an essential step toward embodied intelligence. Due to the diversityof interactive affordance, the uniqueness of different individuals leads todiverse interactions, which makes it difficult to establish an explicit linkbetween object parts and affordance labels. Human has the ability thattransforms the various exocentric interactions into invariant egocentricaffordance to counter the impact of interactive diversity. To empower an agentwith such ability, this paper proposes a task of affordance grounding fromexocentric view, i.e., given exocentric human-object interaction and egocentricobject images, learning the affordance knowledge of the object and transferringit to the egocentric image using only the affordance label as supervision.However, there is some \"interaction bias\" between personas, mainly regardingdifferent regions and different views. To this end, we devise a cross-viewaffordance knowledge transfer framework that extracts affordance-specificfeatures from exocentric interactions and transfers them to the egocentricview. Specifically, the perception of affordance regions is enhanced bypreserving affordance co-relations. In addition, an affordance groundingdataset named AGD20K is constructed by collecting and labeling over 20K imagesfrom $36$ affordance categories. Experimental results demonstrate that ourmethod outperforms the representative models regarding objective metrics andvisual quality. Code is released at", "output": "Grounded Affordance from Exocentric View."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning-enabled medical imaging analysis has become a vital part ofthe current automatic diagnosis system. However, machine learning models havebeen shown to demonstrate a systematic bias towards certain subgroups ofpeople, e.g., giving a worse predictive performance to old females. It isharmful and dangerous in such a sensitive area and therefore researchers havebeen working on developing bias mitigation algorithms to address the fairnessissue in the general machine learning field. However, given the specificcharacteristics of medical imaging, fairness in medical image analysis (MedIA)requires additional efforts. Hence, in this survey, we give a comprehensivereview of the current progress of fairness study and that in MedIA.Specifically, we first discuss the definitions of fairness and analyze thesource of bias in medical imaging. Then, we discuss current research onfairness for MedIA and present a collection of public medical imaging datasetsthat can be used for evaluating fairness in MedIA. Furthermore, we conductextensive experiments to evaluate the fairness of several different tasks formedical imaging, including classification, object detection, and landmarkdetection. Finally, we discuss the challenges and potential future directionsin developing fair MedIA.", "output": "Progress and Prospects for Fairness in Healthcare and Medical Image Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper explores the task of Temporal Video Grounding (TVG) where, givenan untrimmed video and a natural language sentence query, the goal is torecognize and determine temporal boundaries of action instances in the videodescribed by the query. Recent works tackled this task by improving queryinputs with large pre-trained language models (PLM) at the cost of moreexpensive training. However, the effects of this integration are unclear, asthese works also propose improvements in the visual inputs. Therefore, thispaper studies the effects of PLMs in TVG and assesses the applicability ofparameter-efficient training with NLP adapters. We couple popular PLMs with aselection of existing approaches and test different adapters to reduce theimpact of the additional parameters. Our results on three challenging datasetsshow that, without changing the visual inputs, TVG models greatly benefitedfrom the PLM integration and fine-tuning, stressing the importance of sentencequery representation in this task. Furthermore, NLP adapters were an effectivealternative to full fine-tuning, even though they were not tailored to ourtask, allowing PLM integration in larger TVG models and delivering resultscomparable to SOTA models. Finally, our results shed light on which adapterswork best in different scenarios.", "output": "Towards Parameter-Efficient Integration of Pre-Trained Language Models In Temporal Video Grounding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Lung segmentation in chest X-rays (CXRs) is an important prerequisite forimproving the specificity of diagnoses of cardiopulmonary diseases in aclinical decision support system. Current deep learning models for lungsegmentation are trained and evaluated on CXR datasets in which theradiographic projections are captured predominantly from the adult population.However, the shape of the lungs is reported to be significantly differentacross the developmental stages from infancy to adulthood. This might result inage-related data domain shifts that would adversely impact lung segmentationperformance when the models trained on the adult population are deployed forpediatric lung segmentation. In this work, our goal is to (i) analyze thegeneralizability of deep adult lung segmentation models to the pediatricpopulation and (ii) improve performance through a stage-wise, systematicapproach consisting of CXR modality-specific weight initializations, stackedensembles, and an ensemble of stacked ensembles. To evaluate segmentationperformance and generalizability, novel evaluation metrics consisting of meanlung contour distance (MLCD) and average hash score (AHS) are proposed inaddition to the multi-scale structural similarity index measure (MS-SSIM), theintersection of union (IoU), Dice score, 95% Hausdorff distance (HD95), andaverage symmetric surface distance (ASSD). Our results showed a significantimprovement (p &lt; 0.05) in cross-domain generalization through our approach.This study could serve as a paradigm to analyze the cross-domaingeneralizability of deep segmentation models for other medical imagingmodalities and applications.", "output": "Generalizability of Deep Adult Lung Segmentation Models to the Pediatric Population: A Retrospective Study."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep convolutional neural networks have achieved great progress in imagedenoising tasks. However, their complicated architectures and heavycomputational cost hinder their deployments on mobile devices. Some recentefforts in designing lightweight denoising networks focus on reducing eitherFLOPs (floating-point operations) or the number of parameters. However, thesemetrics are not directly correlated with the on-device latency. In this paper,we identify the real bottlenecks that affect the CNN-based models' run-timeperformance on mobile devices: memory access cost and NPU-incompatibleoperations, and build the model based on these. To further improve thedenoising performance, the mobile-friendly attention module MFA and the modelreparameterization module RepConv are proposed, which enjoy both low latencyand excellent denoising performance. To this end, we propose a mobile-friendlydenoising network, namely MFDNet. The experiments show that MFDNet achievesstate-of-the-art performance on real-world denoising benchmarks SIDD and DNDunder real-time latency on mobile devices. The code and pre-trained models willbe released.", "output": "Lightweight network towards real-time image denoising on mobile devices."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Just like weights, bias terms are the learnable parameters of many popularmachine learning models, including neural networks. Biases are believed toeffectively increase the representational power of neural networks to solve awide range of tasks in computer vision. However, we argue that if we considerthe intrinsic distribution of images in the input space as well as some desiredproperties a model should have from the first principles, biases can becompletely ignored in addressing many image-related tasks, such as imageclassification. Our observation indicates that zero-bias neural networks couldperform comparably to neural networks with bias at least on practical imageclassification tasks. In addition, we prove that zero-bias neural networkspossess a nice property called scalar (multiplication) invariance, which allowsthe prediction of neural networks remains the same when altering the contrastof the input image. We then extend scalar invariance to more general cases thatallow us to formally verify certain convex regions of the input space. Besidesthat, we show the fairness of zero-bias neural networks in predicting the zeroimage. In contrast to the state-of-art models which lean towards certainlabels, zero-bias neural networks have a uniform belief in all labels. Based onthose merits, we believe dropping bias terms can be considered as a prior indesigning neural network architecture for some CV tasks, which shares thespirit of adapting convolutions as the transnational invariance prior.", "output": "Scalar Invariant Networks with Zero Bias."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Camera relocalization has various applications in autonomous driving.Previous camera pose regression models consider only ideal scenarios wherethere is little environmental perturbation. To deal with challenging drivingenvironments that may have changing seasons, weather, illumination, and thepresence of unstable objects, we propose RobustLoc, which derives itsrobustness against perturbations from neural differential equations. Our modeluses a convolutional neural network to extract feature maps from multi-viewimages, a robust neural differential equation diffusion block module to diffuseinformation interactively, and a branched pose decoder with multi-layertraining to estimate the vehicle poses. Experiments demonstrate that RobustLocsurpasses current state-of-the-art camera pose regression models and achievesrobust performance in various environments. Our code is released at:", "output": "RobustLoc: Robust Camera Pose Regression in Challenging Driving Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A common challenge in real world classification scenarios with sequentiallyappending target domain data is insufficient training datasets during thetraining phase. Therefore, conventional deep learning and transfer learningclassifiers are not applicable especially when individual classes are notrepresented or are severely underrepresented at the outset. In many technicaldomains, however, it is only the defect or worn reject classes that areinsufficiently represented, while the non-defect class is often available fromthe beginning. The proposed classification approach addresses such conditionsand is based on a CNN encoder. Following a contrastive learning approach, it istrained with a modified triplet loss function using two datasets: Besides thenon-defective target domain class 1st dataset, a state-of-the-art labeledsource domain dataset that contains highly related classes e.g., a relatedmanufacturing error or wear defect but originates from a highly differentdomain e.g., different product, material, or appearance = 2nd dataset isutilized. The approach learns the classification features from the sourcedomain dataset while at the same time learning the differences between thesource and the target domain in a single training step, aiming to transfer therelevant features to the target domain. The classifier becomes sensitive to theclassification features and by architecture robust against the highlydomain-specific context. The approach is benchmarked in a technical and anon-technical domain and shows convincing classification results. Inparticular, it is shown that the domain generalization capabilities andclassification results are improved by the proposed architecture, allowing forlarger domain shifts between source and target domains.", "output": "Cross-domain Transfer of defect features in technical domains based on partial target data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The ubiquity of edge devices has led to a growing amount of unlabeled dataproduced at the edge. Deep learning models deployed on edge devices arerequired to learn from these unlabeled data to continuously improve accuracy.Self-supervised representation learning has achieved promising performancesusing centralized unlabeled data. However, the increasing awareness of privacyprotection limits centralizing the distributed unlabeled image data on edgedevices. While federated learning has been widely adopted to enable distributedmachine learning with privacy preservation, without a data selection method toefficiently select streaming data, the traditional federated learning frameworkfails to handle these huge amounts of decentralized unlabeled data with limitedstorage resources on edge. To address these challenges, we propose aSelf-supervised On-device Federated learning framework with coreset selection,which we call SOFed, to automatically select a coreset that consists of themost representative samples into the replay buffer on each device. It preservesdata privacy as each client does not share raw data while learning good visualrepresentations. Experiments demonstrate the effectiveness and significance ofthe proposed method in visual representation learning.", "output": "Self-supervised On-device Federated Learning from Unlabeled Streams."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-modal image-text models such as CLIP and LiT have demonstratedimpressive performance on image classification benchmarks and their zero-shotgeneralization ability is particularly exciting. While the top-5 zero-shotaccuracies of these models are very high, the top-1 accuracies are much lower(over 25% gap in some cases). We investigate the reasons for this performancegap and find that many of the failure cases are caused by ambiguity in the textprompts. First, we develop a simple and efficient zero-shot post-hoc method toidentify images whose top-1 prediction is likely to be incorrect, by measuringconsistency of the predictions w.r.t. multiple prompts and imagetransformations. We show that our procedure better predicts mistakes,outperforming the popular max logit baseline on selective prediction tasks.Next, we propose a simple and efficient way to improve accuracy on suchuncertain images by making use of the WordNet hierarchy; specifically weaugment the original class by incorporating its parent and children from thesemantic label hierarchy, and plug the augmentation into text prompts. Weconduct experiments on both CLIP and LiT models with five differentImageNet-based datasets. For CLIP, our method improves the top-1 accuracy by17.13% on the uncertain subset and 3.6% on the entire ImageNet validation set.We also show that our method improves across ImageNet shifted datasets, fourother datasets, and other model architectures such as LiT. The proposed methodis hyperparameter-free, requires no additional model training and can be easilyscaled to other large multi-modal architectures. Code is available at", "output": "Improving Zero-shot Generalization and Robustness of Multi-modal Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The mainstream of the existing approaches for video prediction builds uptheir models based on a Single-In-Single-Out (SISO) architecture, which takesthe current frame as input to predict the next frame in a recursive manner.This way often leads to severe performance degradation when they try toextrapolate a longer period of future, thus limiting the practical use of theprediction model. Alternatively, a Multi-In-Multi-Out (MIMO) architecture thatoutputs all the future frames at one shot naturally breaks the recursive mannerand therefore prevents error accumulation. However, only a few MIMO models forvideo prediction are proposed and they only achieve inferior performance due tothe date. The real strength of the MIMO model in this area is not well noticedand is largely under-explored. Motivated by that, we conduct a comprehensiveinvestigation in this paper to thoroughly exploit how far a simple MIMOarchitecture can go. Surprisingly, our empirical studies reveal that a simpleMIMO model can outperform the state-of-the-art work with a large margin muchmore than expected, especially in dealing with longterm error accumulation.After exploring a number of ways and designs, we propose a new MIMOarchitecture based on extending the pure Transformer with local spatio-temporalblocks and a new multi-output decoder, namely MIMO-VP, to establish a newstandard in video prediction. We evaluate our model in four highly competitivebenchmarks (Moving MNIST, Human3.6M, Weather, KITTI). Extensive experimentsshow that our model wins 1st place on all the benchmarks with remarkableperformance gains and surpasses the best SISO model in all aspects includingefficiency, quantity, and quality. We believe our model can serve as a newbaseline to facilitate the future research of video prediction tasks. The codewill be released.", "output": "MIMO Is All You Need : A Strong Multi-In-Multi-Out Baseline for Video Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deepfakes are computationally-created entities that falsely representreality. They can take image, video, and audio modalities, and pose a threat tomany areas of systems and societies, comprising a topic of interest to variousaspects of cybersecurity and cybersafety. In 2020 a workshop consulting AIexperts from academia, policing, government, the private sector, and statesecurity agencies ranked deepfakes as the most serious AI threat. These expertsnoted that since fake material can propagate through many uncontrolled routes,changes in citizen behaviour may be the only effective defence. This study aimsto assess human ability to identify image deepfakes of human faces(StyleGAN2:FFHQ) from nondeepfake images (FFHQ), and to assess theeffectiveness of simple interventions intended to improve detection accuracy.Using an online survey, 280 participants were randomly allocated to one of fourgroups: a control group, and 3 assistance interventions. Each participant wasshown a sequence of 20 images randomly selected from a pool of 50 deepfake and50 real images of human faces. Participants were asked if each image wasAI-generated or not, to report their confidence, and to describe the reasoningbehind each response. Overall detection accuracy was only just above chance andnone of the interventions significantly improved this. Participants' confidencein their answers was high and unrelated to accuracy. Assessing the results on aper-image basis reveals participants consistently found certain images harderto label correctly, but reported similarly high confidence regardless of theimage. Thus, although participant accuracy was 62% overall, this accuracyacross images ranged quite evenly between 85% and 30%, with an accuracy ofbelow 50% for one in every five images. We interpret the findings as suggestingthat there is a need for an urgent call to action to address this threat.", "output": "Testing Human Ability To Detect Deepfake Images of Human Faces."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Recurrent Vision Transformers (RVTs), a novel backbone for objectdetection with event cameras. Event cameras provide visual information withsub-millisecond latency at a high-dynamic range and with strong robustnessagainst motion blur. These unique properties offer great potential forlow-latency object detection and tracking in time-critical scenarios. Priorwork in event-based vision has achieved outstanding detection performance butat the cost of substantial inference time, typically beyond 40 milliseconds. Byrevisiting the high-level design of recurrent vision backbones, we reduceinference time by a factor of 6 while retaining similar performance. To achievethis, we explore a multi-stage design that utilizes three key concepts in eachstage: First, a convolutional prior that can be regarded as a conditionalpositional embedding. Second, local and dilated global self-attention forspatial feature interaction. Third, recurrent temporal feature aggregation tominimize latency while retaining temporal information. RVTs can be trained fromscratch to reach state-of-the-art performance on event-based object detection -achieving an mAP of 47.2% on the Gen1 automotive dataset. At the same time,RVTs offer fast inference (&lt;12 ms on a T4 GPU) and favorable parameterefficiency (5 times fewer than prior art). Our study brings new insights intoeffective design choices that can be fruitful for research beyond event-basedvision.", "output": "Recurrent Vision Transformers for Object Detection with Event Cameras."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Object instance segmentation is a key challenge for indoor robots navigatingcluttered environments with many small objects. Limitations in 3D sensingcapabilities often make it difficult to detect every possible object. Whiledeep learning approaches may be effective for this problem, manually annotating3D data for supervised learning is time-consuming. In this work, we explorezero-shot instance segmentation (ZSIS) from RGB-D data to identify unseenobjects in a semantic category-agnostic manner. We introduce a zero-shot splitfor Tabletop Objects Dataset (TOD-Z) to enable this study and present a methodthat uses annotated objects to learn the ``objectness'' of pixels andgeneralize to unseen object categories in cluttered indoor environments. Ourmethod, SupeRGB-D, groups pixels into small patches based on geometric cues andlearns to merge the patches in a deep agglomerative clustering fashion.SupeRGB-D outperforms existing baselines on unseen objects while achievingsimilar performance on seen objects. We further show competitive results on thereal dataset OCID. With its lightweight design (0.4 MB memory requirement), ourmethod is extremely suitable for mobile and robotic applications. AdditionalDINO features can increase performance with a higher memory requirement. Thedataset split and code are available at ", "output": "SupeRGB-D: Zero-shot Instance Segmentation in Cluttered Indoor Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Motion prediction is essential for safe and efficient autonomous driving.However, the inexplicability and uncertainty of complex artificial intelligencemodels may lead to unpredictable failures of the motion prediction module,which may mislead the system to make unsafe decisions. Therefore, it isnecessary to develop methods to guarantee reliable autonomous driving, wherefailure detection is a potential direction. Uncertainty estimates can be usedto quantify the degree of confidence a model has in its predictions and may bevaluable for failure detection. We propose a framework of failure detection formotion prediction from the uncertainty perspective, considering both motionuncertainty and model uncertainty, and formulate various uncertainty scoresaccording to different prediction stages. The proposed approach is evaluatedbased on different motion prediction algorithms, uncertainty estimationmethods, uncertainty scores, etc., and the results show that uncertainty ispromising for failure detection for motion prediction but should be used withcaution.", "output": "Failure Detection for Motion Prediction of Autonomous Driving: An Uncertainty Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Efficient detectors for edge devices are often optimized for metrics likeparameters or speed counts, which remain weak correlation with the energy ofdetectors. However, among vision applications of convolutional neural networks(CNNs), some, such as always-on surveillance cameras, are critical for energyconstraints. This paper aims to serve as a baseline by designing detectors toreach tradeoffs between energy and performance from two perspectives: 1) Weextensively analyze various CNNs to identify low-energy architectures,including the selection of activation functions, convolutions operators, andfeature fusion structures on necks. These underappreciated details in pastworks seriously affect the energy consumption of detectors; 2) To break throughthe dilemmatic energy-performance problem, we propose a balanced detectordriven by energy using discovered low-energy components namedtextit{FemtoDet}. In addition to the novel construction, we further improveFemtoDet by considering convolutions and training strategy optimizations.Specifically, we develop a new instance boundary enhancement (IBE) module forconvolution optimization to overcome the contradiction between the limitedcapacity of CNNs and detection tasks in diverse spatial representations, andpropose a recursive warm-restart (RecWR) for optimizing training strategy toescape the sub-optimization of light-weight detectors, considering the datashift produced in popular augmentations. As a result, FemtoDet with only 68.77kparameters achieves a competitive score of 46.3 AP50 on PASCAL VOC and power of7.83W on RTX 3090. Extensive experiments on COCO and TJU-DHD datasets indicatethat the proposed method achieves competitive results in diverse scenes.", "output": "FemtoDet: An Object Detection Baseline for Energy Versus Performance Tradeoffs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Gate-defined quantum dots (QDs) have appealing attributes as a quantumcomputing platform. However, near-term devices possess a range of possibleimperfections that need to be accounted for during the tuning and operation ofQD devices. One such problem is the capacitive cross-talk between the metallicgates that define and control QD qubits. A way to compensate for the capacitivecross-talk and enable targeted control of specific QDs independent of couplingis by the use of virtual gates. Here, we demonstrate a reliable automatedcapacitive coupling identification method that combines machine learning withtraditional fitting to take advantage of the desirable properties of each. Wealso show how the cross-capacitance measurement may be used for theidentification of spurious QDs sometimes formed during tuning experimentaldevices. Our systems can autonomously flag devices with spurious dots near theoperating regime, which is crucial information for reliable tuning to a regimesuitable for qubit operations.", "output": "Automated extraction of capacitive coupling for quantum dot systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent ODE/SDE-based generative models, such as diffusion models, rectifiedflows, and flow matching, define a generative process as a time reversal of afixed forward process. Even though these models show impressive performance onlarge-scale datasets, numerical simulation requires multiple evaluations of aneural network, leading to a slow sampling speed. We attribute the reason tothe high curvature of the learned generative trajectories, as it is directlyrelated to the truncation error of a numerical solver. Based on therelationship between the forward process and the curvature, here we present anefficient method of training the forward process to minimize the curvature ofgenerative trajectories without any ODE/SDE simulation. Experiments show thatour method achieves a lower curvature than previous models and, therefore,decreased sampling costs while maintaining competitive performance. Code isavailable at ", "output": "Minimizing Trajectory Curvature of ODE-based Generative Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While leveraging additional training data is well established to improveadversarial robustness, it incurs the unavoidable cost of data collection andthe heavy computation to train models. To mitigate the costs, we propose GuidedAdversarial Training (GAT), a novel adversarial training technique thatexploits auxiliary tasks under a limited set of training data. Our approachextends single-task models into multi-task models during the min-maxoptimization of adversarial training, and drives the loss optimization with aregularization of the gradient curvature across multiple tasks. GAT leveragestwo types of auxiliary tasks: self-supervised tasks, where the labels aregenerated automatically, and domain-knowledge tasks, where human expertsprovide additional labels. Experimentally, GAT increases the robust AUC ofCheXpert medical imaging dataset from 50% to 83% and On CIFAR-10, GAToutperforms eight state-of-the-art adversarial training and achieves 56.21%robust accuracy with Resnet-50. Overall, we demonstrate that guided multi-tasklearning is an actionable and promising avenue to push further the boundariesof model robustness.", "output": "GAT: Guided Adversarial Training with Pareto-optimal Auxiliary Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models have enabled high-quality, conditional image editingcapabilities. We propose to expand their arsenal, and demonstrate thatoff-the-shelf diffusion models can be used for a wide range of cross-domaincompositing tasks. Among numerous others, these include image blending, objectimmersion, texture-replacement and even CG2Real translation or stylization. Weemploy a localized, iterative refinement scheme which infuses the injectedobjects with contextual information derived from the background scene, andenables control over the degree and types of changes the object may undergo. Weconduct a range of qualitative and quantitative comparisons to prior work, andexhibit that our method produces higher quality and realistic results withoutrequiring any annotations or training. Finally, we demonstrate how our methodmay be used for data augmentation of downstream tasks.", "output": "Cross-domain Compositing with Pretrained Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The aim of this paper is to propose a mechanism to efficiently and explicitlymodel image hierarchies in the global, regional, and local range for imagerestoration. To achieve that, we start by analyzing two important properties ofnatural images including cross-scale similarity and anisotropic image features.Inspired by that, we propose the anchored stripe self-attention which achievesa good balance between the space and time complexity of self-attention and themodelling capacity beyond the regional range. Then we propose a new networkarchitecture dubbed GRL to explicitly model image hierarchies in the Global,Regional, and Local range via anchored stripe self-attention, windowself-attention, and channel attention enhanced convolution. Finally, theproposed network is applied to 7 image restoration types, covering both realand synthetic settings. The proposed method sets the new state-of-the-art forseveral of those. Code will be available at", "output": "Efficient and Explicit Modelling of Image Hierarchies for Image Restoration."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, deep learning has been successfully applied in variousscientific domains. Following these promising results and performances, it hasrecently also started being evaluated in the domain of radio astronomy. Inparticular, since radio astronomy is entering the Big Data era, with the adventof the largest telescope in the world - the Square Kilometre Array (SKA), thetask of automatic object detection and instance segmentation is crucial forsource finding and analysis. In this work, we explore the performance of themost affirmed deep learning approaches, applied to astronomical images obtainedby radio interferometric instrumentation, to solve the task of automatic sourcedetection. This is carried out by applying models designed to accomplish twodifferent kinds of tasks: object detection and semantic segmentation. The goalis to provide an overview of existing techniques, in terms of predictionperformance and computational efficiency, to scientists in the astrophysicscommunity who would like to employ machine learning in their research.", "output": "Radio astronomical images object detection and segmentation: A benchmark on deep learning methods."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep Learning (DL) systems have proliferated in many applications, requiringspecialized hardware accelerators and chips. In the nano-era, devices havebecome increasingly more susceptible to permanent and transient faults.Therefore, we need an efficient methodology for analyzing the resilience ofadvanced DL systems against such faults, and understand how the faults inneural accelerator chips manifest as errors at the DL application level, wherefaults can lead to undetectable and unrecoverable errors. Using faultinjection, we can perform resilience investigations of the DL system bymodifying neuron weights and outputs at the software-level, as if the hardwarehad been affected by a transient fault. Existing fault models reduce the searchspace, allowing faster analysis, but requiring a-priori knowledge on the model,and not allowing further analysis of the filtered-out search space. Therefore,we propose ISimDL, a novel methodology that employs neuron sensitivity togenerate importance sampling-based fault-scenarios. Without any a-prioriknowledge of the model-under-test, ISimDL provides an equivalent reduction ofthe search space as existing works, while allowing long simulations to coverall the possible faults, improving on existing model requirements. Ourexperiments show that the importance sampling provides up to 15x higherprecision in selecting critical faults than the random uniform sampling,reaching such precision in less than 100 faults. Additionally, we showcaseanother practical use-case for importance sampling for reliable DNN design,namely Fault Aware Training (FAT). By using ISimDL to select the faults leadingto errors, we can insert the faults during the DNN training process to hardenthe DNN against such faults. Using importance sampling in FAT reduces theoverhead required for finding faults that lead to a predetermined drop inaccuracy by more than 12x.", "output": "ISimDL: Importance Sampling-Driven Acceleration of Fault Injection Simulations for Evaluating the Robustness of Deep Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep learning in computer vision has achieved great success with the price oflarge-scale labeled training data. However, exhaustive data annotation isimpracticable for each task of all domains of interest, due to high labor costsand unguaranteed labeling accuracy. Besides, the uncontrollable data collectionprocess produces non-IID training and test data, where undesired duplicationmay exist. All these nuisances may hinder the verification of typical theoriesand exposure to new findings. To circumvent them, an alternative is to generatesynthetic data via 3D rendering with domain randomization. We in this work pushforward along this line by doing profound and extensive research on baresupervised learning and downstream domain adaptation. Specifically, under thewell-controlled, IID data setting enabled by 3D rendering, we systematicallyverify the typical, important learning insights, e.g., shortcut learning, anddiscover the new laws of various data regimes and network architectures ingeneralization. We further investigate the effect of image formation factors ongeneralization, e.g., object scale, material texture, illumination, cameraviewpoint, and background in a 3D scene. Moreover, we use thesimulation-to-reality adaptation as a downstream task for comparing thetransferability between synthetic and real data when used for pre-training,which demonstrates that synthetic data pre-training is also promising toimprove real test results. Lastly, to promote future research, we develop a newlarge-scale synthetic-to-real benchmark for image classification, termed S2RDA,which provides more significant challenges for transfer from simulation toreality. The code and datasets are available at", "output": "A New Benchmark: On the Utility of Synthetic Data with Blender for Bare Supervised Learning and Downstream Domain Adaptation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Solving complicated AI tasks with different domains and modalities is a keystep toward artificial general intelligence. While there are abundant AI modelsavailable for different domains and modalities, they cannot handle complicatedAI tasks. Considering large language models (LLMs) have exhibited exceptionalability in language understanding, generation, interaction, and reasoning, weadvocate that LLMs could act as a controller to manage existing AI models tosolve complicated AI tasks and language could be a generic interface to empowerthis. Based on this philosophy, we present HuggingGPT, a framework thatleverages LLMs (e.g., ChatGPT) to connect various AI models in machine learningcommunities (e.g., Hugging Face) to solve AI tasks. Specifically, we useChatGPT to conduct task planning when receiving a user request, select modelsaccording to their function descriptions available in Hugging Face, executeeach subtask with the selected AI model, and summarize the response accordingto the execution results. By leveraging the strong language capability ofChatGPT and abundant AI models in Hugging Face, HuggingGPT is able to covernumerous sophisticated AI tasks in different modalities and domains and achieveimpressive results in language, vision, speech, and other challenging tasks,which paves a new way towards artificial general intelligence.", "output": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "LiDAR relocalization plays a crucial role in many fields, including robotics,autonomous driving, and computer vision. LiDAR-based retrieval from a databasetypically incurs high computation storage costs and can lead to globallyinaccurate pose estimations if the database is too sparse. On the other hand,pose regression methods take images or point clouds as inputs and directlyregress global poses in an end-to-end manner. They do not perform databasematching and are more computationally efficient than retrieval techniques. Wepropose HypLiLoc, a new model for LiDAR pose regression. We use two branchedbackbones to extract 3D features and 2D projection features, respectively. Weconsider multi-modal feature fusion in both Euclidean and hyperbolic spaces toobtain more effective feature representations. Experimental results indicatethat HypLiLoc achieves state-of-the-art performance in both outdoor and indoordatasets. We also conduct extensive ablation studies on the framework design,which demonstrate the effectiveness of multi-modal feature extraction andmulti-space embedding. Our code is released at:", "output": "HypLiLoc: Towards Effective LiDAR Pose Regression with Hyperbolic Fusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative foundation models are susceptible to implicit biases that canarise from extensive unsupervised training data. Such biases can producesuboptimal samples, skewed outcomes, and unfairness, with potentiallysignificant repercussions. Consequently, aligning these models with humanethics and preferences is an essential step toward ensuring their responsibleand effective deployment in real-world applications. Prior research hasprimarily employed Reinforcement Learning from Human Feedback (RLHF) as a meansof addressing this problem, wherein generative models are fine-tuned using RLalgorithms guided by a human-feedback-informed reward model. However, theinefficiencies and instabilities associated with RL algorithms frequentlypresent substantial obstacles to the successful alignment of generative models,necessitating the development of a more robust and streamlined approach. Tothis end, we introduce a new framework, Reward rAnked FineTuning (RAFT),designed to align generative models more effectively. Utilizing a reward modeland a sufficient number of samples, our approach selects the high-qualitysamples, discarding those that exhibit undesired behavior, and subsequentlyassembles a streaming dataset. This dataset serves as the basis for aligningthe generative model and can be employed under both offline and onlinesettings. Notably, the sample generation process within RAFT is gradient-free,rendering it compatible with black-box generators. Through extensiveexperiments, we demonstrate that our proposed algorithm exhibits strongperformance in the context of both large language models and diffusion models.", "output": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In a federated learning (FL) system, distributed clients upload their localmodels to a central server to aggregate into a global model. Malicious clientsmay plant backdoors into the global model through uploading poisoned localmodels, causing images with specific patterns to be misclassified into sometarget labels. Backdoors planted by current attacks are not durable, and vanishquickly once the attackers stop model poisoning. In this paper, we investigatethe connection between the durability of FL backdoors and the relationshipsbetween benign images and poisoned images (i.e., the images whose labels areflipped to the target label during local training). Specifically, benign imageswith the original and the target labels of the poisoned images are found tohave key effects on backdoor durability. Consequently, we propose a novelattack, Chameleon, which utilizes contrastive learning to further amplify sucheffects towards a more durable backdoor. Extensive experiments demonstrate thatChameleon significantly extends the backdoor lifespan over baselines by$1.2times sim 4times$, for a wide range of image datasets, backdoor types,and model architectures.", "output": "Chameleon: Adapting to Peer Images for Planting Durable Backdoors in Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, soft prompt learning methods have been proposed to fine-tunelarge-scale vision-language pre-trained models for various downstream tasks.These methods typically combine learnable textual tokens with class tokens asinput for models with frozen parameters. However, they often employ a singleprompt to describe class contexts, failing to capture categories' diverseattributes adequately. This study introduces the Partitioned Multi-modal Prompt(PMPO), a multi-modal prompting technique that extends the soft prompt from asingle learnable prompt to multiple prompts. Our method divides the visualencoder depths and connects learnable prompts to the separated visual depths,enabling different prompts to capture the hierarchical contextual depths ofvisual representations. Furthermore, to maximize the advantages of multi-promptlearning, we incorporate prior information from manually designed templates andlearnable multi-prompts, thus improving the generalization capabilities of ourapproach. We evaluate the effectiveness of our approach on three challengingtasks: new class generalization, cross-dataset evaluation, and domaingeneralization. For instance, our method achieves a $79.28$ harmonic mean,averaged over 11 diverse image recognition datasets ($+7.62$ compared to CoOp),demonstrating significant competitiveness compared to state-of-the-artprompting methods.", "output": "Multi-Prompt with Depth Partitioned Cross-Modal Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a physics-based humanoid controller that achieves high-fidelitymotion imitation and fault-tolerant behavior in the presence of noisy input(e.g. pose estimates from video or generated from language) and unexpectedfalls. Our controller scales up to learning ten thousand motion clips withoutusing any external stabilizing forces and learns to naturally recover fromfail-state. Given reference motion, our controller can perpetually controlsimulated avatars without requiring resets. At its core, we propose theprogressive multiplicative control policy (PMCP), which dynamically allocatesnew network capacity to learn harder and harder motion sequences. PMCP allowsefficient scaling for learning from large-scale motion databases and adding newtasks, such as fail-state recovery, without catastrophic forgetting. Wedemonstrate the effectiveness of our controller by using it to imitate noisyposes from video-based pose estimators and language-based motion generators ina live and real-time multi-person avatar use case.", "output": "Perpetual Humanoid Control for Real-time Simulated Avatars."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Convolutional neural networks have made significant strides in medical imageanalysis in recent years. However, the local nature of the convolution operatorinhibits the CNNs from capturing global and long-range interactions. Recently,Transformers have gained popularity in the computer vision community and alsomedical image segmentation. But scalability issues of self-attention mechanismand lack of the CNN like inductive bias have limited their adoption. In thiswork, we present MaxViT-UNet, an Encoder-Decoder based hybrid visiontransformer for medical image segmentation. The proposed hybrid decoder, alsobased on MaxViT-block, is designed to harness the power of convolution andself-attention mechanism at each decoding stage with minimal computationalburden. The multi-axis self-attention in each decoder stage helps indifferentiating between the object and background regions much moreefficiently. The hybrid decoder block initially fuses the lower level featuresupsampled via transpose convolution, with skip-connection features coming fromhybrid encoder, then fused features are refined using multi-axis attentionmechanism. The proposed decoder block is repeated multiple times to accuratelysegment the nuclei regions. Experimental results on MoNuSeg dataset proves theeffectiveness of the proposed technique. Our MaxViT-UNet outperformed theprevious CNN only (UNet) and Transformer only (Swin-UNet) techniques by a largemargin of 2.36% and 5.31% on Dice metric respectively.", "output": "MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Vision transformers have recently become popular as a possible alternative toconvolutional neural networks (CNNs) for a variety of computer visionapplications. These vision transformers due to their ability to focus on globalrelationships in images have large capacity, but may result in poorgeneralization as compared to CNNs. Very recently, the hybridization ofconvolution and self-attention mechanisms in vision transformers is gainingpopularity due to their ability of exploiting both local and global imagerepresentations. These CNN-Transformer architectures also known as hybridvision transformers have shown remarkable results for vision applications.Recently, due to the rapidly growing number of these hybrid visiontransformers, there is a need for a taxonomy and explanation of thesearchitectures. This survey presents a taxonomy of the recent vision transformerarchitectures, and more specifically that of the hybrid vision transformers.Additionally, the key features of each architecture such as the attentionmechanisms, positional embeddings, multi-scale processing, and convolution arealso discussed. This survey highlights the potential of hybrid visiontransformers to achieve outstanding performance on a variety of computer visiontasks. Moreover, it also points towards the future directions of this rapidlyevolving field.", "output": "A survey of the Vision Transformers and its CNN-Transformer based Variants."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "3D object detection plays a crucial role in numerous intelligent visionsystems. Detection in the open world inevitably encounters various adversescenes, such as dense fog, heavy rain, and low light conditions. Althoughexisting efforts primarily focus on diversifying network architecture ortraining schemes, resulting in significant progress in 3D object detection,most of these learnable modules fail in adverse scenes, thereby hinderingdetection performance. To address this issue, this paper proposes a monocular3D detection model designed to perceive twin depth in adverse scenes, termedMonoTDP, which effectively mitigates the degradation of detection performancein various harsh environments. Specifically, we first introduce an adaptivelearning strategy to aid the model in handling uncontrollable weatherconditions, significantly resisting degradation caused by various degradingfactors. Then, to address the depth/content loss in adverse regions, we proposea novel twin depth perception module that simultaneously estimates scene andobject depth, enabling the integration of scene-level features and object-levelfeatures. Additionally, we assemble a new adverse 3D object detection datasetencompassing a wide range of challenging scenes, including rainy, foggy, andlow light weather conditions, with each type of scene containing 7,481 images.Experimental results demonstrate that our proposed method outperforms currentstate-of-the-art approaches by an average of 3.12% in terms of AP_R40 for carcategory across various adverse environments.", "output": "MonoTDP: Twin Depth Perception for Monocular 3D Object Detection in Adverse Scenes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Achieving machine autonomy and human control often represent divergentobjectives in the design of interactive AI systems. Visual generativefoundation models such as Stable Diffusion show promise in navigating thesegoals, especially when prompted with arbitrary languages. However, they oftenfall short in generating images with spatial, structural, or geometriccontrols. The integration of such controls, which can accommodate variousvisual conditions in a single unified model, remains an unaddressed challenge.In response, we introduce UniControl, a new generative foundation model thatconsolidates a wide array of controllable condition-to-image (C2I) tasks withina singular framework, while still allowing for arbitrary language prompts.UniControl enables pixel-level-precise image generation, where visualconditions primarily influence the generated structures and language promptsguide the style and context. To equip UniControl with the capacity to handlediverse visual conditions, we augment pretrained text-to-image diffusion modelsand introduce a task-aware HyperNet to modulate the diffusion models, enablingthe adaptation to different C2I tasks simultaneously. Trained on nine uniqueC2I tasks, UniControl demonstrates impressive zero-shot generation abilitieswith unseen visual conditions. Experimental results show that UniControl oftensurpasses the performance of single-task-controlled methods of comparable modelsizes. This control versatility positions UniControl as a significantadvancement in the realm of controllable visual generation.", "output": "UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (LLMs) have notably accelerated progress towardsartificial general intelligence (AGI), with their impressive zero-shot capacityfor user-tailored tasks, endowing them with immense potential across a range ofapplications. However, in the field of computer vision, despite theavailability of numerous powerful vision foundation models (VFMs), they arestill restricted to tasks in a pre-defined form, struggling to match theopen-ended task capabilities of LLMs. In this work, we present an LLM-basedframework for vision-centric tasks, termed VisionLLM. This framework provides aunified perspective for vision and language tasks by treating images as aforeign language and aligning vision-centric tasks with language tasks that canbe flexibly defined and managed using language instructions. An LLM-baseddecoder can then make appropriate predictions based on these instructions foropen-ended tasks. Extensive experiments show that the proposed VisionLLM canachieve different levels of task customization through language instructions,from fine-grained object-level to coarse-grained task-level customization, allwith good results. It's noteworthy that, with a generalist LLM-based framework,our model can achieve over 60% mAP on COCO, on par with detection-specificmodels. We hope this model can set a new baseline for generalist vision andlanguage models. The demo shall be released based on The code shall be released at", "output": "VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models, either with or without text condition, have demonstratedimpressive capability in synthesizing photorealistic images given a few or evenno words. These models may not fully satisfy user need, as normal users orartists intend to control the synthesized images with specific guidance, likeoverall layout, color, structure, object shape, and so on. To adapt diffusionmodels for controllable image synthesis, several methods have been proposed toincorporate the required conditions as regularization upon the intermediatefeatures of the diffusion denoising network. These methods, known asearly-constraint ones in this paper, have difficulties in handling multipleconditions with a single solution. They intend to train separate models foreach specific condition, which require much training cost and result innon-generalizable solutions. To address these difficulties, we propose a newapproach namely late-constraint: we leave the diffusion networks unchanged, butconstrain its output to be aligned with the required conditions. Specifically,we train a lightweight condition adapter to establish the correlation betweenexternal conditions and internal representations of diffusion models. Duringthe iterative denoising process, the conditional guidance is sent intocorresponding condition adapter to manipulate the sampling process with theestablished correlation. We further equip the introduced late-constraintstrategy with a timestep resampling method and an early stopping technique,which boost the quality of synthesized image meanwhile complying with theguidance. Our method outperforms the existing early-constraint methods andgeneralizes better to unseen condition.", "output": "Late-Constraint Diffusion Guidance for Controllable Image Synthesis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a lightweight system for stereo matching through embedded GPUs. Itbreaks the trade-off between accuracy and processing speed in stereo matching,enabling our embedded system to further improve the matching accuracy whileensuring real-time processing. The main idea of our method is to construct atiny neural network based on variational auto-encoder (VAE) to upsample andrefinement a small size of coarse disparity map, which is first generated by atraditional matching method. The proposed hybrid structure cannot only bringthe advantage of traditional methods in terms of computational complexity, butalso ensure the matching accuracy under the impact of neural network. Extensiveexperiments on the KITTI 2015 benchmark demonstrate that our tiny systemexhibits high robustness in improving the accuracy of the coarse disparity mapsgenerated by different algorithms, while also running in real-time on embeddedGPUs.", "output": "StereoVAE: A lightweight stereo matching system through embedded GPUs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing research on multimodal relation extraction (MRE) faces twoco-existing challenges, internal-information over-utilization andexternal-information under-exploitation. To combat that, we propose a novelframework that simultaneously implements the idea of internal-informationscreening and external-information exploiting. First, we represent thefine-grained semantic structures of the input image and text with the visualand textual scene graphs, which are further fused into a unified cross-modalgraph (CMG). Based on CMG, we perform structure refinement with the guidance ofthe graph information bottleneck principle, actively denoising theless-informative features. Next, we perform topic modeling over the input imageand text, incorporating latent multimodal topic features to enrich thecontexts. On the benchmark MRE dataset, our system outperforms the current bestmodel significantly. With further in-depth analyses, we reveal the greatpotential of our method for the MRE task. Our codes are open at", "output": "Information Screening whilst Exploiting! Multimodal Relation Extraction with Feature Denoising and Multimodal Topic Modeling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Visual spatial description (VSD) aims to generate texts that describe thespatial relations of the given objects within images. Existing VSD work merelymodels the 2D geometrical vision features, thus inevitably falling prey to theproblem of skewed spatial understanding of target objects. In this work, weinvestigate the incorporation of 3D scene features for VSD. With an external 3Dscene extractor, we obtain the 3D objects and scene features for input images,based on which we construct a target object-centered 3D spatial scene graph(Go3D-S2G), such that we model the spatial semantics of target objects withinthe holistic 3D scenes. Besides, we propose a scene subgraph selectingmechanism, sampling topologically-diverse subgraphs from Go3D-S2G, where thediverse local structure features are navigated to yield spatially-diversifiedtext generation. Experimental results on two VSD datasets demonstrate that ourframework outperforms the baselines significantly, especially improving on thecases with complex visual spatial relations. Meanwhile, our method can producemore spatially-diversified generation. Code is available at", "output": "Generating Visual Spatial Description via Holistic 3D Scene Understanding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unpaired cross-lingual image captioning has long suffered from irrelevancyand disfluency issues, due to the inconsistencies of the semantic scene andsyntax attributes during transfer. In this work, we propose to address theabove problems by incorporating the scene graph (SG) structures and thesyntactic constituency (SC) trees. Our captioner contains the semanticstructure-guided image-to-pivot captioning and the syntactic structure-guidedpivot-to-target translation, two of which are joined via pivot language. Wethen take the SG and SC structures as pivoting, performing cross-modal semanticstructure alignment and cross-lingual syntactic structure alignment learning.We further introduce cross-lingual&amp;cross-modal back-translation training tofully align the captioning and translation stages. Experiments onEnglish-Chinese transfers show that our model shows great superiority inimproving captioning relevancy and fluency.", "output": "Cross2StrA: Unpaired Cross-lingual Image Captioning with Cross-lingual Cross-modal Structure-pivoted Alignment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To implement autonomous driving, one essential step is to model the vehicleenvironment based on the sensor inputs. Radars, with their well-knownadvantages, became a popular option to infer the occupancy state of grid cellssurrounding the vehicle. To tackle data sparsity and noise of radar detections,we propose a deep learning-based Inverse Sensor Model (ISM) to learn themapping from sparse radar detections to polar measurement grids. Improvedlidar-based measurement grids are used as reference. The learned radarmeasurement grids, combined with radar Doppler velocity measurements, arefurther used to generate a Dynamic Grid Map (DGM). Experiments in real-worldhighway scenarios show that our approach outperforms the hand-crafted geometricISMs. In comparison to state-of-the-art deep learning methods, our approach isthe first one to learn a single-frame measurement grid in the polar scheme fromradars with a limited Field Of View (FOV). The learning framework makes thelearned ISM independent of the radar mounting. This enables us to flexibly useone or more radar sensors without network retraining and without requirementson 360{deg} sensor coverage.", "output": "Deep Radar Inverse Sensor Models for Dynamic Occupancy Grid Maps."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Visual-Language Fields (VL-Fields), a neural implicit spatialrepresentation that enables open-vocabulary semantic queries. Our model encodesand fuses the geometry of a scene with vision-language trained latent featuresby distilling information from a language-driven segmentation model. VL-Fieldsis trained without requiring any prior knowledge of the scene object classes,which makes it a promising representation for the field of robotics. Our modeloutperformed the similar CLIP-Fields model in the task of semantic segmentationby almost 10%.", "output": "VL-Fields: Towards Language-Grounded Neural Implicit Spatial Representations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present FIT: a transformer-based architecture with efficientself-attention and adaptive computation. Unlike original transformers, whichoperate on a single sequence of data tokens, we divide the data tokens intogroups, with each group being a shorter sequence of tokens. We employ two typesof transformer layers: local layers operate on data tokens within each group,while global layers operate on a smaller set of introduced latent tokens. Theselayers, comprising the same set of self-attention and feed-forward layers asstandard transformers, are interleaved, and cross-attention is used tofacilitate information exchange between data and latent tokens within the samegroup. The attention complexity is $O(n^2)$ locally within each group of size$n$, but can reach $O(L^{{4}/{3}})$ globally for sequence length of $L$. Theefficiency can be further enhanced by relying more on global layers thatperform adaptive computation using a smaller set of latent tokens. FIT is aversatile architecture and can function as an encoder, diffusion decoder, orautoregressive decoder. We provide initial evidence demonstrating itseffectiveness in high-resolution image understanding and generation tasks.Notably, FIT exhibits potential in performing end-to-end training ongigabit-scale data, such as 6400$times$6400 images, or 160K tokens (afterpatch tokenization), within a memory capacity of 16GB, without requiringspecific optimizations or model parallelism.", "output": "FIT: Far-reaching Interleaved Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Overfitting remains a prevalent challenge in deep neural networks, leading tosuboptimal real-world performance. Employing regularization techniques is acommon strategy to counter this challenge, improving model generalization. Thispaper proposes Simultaneous Learning, a novel regularization approach drawingon Transfer Learning and Multi-task Learning principles, applied specificallyto the classification of hop varieties - an integral component of beerproduction. Our approach harnesses the power of auxiliary datasets in synergywith the target dataset to amplify the acquisition of highly relevant features.Through a strategic modification of the model's final layer, we enable thesimultaneous classification of both datasets without the necessity to treatthem as disparate tasks. To realize this, we formulate a loss function thatincludes an inter-group penalty. We conducted experimental evaluations usingthe InceptionV3 and ResNet50 models, designating the UFOP-HVD hop leaf datasetas the target and ImageNet and PlantNet as auxiliary datasets. Our proposedmethod exhibited a substantial performance advantage over models withoutregularization and those adopting dropout regularization, with accuracyimprovements ranging from 5 to 22 percentage points. Additionally, we introducea technique for interpretability devised to assess the quality of features byanalyzing correlations among class features in the network's convolutionallayers.", "output": "Regularization Through Simultaneous Learning: A Case Study for Hop Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deformable Image Registration (DIR) plays a significant role in quantifyingdeformation in medical data. Recent Deep Learning methods have shown promisingaccuracy and speedup for registering a pair of medical images. However, in 4D(3D + time) medical data, organ motion, such as respiratory motion and heartbeating, can not be effectively modeled by pair-wise methods as they wereoptimized for image pairs but did not consider the organ motion patternsnecessary when considering 4D data. This paper presents ORRN, an OrdinaryDifferential Equations (ODE)-based recursive image registration network. Ournetwork learns to estimate time-varying voxel velocities for an ODE that modelsdeformation in 4D image data. It adopts a recursive registration strategy toprogressively estimate a deformation field through ODE integration of voxelvelocities. We evaluate the proposed method on two publicly available lung 4DCTdatasets, DIRLab and CREATIS, for two tasks: 1) registering all images to theextreme inhale image for 3D+t deformation tracking and 2) registering extremeexhale to inhale phase images. Our method outperforms other learning-basedmethods in both tasks, producing the smallest Target Registration Error of1.24mm and 1.26mm, respectively. Additionally, it produces less than 0.001%unrealistic image folding, and the computation speed is less than 1 second foreach CT volume. ORRN demonstrates promising registration accuracy, deformationplausibility, and computation efficiency on group-wise and pair-wiseregistration tasks. It has significant implications in enabling fast andaccurate respiratory motion estimation for treatment planning in radiationtherapy or robot motion planning in thoracic needle insertion.", "output": "ORRN: An ODE-based Recursive Registration Network for Deformable Respiratory Motion Estimation with Lung 4DCT Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "emph{Consistent teaching} is an effective paradigm for implementingknowledge distillation (KD), where both student and teacher models receiveidentical inputs, and KD is treated as a function matching task (FunMatch).However, one limitation of FunMatch is that it does not account for thetransfer of adversarial robustness, a model's resistance to adversarialattacks. To tackle this problem, we propose a simple but effective strategycalled Adversarial Function Matching (AdvFunMatch), which aims to matchdistributions for all data points within the $ell_p$-norm ball of the trainingdata, in accordance with consistent teaching. Formulated as a min-maxoptimization problem, AdvFunMatch identifies the worst-case instances thatmaximizes the KL-divergence between teacher and student model outputs, which werefer to as \"mismatched examples,\" and then matches the outputs on thesemismatched examples. Our experimental results show that AdvFunMatch effectivelyproduces student models with both high clean accuracy and robustness.Furthermore, we reveal that strong data augmentations (emph{e.g.},AutoAugment) are beneficial in AdvFunMatch, whereas prior works have found themless effective in adversarial training. Code is available aturl{", "output": "AdvFunMatch: When Consistent Teaching Meets Adversarial Robustness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowledge distillation (KD) is a simple and successful method to transferknowledge from a teacher to a student model solely based on functionalactivity. However, current KD has a few shortcomings: it has recently beenshown that this method is unsuitable to transfer simple inductive biases likeshift equivariance, struggles to transfer out of domain generalization, andoptimization time is magnitudes longer compared to default non-KD modeltraining. To improve these aspects of KD, we propose Hard Augmentations forRobust Distillation (HARD), a generally applicable data augmentation framework,that generates synthetic data points for which the teacher and the studentdisagree. We show in a simple toy example that our augmentation frameworksolves the problem of transferring simple equivariances with KD. We then applyour framework in real-world tasks for a variety of augmentation models, rangingfrom simple spatial transformations to unconstrained image manipulations with apretrained variational autoencoder. We find that our learned augmentationssignificantly improve KD performance on in-domain and out-of-domain evaluation.Moreover, our method outperforms even state-of-the-art data augmentations andsince the augmented training inputs can be visualized, they offer a qualitativeinsight into the properties that are transferred from the teacher to thestudent. Thus HARD represents a generally applicable, dynamically optimizeddata augmentation technique tailored to improve the generalization andconvergence speed of models trained with KD.", "output": "HARD: Hard Augmentations for Robust Distillation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Current state-of-the-art object-centric models use slots and attention-basedrouting for binding. However, this class of models has several conceptuallimitations: the number of slots is hardwired; all slots have equal capacity;training has high computational cost; there are no object-level relationalfactors within slots. Synchrony-based models in principle can address theselimitations by using complex-valued activations which store binding informationin their phase components. However, working examples of such synchrony-basedmodels have been developed only very recently, and are still limited to toygrayscale datasets and simultaneous storage of less than three objects inpractice. Here we introduce architectural modifications and a novel contrastivelearning method that greatly improve the state-of-the-art synchrony-basedmodel. For the first time, we obtain a class of synchrony-based models capableof discovering objects in an unsupervised manner in multi-object color datasetsand simultaneously representing more than three objects", "output": "Contrastive Training of Complex-Valued Autoencoders for Object Discovery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generating facial reactions in a human-human dyadic interaction is complexand highly dependent on the context since more than one facial reactions can beappropriate for the speaker's behaviour. This has challenged existing machinelearning (ML) methods, whose training strategies enforce models to reproduce aspecific (not multiple) facial reaction from each input speaker behaviour. Thispaper proposes the first multiple appropriate facial reaction generationframework that re-formulates the one-to-many mapping facial reaction generationproblem as a one-to-one mapping problem. This means that we approach thisproblem by considering the generation of a distribution of the listener'sappropriate facial reactions instead of multiple different appropriate facialreactions, i.e., 'many' appropriate facial reaction labels are summarised as'one' distribution label during training. Our model consists of a perceptualprocessor, a cognitive processor, and a motor processor. The motor processor isimplemented with a novel Reversible Multi-dimensional Edge Graph Neural Network(REGNN). This allows us to obtain a distribution of appropriate real facialreactions during the training process, enabling the cognitive processor to betrained to predict the appropriate facial reaction distribution. At theinference stage, the REGNN decodes an appropriate facial reaction by using thisdistribution as input. Experimental results demonstrate that our approachoutperforms existing models in generating more appropriate, realistic, andsynchronized facial reactions. The improved performance is largely attributedto the proposed appropriate facial reaction distribution learning strategy andthe use of a REGNN. The code is available at", "output": "Reversible Graph Neural Network-based Reaction Distribution Learning for Multiple Appropriate Facial Reactions Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Machine learning has been applied to the problem of X-ray diffraction phaseprediction with promising results. In this paper, we describe a method forusing machine learning to predict crystal structure phases from X-raydiffraction data of transition metals and their oxides. We evaluate theperformance of our method and compare the variety of its settings. Our resultsdemonstrate that the proposed machine learning framework achieves competitiveperformance. This demonstrates the potential for machine learning tosignificantly impact the field of X-ray diffraction and crystal structuredetermination. Open-source implementation:", "output": "Machine learning-assisted close-set X-ray diffraction phase identification of transition metals."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Clustering data is an unsupervised learning approach that aims to divide aset of data points into multiple groups. It is a crucial yet demanding subjectin machine learning and data mining. Its successful applications span variousfields. However, conventional clustering techniques necessitate theconsideration of balance significance in specific applications. Therefore, thispaper addresses the challenge of imbalanced clustering problems and presents anew method for balanced clustering by utilizing entropy-aware similarity, whichcan be defined as the degree of balances. We have coined the term,entropy-aware similarity for balanced clustering (EASB), which maximizesbalance during clustering by complementary clustering of unbalanced data andincorporating entropy in a novel similarity formula that accounts for bothangular differences and distances. The effectiveness of the proposed approachis evaluated on actual melanoma medial data, specifically the InternationalSkin Imaging Collaboration (ISIC) 2019 and 2020 challenge datasets, todemonstrate how it can successfully cluster the data while preserving balance.Lastly, we can confirm that the proposed method exhibited outstandingperformance in detecting melanoma, comparing to classical methods.", "output": "Entropy-Aware Similarity for Balanced Clustering: A Case Study with Melanoma Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In medical imaging, image synthesis is the estimation process of one image(sequence, modality) from another image (sequence, modality). Since images withdifferent modalities provide diverse biomarkers and capture various features,multi-modality imaging is crucial in medicine. While multi-screening isexpensive, costly, and time-consuming to report by radiologists, imagesynthesis methods are capable of artificially generating missing modalities.Deep learning models can automatically capture and extract the high dimensionalfeatures. Especially, generative adversarial network (GAN) as one of the mostpopular generative-based deep learning methods, uses convolutional networks asgenerators, and estimated images are discriminated as true or false based on adiscriminator network. This review provides brain image synthesis via GANs. Wesummarized the recent developments of GANs for cross-modality brain imagesynthesis including CT to PET, CT to MRI, MRI to PET, and vice versa.", "output": "Generative Adversarial Networks for Brain Images Synthesis: A Review."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Evaluating canine electrocardiograms (ECG) require skilled veterinarians, butcurrent availability of veterinary cardiologists for ECG interpretation anddiagnostic support is limited. Developing tools for automated assessment of ECGsequences can improve veterinary care by providing clinicians real-time resultsand decision support tools. We implement a deep convolutional neural network(CNN) approach for classifying canine electrocardiogram sequences as eithernormal or abnormal. ECG records are converted into 8 second Lead II sequencesand classified as either normal (no evidence of cardiac abnormalities) orabnormal (presence of one or more cardiac abnormalities). For training ECGsequences are randomly augmented using RandomAugmentECG, a new augmentationlibrary implemented specifically for this project. Each chunk is then isconverted using a continuous wavelet transform into a 2D scalogram. The 2Dscalogram are then classified as either normal or abnormal by a binary CNNclassifier. Experimental results are validated against three boarded veterinarycardiologists achieving an AUC-ROC score of 0.9506 on test dataset matchinghuman level performance. Additionally, we describe model deployment toMicrosoft Azure using an MLOps approach. To our knowledge, this work is one ofthe first attempts to implement a deep learning model to automatically classifyECG sequences for canines.Implementing automated ECG classification willenhance veterinary care through improved diagnostic performance and increasedclinic efficiency.", "output": "PulseNet: Deep Learning ECG-signal classification using random augmentation policy and continous wavelet transform for canines."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent language models have shown impressive multilingual performance, evenwhen not explicitly trained for it. Despite this, concerns have been raisedabout the quality of their outputs across different languages. In this paper,we show how disparity in the treatment of different languages arises at thetokenization stage, well before a model is even invoked. The same texttranslated into different languages can have drastically different tokenizationlengths, with differences up to 15 times in some cases. These disparitiespersist across the 17 tokenizers we evaluate, even if they are intentionallytrained for multilingual support. Character-level and byte-level models alsoexhibit over 4 times the difference in the encoding length for some languagepairs. This induces unfair treatment for some language communities in regard tothe cost of accessing commercial language services, the processing time andlatency, as well as the amount of content that can be provided as context tothe models. Therefore, we make the case that we should train future languagemodels using multilingually fair tokenizers.", "output": "Language Model Tokenizers Introduce Unfairness Between Languages."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In healthcare, accurately classifying medical images is vital, butconventional methods often hinge on medical data with a consistent gridstructure, which may restrict their overall performance. Recent medicalresearch has been focused on tweaking the architectures to attain betterperformance without giving due consideration to the representation of data. Inthis paper, we present a novel approach for transforming grid based data intoits higher dimensional representations, leveraging unstructured point clouddata structures. We first generate a sparse point cloud from an image byintegrating pixel color information as spatial coordinates. Next, we constructa hypersurface composed of points based on the image dimensions, with eachsmooth section within this hypersurface symbolizing a specific pixel location.Polygonal face construction is achieved using an adjacency tensor. Finally, adense point cloud is generated by densely sampling the constructedhypersurface, with a focus on regions of higher detail. The effectiveness ofour approach is demonstrated on a publicly accessible brain tumor dataset,achieving significant improvements over existing classification techniques.This methodology allows the extraction of intricate details from the originalimage, opening up new possibilities for advanced image analysis and processingtasks.", "output": "Transcending Grids: Point Clouds and Surface Representations Powering Neurological Processing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study online influence maximization (OIM) under a new model of decreasingcascade (DC). This model is a generalization of the independent cascade (IC)model by considering the common phenomenon of market saturation. In DC, thechance of an influence attempt being successful reduces with previous failures.The effect is neglected by previous OIM works under IC and linear thresholdmodels. We propose the DC-UCB algorithm to solve this problem, which achieves aregret bound of the same order as the state-of-the-art works on the IC model.Extensive experiments on both synthetic and real datasets show theeffectiveness of our algorithm.", "output": "Online Influence Maximization under Decreasing Cascade Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Community detection is an important problem in unsupervised learning. Thispaper proposes to solve a projection matrix approximation problem with anadditional entrywise bounded constraint. Algorithmically, we introduce a newdifferentiable convex penalty and derive an alternating direction method ofmultipliers (ADMM) algorithm. Theoretically, we establish the convergenceproperties of the proposed algorithm. Numerical experiments demonstrate thesuperiority of our algorithm over its competitors, such as the semi-definiterelaxation method and spectral clustering.", "output": "Bounded Projection Matrix Approximation with Applications to Community Detection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Massive amounts of data are the foundation of data-driven recommendationmodels. As an inherent nature of big data, data heterogeneity widely exists inreal-world recommendation systems. It reflects the differences in theproperties among sub-populations. Ignoring the heterogeneity in recommendationdata could limit the performance of recommendation models, hurt thesub-populational robustness, and make the models misled by biases. However,data heterogeneity has not attracted substantial attention in therecommendation community. Therefore, it inspires us to adequately explore andexploit heterogeneity for solving the above problems and assisting dataanalysis. In this work, we focus on exploring two representative categories ofheterogeneity in recommendation data that is the heterogeneity of predictionmechanism and covariate distribution and propose an algorithm that explores theheterogeneity through a bilevel clustering method. Furthermore, the uncoveredheterogeneity is exploited for two purposes in recommendation scenarios whichare prediction with multiple sub-models and supporting debias. Extensiveexperiments on real-world data validate the existence of heterogeneity inrecommendation data and the effectiveness of exploring and exploiting dataheterogeneity in recommendation.", "output": "Exploring and Exploiting Data Heterogeneity in Recommendation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Designing novel functional proteins remains a slow and expensive process dueto a variety of protein engineering challenges; in particular, the number ofprotein variants that can be experimentally tested in a given assay pales incomparison to the vastness of the overall sequence space, resulting in low hitrates and expensive wet lab testing cycles. In this paper, we propose afew-shot learning approach to novel protein design that aims to accelerate theexpensive wet lab testing cycle and is capable of leveraging a training datasetthat is both small and skewed ($approx 10^5$ datapoints, $&lt; 1%$ positivehits). Our approach is composed of two parts: a semi-supervised transferlearning approach to generate a discrete fitness landscape for a desiredprotein function and a novel evolutionary Monte Carlo Markov Chain samplingalgorithm to more efficiently explore the fitness landscape. We demonstrate theperformance of our approach by experimentally screening predicted high fitnessgene activators, resulting in a dramatically improved hit rate compared toexisting methods. Our method can be easily adapted to other protein engineeringand design problems, particularly where the cost associated with obtaininglabeled data is significantly high. We have provided open source code for ourmethod at ", "output": "Improving few-shot learning-based protein engineering with evolutionary sampling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In a surprising turn, Large Language Models (LLMs) together with a growingarsenal of prompt-based heuristics now offer powerful off-the-shelf approachesproviding few-shot solutions to myriad classic NLP problems. However, despitepromising early results, these LLM-based few-shot methods remain far from thestate of the art in Named Entity Recognition (NER), where prevailing methodsinclude learning representations via end-to-end structural understanding andfine-tuning on standard labeled corpora. In this paper, we introduce PromptNER,a new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt toany new NER task PromptNER requires a set of entity definitions in addition tothe standard few-shot examples. Given a sentence, PromptNER prompts an LLM toproduce a list of potential entities along with corresponding explanationsjustifying their compatibility with the provided entity type definitions.Remarkably, PromptNER achieves state-of-the-art performance on few-shot NER,achieving an 11% (absolute) improvement in F1 score on the ConLL dataset, and a10% (absolute) improvement on the FewNERD dataset. PromptNER also moves thestate of the art on Cross Domain NER, outperforming all prior methods(including those not limited to the few-shot setting), setting a new mark onall 5 CrossNER target domains, with an average F1 gain of 9%, despite usingless than 2% of the available data.", "output": "PromptNER: Prompting For Named Entity Recognition."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Flexible district heating grids form an important part of future, low-carbonenergy systems. We examine probabilistic state estimation in such grids, i.e.,we aim to estimate the posterior probability distribution over all grid statevariables such as pressures, temperatures, and mass flows conditional onmeasurements of a subset of these states. Since the posterior statedistribution does not belong to a standard class of probability distributions,we use Markov Chain Monte Carlo (MCMC) sampling in the space of network heatexchanges and evaluate the samples in the grid state space to estimate theposterior. Converting the heat exchange samples into grid states by solving thenon-linear grid equations makes this approach computationally burdensome.However, we propose to speed it up by employing a deep neural network that istrained to approximate the solution of the exact but slow non-linear solver.This novel approach is shown to deliver highly accurate posterior distributionsboth for classic tree-shaped as well as meshed heating grids, at significantlyreduced computational costs that are acceptable for online control. Our stateestimation approach thus enables tightening the safety margins for temperatureand pressure control and thereby a more efficient grid operation.", "output": "Deep Learning-enabled MCMC for Probabilistic State Estimation in District Heating Grids."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In adaptive data analysis, a mechanism gets $n$ i.i.d. samples from anunknown distribution $D$, and is required to provide accurate estimations to asequence of adaptively chosen statistical queries with respect to $D$. Hardtand Ullman (FOCS 2014) and Steinke and Ullman (COLT 2015) showed that ingeneral, it is computationally hard to answer more than $Theta(n^2)$ adaptivequeries, assuming the existence of one-way functions.However, these negative results strongly rely on an adversarial model thatsignificantly advantages the adversarial analyst over the mechanism, as theanalyst, who chooses the adaptive queries, also chooses the underlyingdistribution $D$. This imbalance raises questions with respect to theapplicability of the obtained hardness results -- an analyst who has completeknowledge of the underlying distribution $D$ would have little need, if at all,to issue statistical queries to a mechanism which only holds a finite number ofsamples from $D$.We consider more restricted adversaries, called emph{balanced}, where eachsuch adversary consists of two separated algorithms: The emph{sampler} who isthe entity that chooses the distribution and provides the samples to themechanism, and the emph{analyst} who chooses the adaptive queries, but doesnot have a prior knowledge of the underlying distribution. We improve thequality of previous lower bounds by revisiting them using an efficientemph{balanced} adversary, under standard public-key cryptography assumptions.We show that these stronger hardness assumptions are unavoidable in the sensethat any computationally bounded emph{balanced} adversary that has thestructure of all known attacks, implies the existence of public-keycryptography.", "output": "Adaptive Data Analysis in a Balanced Adversarial Model."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Open-world survival games pose significant challenges for AI algorithms dueto their multi-tasking, deep exploration, and goal prioritization requirements.Despite reinforcement learning (RL) being popular for solving games, its highsample complexity limits its effectiveness in complex open-world games likeCrafter or Minecraft. We propose a novel approach, SPRING, to read the game'soriginal academic paper and use the knowledge learned to reason and play thegame through a large language model (LLM). Prompted with the LaTeX source asgame context and a description of the agent's current observation, our SPRINGframework employs a directed acyclic graph (DAG) with game-related questions asnodes and dependencies as edges. We identify the optimal action to take in theenvironment by traversing the DAG and calculating LLM responses for each nodein topological order, with the LLM's answer to final node directly translatingto environment actions. In our experiments, we study the quality of in-context\"reasoning\" induced by different forms of prompts under the setting of theCrafter open-world environment. Our experiments suggest that LLMs, whenprompted with consistent chain-of-thought, have great potential in completingsophisticated high-level trajectories. Quantitatively, SPRING with GPT-4outperforms all state-of-the-art RL baselines, trained for 1M steps, withoutany training. Finally, we show the potential of games as a test bed for LLMs.", "output": "SPRING: GPT-4 Out-performs RL Algorithms by Studying Papers and Reasoning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work presents two novel approaches for the symplectic model reduction ofhigh-dimensional Hamiltonian systems using data-driven quadratic manifolds.Classical symplectic model reduction approaches employ linear symplecticsubspaces for representing the high-dimensional system states in areduced-dimensional coordinate system. While these approximations respect thesymplectic nature of Hamiltonian systems, the linearity of the approximationimposes a fundamental limitation to the accuracy that can be achieved. Wepropose two different model reduction methods based on recently developedquadratic manifolds, each presenting its own advantages and limitations. Theaddition of quadratic terms in the state approximation, which sits at the heartof the proposed methodologies, enables us to better represent intrinsiclow-dimensionality in the problem at hand. Both approaches are effective forissuing predictions in settings well outside the range of their training datawhile providing more accurate solutions than the linear symplecticreduced-order models.", "output": "Symplectic model reduction of Hamiltonian systems using data-driven quadratic manifolds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper addresses the problem of selective classification for deep neuralnetworks, where a model is allowed to abstain from low-confidence predictionsto avoid potential errors. Specifically, we tackle the problem of optimizingthe confidence estimator of a fixed classifier, aiming to enhance itsmisclassification detection performance, i.e., its ability to discriminatebetween correct and incorrect predictions by assigning higher confidence valuesto the correct ones. Previous work has found that different classifiers exhibitvarying levels of misclassification detection performance, particularly whenusing the maximum softmax probability (MSP) as a measure of confidence.However, we argue that these findings are mainly due to a sub-optimalconfidence estimator being used for each model. To overcome this issue, wepropose a simple and efficient post-hoc confidence estimator, named$p$-NormSoftmax, which consists of transforming the logits through $p$-normnormalization and temperature scaling, followed by taking the MSP, where $p$and the temperature are optimized based on a hold-out set. This estimator canbe easily applied on top of an already trained model and, in many cases, cansignificantly improve its selective classification performance. When applied to84 pretrained Imagenet classifiers, our method yields an average improvement of16% in the area under the risk-coverage curve (AURC), exceeding 40% for somemodels. Furthermore, after applying $p$-NormSoftmax, we observe that thesemodels exhibit approximately the same level of misclassification detectionperformance, implying that a model's selective classification performance isalmost entirely determined by its accuracy at full coverage.", "output": "Improving selective classification performance of deep neural networks through post-hoc logit normalization and temperature scaling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (LLMs) can capture rich representations of conceptsthat are useful for real-world tasks. However, language alone is limited. Whileexisting LLMs excel at text-based inferences, health applications require thatmodels be grounded in numerical data (e.g., vital signs, laboratory values inclinical domains; steps, movement in the wellness domain) that is not easily orreadily expressed as text in existing training corpus. We demonstrate that withonly few-shot tuning, a large language model is capable of grounding variousphysiological and behavioral time-series data and making meaningful inferenceson numerous health tasks for both clinical and wellness contexts. Using datafrom wearable and medical sensor recordings, we evaluate these capabilities onthe tasks of cardiac signal analysis, physical activity recognition, metaboliccalculation (e.g., calories burned), and estimation of stress reports andmental health screeners.", "output": "Large Language Models are Few-Shot Health Learners."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite Graph Neural Networks (GNNs) have achieved prominent success in manygraph-based learning problem, such as credit risk assessment in financialnetworks and fake news detection in social networks. However, the trained GNNsstill make errors and these errors may cause serious negative impact onsociety. textit{Model editing}, which corrects the model behavior on wronglypredicted target samples while leaving model predictions unchanged on unrelatedsamples, has garnered significant interest in the fields of computer vision andnatural language processing. However, model editing for graph neural networks(GNNs) is rarely explored, despite GNNs' widespread applicability. To fill thegap, we first observe that existing model editing methods significantlydeteriorate prediction accuracy (up to $50%$ accuracy drop) in GNNs while aslight accuracy drop in multi-layer perception (MLP). The rationale behind thisobservation is that the node aggregation in GNNs will spread the editing effectthroughout the whole graph. This propagation pushes the node representation farfrom its original one. Motivated by this observation, we proposeunderline{E}ditable underline{G}raph underline{N}eural underline{N}etworks(EGNN), a neighbor propagation-free approach to correct the model prediction onmisclassified nodes. Specifically, EGNN simply stitches an MLP to theunderlying GNNs, where the weights of GNNs are frozen during model editing. Inthis way, EGNN disables the propagation during editing while still utilizingthe neighbor propagation scheme for node prediction to obtain satisfactoryresults. Experiments demonstrate that EGNN outperforms existing baselines interms of effectiveness (correcting wrong predictions with lower accuracy drop),generalizability (correcting wrong predictions for other similar nodes), andefficiency (low training time and memory) on various graph datasets.", "output": "Editable Graph Neural Network for Node Classifications."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As the use of online platforms continues to grow across all demographics,users often express a desire to feel represented in the content. To improverepresentation in search results and recommendations, we introduce end-to-enddiversification, ensuring that diverse content flows throughout the variousstages of these systems, from retrieval to ranking. We develop, experiment, anddeploy scalable diversification mechanisms in multiple production surfaces onthe Pinterest platform, including Search, Related Products, and New UserHomefeed, to improve the representation of different skin tones in beauty andfashion content. Diversification in production systems includes threecomponents: identifying requests that will trigger diversification, ensuringdiverse content is retrieved from the large content corpus during the retrievalstage, and finally, balancing the diversity-utility trade-off in aself-adjusting manner in the ranking stage. Our approaches, which evolved fromusing Strong-OR logical operator to bucketized retrieval at the retrieval stageand from greedy re-rankers to multi-objective optimization using determinantalpoint processes for the ranking stage, balances diversity and utility whileenabling fast iterations and scalable expansion to diversification overmultiple dimensions. Our experiments indicate that these approachessignificantly improve diversity metrics, with a neutral to a positive impact onutility metrics and improved user satisfaction, both qualitatively andquantitatively, in production.", "output": "Representation Online Matters: Practical End-to-End Diversification in Search and Recommender Systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the rapid increase in the size of neural networks, model compression hasbecome an important area of research. Quantization is an effective technique atdecreasing the model size, memory access, and compute load of large models.Despite recent advances in quantization aware training (QAT) technique, mostpapers present evaluations that are focused on computer vision tasks, whichhave different training dynamics compared to sequence tasks. In this paper, wefirst benchmark the impact of popular techniques such as straight throughestimator, pseudo-quantization noise, learnable scale parameter, clipping, etc.on 4-bit seq2seq models across a suite of speech recognition datasets rangingfrom 1,000 hours to 1 million hours, as well as one machine translation datasetto illustrate its applicability outside of speech.Through the experiments, we report that noise based QAT suffers when there isinsufficient regularization signal flowing back to the quantization scale. Wepropose low complexity changes to the QAT process to improve model accuracy(outperforming popular learnable scale and clipping methods). With the improvedaccuracy, it opens up the possibility to exploit some of the other benefits ofnoise based QAT: 1) training a single model that performs well in mixedprecision mode and 2) improved generalization on long form speech recognition.", "output": "RAND: Robustness Aware Norm Decay For Quantized Seq2seq Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing private synthetic data generation algorithms are agnostic todownstream tasks. However, end users may have specific requirements that thesynthetic data must satisfy. Failure to meet these requirements couldsignificantly reduce the utility of the data for downstream use. We introduce apost-processing technique that improves the utility of the synthetic data withrespect to measures selected by the end user, while preserving strong privacyguarantees and dataset quality. Our technique involves resampling from thesynthetic data to filter out samples that do not meet the selected utilitymeasures, using an efficient stochastic first-order algorithm to find optimalresampling weights. Through comprehensive numerical experiments, we demonstratethat our approach consistently improves the utility of synthetic data acrossmultiple benchmark datasets and state-of-the-art synthetic data generationalgorithms.", "output": "Post-processing Private Synthetic Data for Improving Utility on Selected Measures."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transfer learning involves adapting a pre-trained model to novel downstreamtasks. However, we observe that current transfer learning methods often fail tofocus on task-relevant features. In this work, we emphasize the importance ofrefocusing the attention in transfer learning. We introduce Top-Down AttentionSteering (TOAST), a novel transfer learning algorithm that keeps thepre-trained backbone frozen, while selecting the task-relevant elements in theoutput and feeding them back to the model to steer its attention to thetask-specific features. By refocusing the attention only, TOAST achievesstate-of-the-art results on a number of transfer learning benchmarks, whilehaving a small portion of tunable parameters. Compared to fully fine-tuning,LoRA, and prompt tuning, TOAST substantially improves performance across arange of fine-grained visual classification datasets (e.g., 81.1% -&gt; 86.2% onFGVC). TOAST also outperforms the fully fine-tuned Alpaca model oninstruction-following language generation. Code is available at", "output": "Refocusing Is Key to Transfer Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A crucial problem in reinforcement learning is learning the optimal policy.We study this in tabular infinite-horizon discounted Markov decision processesunder the online setting. The existing algorithms either fail to achieve regretoptimality or have to incur a high memory and computational cost. In addition,existing optimal algorithms all require a long burn-in time in order to achieveoptimal sample efficiency, i.e., their optimality is not guaranteed unlesssample size surpasses a high threshold. We address both open problems byintroducing a model-free algorithm that employs variance reduction and a noveltechnique that switches the execution policy in a slow-yet-adaptive manner.This is the first regret-optimal model-free algorithm in the discountedsetting, with the additional benefit of a low burn-in time.", "output": "Regret-Optimal Model-Free Reinforcement Learning for Discounted MDPs with Short Burn-In Time."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A growing body of evidence suggests that neural networks employed in deepreinforcement learning (RL) gradually lose their plasticity, the ability tolearn from new data; however, the analysis and mitigation of this phenomenon ishampered by the complex relationship between plasticity, exploration, andperformance in RL. This paper introduces plasticity injection, a minimalisticintervention that increases the network plasticity without changing the numberof trainable parameters or biasing the predictions. The applications of thisintervention are two-fold: first, as a diagnostic tool $unicode{x2014}$ ifinjection increases the performance, we may conclude that an agent's networkwas losing its plasticity. This tool allows us to identify a subset of Atarienvironments where the lack of plasticity causes performance plateaus,motivating future studies on understanding and combating plasticity loss.Second, plasticity injection can be used to improve the computationalefficiency of RL training if the agent has to re-learn from scratch due toexhausted plasticity or by growing the agent's network dynamically withoutcompromising performance. The results on Atari show that plasticity injectionattains stronger performance compared to alternative methods while beingcomputationally efficient.", "output": "Deep Reinforcement Learning with Plasticity Injection."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a novel non-parametric learning paradigm for the identification ofdrift and diffusion coefficients of non-linear stochastic differentialequations, which relies upon discrete-time observations of the state. The keyidea essentially consists of fitting a RKHS-based approximation of thecorresponding Fokker-Planck equation to such observations, yielding theoreticalestimates of learning rates which, unlike previous works, become increasinglytighter when the regularity of the unknown drift and diffusion coefficientsbecomes higher. Our method being kernel-based, offline pre-processing may inprinciple be profitably leveraged to enable efficient numerical implementation.", "output": "Non-Parametric Learning of Stochastic Differential Equations with Fast Rates of Convergence."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we study an optimal online resource reservation problem in asimple communication network. The network is composed of two compute nodeslinked by a local communication link. The system operates in discrete time; ateach time slot, the administrator reserves resources for servers before theactual job requests are known. A cost is incurred for the reservations made.Then, after the client requests are observed, jobs may be transferred from oneserver to the other to best accommodate the demands by incurring an additionaltransport cost. If certain job requests cannot be satisfied, there is aviolation that engenders a cost to pay for each of the blocked jobs. The goalis to minimize the overall reservation cost over finite horizons whilemaintaining the cumulative violation and transport costs under a certain budgetlimit. To study this problem, we first formalize it as a repeated game againstnature where the reservations are drawn randomly according to a sequence ofprobability distributions that are derived from an online optimization problemover the space of allowable reservations. We then propose an onlinesaddle-point algorithm for which we present an upper bound for the associatedK-benchmark regret together with an upper bound for the cumulative constraintviolations. Finally, we present numerical experiments where we compare theperformance of our algorithm with those of simple deterministic resourceallocation policies.", "output": "Online Optimization for Randomized Network Resource Allocation with Long-Term Constraints."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generating differentially private (DP) synthetic data that closely resemblesthe original private data without leaking sensitive user information is ascalable way to mitigate privacy concerns in the current data-driven world. Incontrast to current practices that train customized models for this task, weaim to generate DP Synthetic Data via APIs (DPSDA), where we treat foundationmodels as blackboxes and only utilize their inference APIs. Such API-based,training-free approaches are easier to deploy as exemplified by the recentsurge in the number of API-based apps. These approaches can also leverage thepower of large foundation models which are accessible via their inference APIswhile the model weights are unreleased. However, this comes with greaterchallenges due to strictly more restrictive model access and the additionalneed to protect privacy from the API provider.In this paper, we present a new framework called Private Evolution (PE) tosolve this problem and show its initial promise on synthetic images.Surprisingly, PE can match or even outperform state-of-the-art (SOTA) methodswithout any model training. For example, on CIFAR10 (with ImageNet as thepublic data), we achieve FID&lt;=7.9 with privacy cost epsilon=0.67, significantlyimproving the previous SOTA from epsilon=32. We further demonstrate the promiseof applying PE on large foundation models such as Stable Diffusion to tacklechallenging private datasets with a small number of high-resolution images.", "output": "Differentially Private Synthetic Data via Foundation Model APIs 1: Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Conditional graph generation tasks involve training a model to generate agraph given a set of input conditions. Many previous studies employautoregressive models to incrementally generate graph components such as nodesand edges. However, as graphs typically lack a natural ordering among theircomponents, converting a graph into a sequence of tokens is notstraightforward. While prior works mostly rely on conventional heuristics orgraph traversal methods like breadth-first search (BFS) or depth-first search(DFS) to convert graphs to sequences, the impact of ordering on graphgeneration has largely been unexplored. This paper contributes to this problemby: (1) highlighting the crucial role of ordering in autoregressive graphgeneration models, (2) proposing a novel theoretical framework that perceivesordering as a dimensionality reduction problem, thereby facilitating a deeperunderstanding of the relationship between orderings and generated graphaccuracy, and (3) introducing \"latent sort,\" a learning-based ordering schemeto perform dimensionality reduction of graph tokens. Our experimental resultsshowcase the effectiveness of latent sort across a wide range of graphgeneration tasks, encouraging future works to further explore and developlearning-based ordering schemes for autoregressive graph generation.", "output": "Let There Be Order: Rethinking Ordering in Autoregressive Graph Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Current algorithms and architecture can create excellent DNN classifiermodels from example data. In general, larger training datasets result in bettermodel estimations, which improve test performance. Existing methods forpredicting generalization performance are based on hold-out test examples. Tothe best of our knowledge, at present no method exists that can estimate thequality of a trained DNN classifier without test data. In this paper, we showthat the quality of a trained DNN classifier can be assessed without anyexample data. We consider DNNs to be composed of a feature extractor and afeature classifier; the feature extractor's output is fed to the classifier.The proposed method iteratively creates class prototypes in the input space foreach class by minimizing a cross-entropy loss function at the output of thenetwork. We use these prototypes and their feature relationships to reveal thequality of the classifier. We have developed two metrics: one using thefeatures of the prototypes and the other using adversarial examplescorresponding to each prototype. Empirical evaluations show that accuracyobtained from test examples is directly proportional to quality measuresobtained from the proposed metrics. We report our observations for ResNet18with Tiny ImageNet, CIFAR100, and CIFAR10 datasets. The proposed metrics can beused to compare performances of two or more classifiers without test examples.", "output": "Fantastic DNN Classifiers and How to Identify them without Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The research in Deep Learning applications in sound and music computing havegathered an interest in the recent years; however, there is still a missinglink between these new technologies and on how they can be incorporated intoreal-world artistic practices. In this work, we explore a well-known DeepLearning architecture called Variational Autoencoders (VAEs). Thesearchitectures have been used in many areas for generating latent spaces wheredata points are organized so that similar data points locate closer to eachother. Previously, VAEs have been used for generating latent timbre spaces orlatent spaces of symbolic music excepts. Applying VAE to audio features oftimbre requires a vocoder to transform the timbre generated by the network toan audio signal, which is computationally expensive. In this work, we applyVAEs to raw audio data directly while bypassing audio feature extraction. Thisapproach allows the practitioners to use any audio recording while givingflexibility and control over the aesthetics through dataset curation. The lowercomputation time in audio signal generation allows the raw audio approach to beincorporated into real-time applications. In this work, we propose threestrategies to explore latent spaces of audio and timbre for sound designapplications. By doing so, our aim is to initiate a conversation on artisticapproaches and strategies to utilize latent audio spaces in sound and musicpractices.", "output": "Sound Design Strategies for Latent Audio Space Explorations Using Deep Learning Architectures."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A recent development in Bayesian optimization is the use of localoptimization strategies, which can deliver strong empirical performance onhigh-dimensional problems compared to traditional global strategies. The \"folkwisdom\" in the literature is that the focus on local optimization sidesteps thecurse of dimensionality; however, little is known concretely about the expectedbehavior or convergence of Bayesian local optimization routines. We first studythe behavior of the local approach, and find that the statistics of individuallocal solutions of Gaussian process sample paths are surprisingly good comparedto what we would expect to recover from global methods. We then present thefirst rigorous analysis of such a Bayesian local optimization algorithmrecently proposed by M\"uller et al. (2021), and derive convergence rates inboth the noisy and noiseless settings.", "output": "The Behavior and Convergence of Local Bayesian Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce Markov Neural Processes (MNPs), a new class of StochasticProcesses (SPs) which are constructed by stacking sequences of neuralparameterised Markov transition operators in function space. We prove thatthese Markov transition operators can preserve the exchangeability andconsistency of SPs. Therefore, the proposed iterative construction addssubstantial flexibility and expressivity to the original framework of NeuralProcesses (NPs) without compromising consistency or adding restrictions. Ourexperiments demonstrate clear advantages of MNPs over baseline models on avariety of tasks.", "output": "Deep Stochastic Processes via Functional Markov Transition Operators."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Stein Variational Gradient Descent (SVGD) can transport particles alongtrajectories that reduce the KL divergence between the target and particledistribution but requires the target score function to compute the update. Weintroduce a new perspective on SVGD that views it as a local estimator of thereversed KL gradient flow. This perspective inspires us to propose newestimators that use local linear models to achieve the same purpose. Theproposed estimators can be computed using only samples from the target andparticle distribution without needing the target score function. Our proposedvariational gradient estimators utilize local linear models, resulting incomputational simplicity while maintaining effectiveness comparable to SVGD interms of estimation biases. Additionally, we demonstrate that under a mildassumption, the estimation of high-dimensional gradient flow can be translatedinto a lower-dimensional estimation problem, leading to improved estimationaccuracy. We validate our claims with experiments on both simulated andreal-world datasets.", "output": "Variational Gradient Descent using Local Linear Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Annotating data for multi-label classification is prohibitively expensivebecause every category of interest must be confirmed to be present or absent.Recent work on single positive multi-label (SPML) learning shows that it ispossible to train effective multi-label classifiers using only one positivelabel per image. However, the standard benchmarks for SPML are derived fromtraditional multi-label classification datasets by retaining one positive labelfor each training example (chosen uniformly at random) and discarding all otherlabels. In realistic settings it is not likely that positive labels are chosenuniformly at random. This work introduces protocols for studying label bias inSPML and provides new empirical results.", "output": "Understanding Label Bias in Single Positive Multi-Label Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present Manifold Diffusion Fields (MDF), an approach to learn generativemodels of continuous functions defined over Riemannian manifolds. Leveraginginsights from spectral geometry analysis, we define an intrinsic coordinatesystem on the manifold via the eigen-functions of the Laplace-BeltramiOperator. MDF represents functions using an explicit parametrization formed bya set of multiple input-output pairs. Our approach allows to sample continuousfunctions on manifolds and is invariant with respect to rigid and isometrictransformations of the manifold. Empirical results on several datasets andmanifolds show that MDF can capture distributions of such functions with betterdiversity and fidelity than previous approaches.", "output": "Manifold Diffusion Fields."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In Lifelong Learning (LL), agents continually learn as they encounter newconditions and tasks. Most current LL is limited to a single agent that learnstasks sequentially. Dedicated LL machinery is then deployed to mitigate theforgetting of old tasks as new tasks are learned. This is inherently slow. Wepropose a new Shared Knowledge Lifelong Learning (SKILL) challenge, whichdeploys a decentralized population of LL agents that each sequentially learndifferent tasks, with all agents operating independently and in parallel. Afterlearning their respective tasks, agents share and consolidate their knowledgeover a decentralized communication network, so that, in the end, all agents canmaster all tasks. We present one solution to SKILL which uses LightweightLifelong Learning (LLL) agents, where the goal is to facilitate efficientsharing by minimizing the fraction of the agent that is specialized for anygiven task. Each LLL agent thus consists of a common task-agnostic immutablepart, where most parameters are, and individual task-specific modules thatcontain fewer parameters but are adapted to each task. Agents share theirtask-specific modules, plus summary information (\"task anchors\") representingtheir tasks in the common task-agnostic latent space of all agents. Receivingagents register each received task-specific module using the correspondinganchor. Thus, every agent improves its ability to solve new tasks each time newtask-specific modules and anchors are received. On a new, very challengingSKILL-102 dataset with 102 image classification tasks (5,033 classes in total,2,041,225 training, 243,464 validation, and 243,464 test images), we achievemuch higher (and SOTA) accuracy over 8 LL baselines, while also achieving nearperfect parallelization. Code and data can be found at", "output": "Lightweight Learner for Shared Knowledge Lifelong Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (LLMs) are excellent in-context learners. However, thesensitivity of data contained in prompts raises privacy concerns. Our workfirst shows that these concerns are valid: we instantiate a simple but highlyeffective membership inference attack against the data used to prompt LLMs. Toaddress this vulnerability, one could forego prompting and resort tofine-tuning LLMs with known algorithms for private gradient descent. However,this comes at the expense of the practicality and efficiency offered byprompting. Therefore, we propose to privately learn to prompt. We first showthat soft prompts can be obtained privately through gradient descent ondownstream data. However, this is not the case for discrete prompts. Thus, weorchestrate a noisy vote among an ensemble of LLMs presented with differentprompts, i.e., a flock of stochastic parrots. The vote privately transfers theflock's knowledge into a single public prompt. We show that LLMs prompted withour private algorithms closely match the non-private baselines. For example,using GPT3 as the base model, we achieve a downstream accuracy of 92.7% on thesst2 dataset with ($epsilon=0.147, delta=10^{-6}$)-differential privacy vs.95.2% for the non-private baseline. Through our experiments, we also show thatour prompt-based approach is easily deployed with existing commercial APIs.", "output": "Flocks of Stochastic Parrots: Differentially Private Prompt Learning for Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper explores the implicit bias of overparameterized neural networks ofdepth greater than two layers. Our framework considers a family of networks ofvarying depths that all have the same capacity but different implicitly definedrepresentation costs. The representation cost of a function induced by a neuralnetwork architecture is the minimum sum of squared weights needed for thenetwork to represent the function; it reflects the function space biasassociated with the architecture. Our results show that adding linear layers toa ReLU network yields a representation cost that favors functions that can beapproximated by a low-rank linear operator composed with a function with lowrepresentation cost using a two-layer network. Specifically, using a neuralnetwork to fit training data with minimum representation cost yields aninterpolating function that is nearly constant in directions orthogonal to alow-dimensional subspace. This means that the learned network willapproximately be a single- or multiple-index model. Our experiments show thatwhen this active subspace structure exists in the data, adding linear layerscan improve generalization and result in a network that is well-aligned withthe true active subspace.", "output": "Linear Neural Network Layers Promote Learning Single- and Multiple-Index Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reinforcement learning (RL) is an area of significant research interest, andsafe RL in particular is attracting attention due to its ability to handlesafety-driven constraints that are crucial for real-world applications. Thiswork proposes a novel approach to RL training, called control invariant set(CIS) enhanced RL, which leverages the advantages of utilizing the explicitform of CIS to improve stability guarantees and sampling efficiency.Furthermore, the robustness of the proposed approach is investigated in thepresence of uncertainty. The approach consists of two learning stages: offlineand online. In the offline stage, CIS is incorporated into the reward design,initial state sampling, and state reset procedures. This incorporation of CISfacilitates improved sampling efficiency during the offline training process.In the online stage, RL is retrained whenever the predicted next step state isoutside of the CIS, which serves as a stability criterion, by introducing aSafety Supervisor to examine the safety of the action and make necessarycorrections. The stability analysis is conducted for both cases, with andwithout uncertainty. To evaluate the proposed approach, we apply it to asimulated chemical reactor. The results show a significant improvement insampling efficiency during offline training and closed-loop stability guaranteein the online implementation, with and without uncertainty.", "output": "Control invariant set enhanced safe reinforcement learning: improved sampling efficiency, guaranteed stability and robustness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We contribute to the vastly growing field of machine learning for engineeringsystems by demonstrating that equivariant graph neural networks have thepotential to learn more accurate dynamic-interaction models than theirnon-equivariant counterparts. We benchmark two well-studied fluid-flow systems,namely 3D decaying Taylor-Green vortex and 3D reverse Poiseuille flow, andevaluate the models based on different performance measures, such as kineticenergy or Sinkhorn distance. In addition, we investigate different embeddingmethods of physical-information histories for equivariant models. We find thatwhile currently being rather slow to train and evaluate, equivariant modelswith our proposed history embeddings learn more accurate physical interactions.", "output": "Learning Lagrangian Fluid Mechanics with E($3$)-Equivariant Graph Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate the question of whether the knowledge learned by graph neuralnetworks (GNNs) from small graphs is generalizable to large graphs in the samedomain. Prior works suggest that the distribution shift, particularly in thedegree distribution, between graphs of different sizes can lead to performancedegradation in the graph classification task. However, this may not be the casefor biological datasets where the degrees are bounded and the distributionshift of degrees is small. Even with little degree distribution shift, ourobservations show that GNNs' performance on larger graphs from the samedatasets still degrades, suggesting other causes. In fact, there has been alack of exploration in real datasets to understand the types and properties ofdistribution shifts caused by various graph sizes. Furthermore, previousanalyses of size generalizability mostly focus on the spatial domain.To fill these gaps, we take the spectral perspective and study the sizegeneralizability of GNNs on biological data. We identify a distribution shiftbetween small and large graphs in the eigenvalues of the normalizedLaplacian/adjacency matrix, indicating a difference in the global nodeconnectivity, which is found to be correlated with the node closenesscentrality. We further find that despite of the variations in globalconnectivity, graphs of different sizes share similar local connectivity, whichcan be utilized to improve the size generalizability of GNNs. Based on ourspectral insights and empirical observations, we propose a model-agnosticstrategy, SIA, which uses size-irrelevant local structural features, i.e., thelocal closeness centrality of a node, to guide the learning process. Ourempirical results demonstrate that our strategy improves the graphclassification performance of various GNNs on small and large graphs whentraining with only small graphs.", "output": "Size Generalizability of Graph Neural Networks on Biological Data: Insights and Practices from the Spectral Perspective."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Bayesian optimization has attracted huge attention from diverse researchareas in science and engineering, since it is capable of finding a globaloptimum of an expensive-to-evaluate black-box function efficiently. In general,a probabilistic regression model, e.g., Gaussian processes, random forests, andBayesian neural networks, is widely used as a surrogate function to model anexplicit distribution over function evaluations given an input to estimate anda training dataset. Beyond the probabilistic regression-based Bayesianoptimization, density ratio estimation-based Bayesian optimization has beensuggested in order to estimate a density ratio of the groups relatively closeand relatively far to a global optimum. Developing this line of researchfurther, a supervised classifier can be employed to estimate a classprobability for the two groups instead of a density ratio. However, thesupervised classifiers used in this strategy tend to be overconfident for aglobal solution candidate. To solve this overconfidence problem, we proposedensity ratio estimation-based Bayesian optimization with semi-supervisedlearning. Finally, we demonstrate the experimental results of our methods andseveral baseline methods in two distinct scenarios with unlabeled pointsampling and a fixed-size pool.", "output": "Density Ratio Estimation-based Bayesian Optimization with Semi-Supervised Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper presents an approach to learning nD features equivariant underorthogonal transformations for point cloud analysis, utilizing hyperspheres andregular n-simplexes. Our main contributions are theoretical and tackle majorissues in geometric deep learning such as equivariance and invariance undergeometric transformations. Namely, we enrich the recently developed theory ofsteerable 3D spherical neurons -- SO(3)-equivariant filter banks based onneurons with spherical decision surfaces -- by extending said neurons to nD,which we call deep equivariant hyperspheres, and enabling their stacking inmultiple layers. Using the ModelNet40 benchmark, we experimentally verify ourtheoretical contributions and show a potential practical configuration of theproposed equivariant hyperspheres.", "output": "Deep Equivariant Hyperspheres."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Self-supervised learning (SSL) is a powerful tool in machine learning, butunderstanding the learned representations and their underlying mechanismsremains a challenge. This paper presents an in-depth empirical analysis ofSSL-trained representations, encompassing diverse models, architectures, andhyperparameters. Our study reveals an intriguing aspect of the SSL trainingprocess: it inherently facilitates the clustering of samples with respect tosemantic labels, which is surprisingly driven by the SSL objective'sregularization term. This clustering process not only enhances downstreamclassification but also compresses the data information. Furthermore, weestablish that SSL-trained representations align more closely with semanticclasses rather than random classes. Remarkably, we show that learnedrepresentations align with semantic classes across various hierarchical levels,and this alignment increases during training and when moving deeper into thenetwork. Our findings provide valuable insights into SSL's representationlearning mechanisms and their impact on performance across different sets ofclasses.", "output": "Reverse Engineering Self-Supervised Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent works have shown that physics-inspired architectures allow thetraining of deep graph neural networks (GNNs) without oversmoothing. The roleof these physics is unclear, however, with successful examples of bothreversible (e.g., Hamiltonian) and irreversible (e.g., diffusion) phenomenaproducing comparable results despite diametrically opposed mechanisms, andfurther complications arising due to empirical departures from mathematicaltheory. This work presents a series of novel GNN architectures based uponstructure-preserving bracket-based dynamical systems, which are provablyguaranteed to either conserve energy or generate positive dissipation withincreasing depth. It is shown that the theoretically principled frameworkemployed here allows for inherently explainable constructions, whichcontextualize departures from theory in current architectures and betterelucidate the roles of reversibility and irreversibility in networkperformance.", "output": "Reversible and irreversible bracket-based dynamics for deep graph neural networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As the adoption of AI systems within the clinical setup grows, limitations inbandwidth could create communication bottlenecks when streaming imaging data,leading to delays in patient diagnosis and treatment. As such, healthcareproviders and AI vendors will require greater computational infrastructure,therefore dramatically increasing costs. To that end, we developed intelligentstreaming, a state-of-the-art framework to enable accelerated, cost-effective,bandwidth-optimized, and computationally efficient AI inference for clinicaldecision making at scale. For classification, intelligent streaming reduced thedata transmission by 99.01% and decoding time by 98.58%, while increasingthroughput by 27.43x. For segmentation, our framework reduced data transmissionby 90.32%, decoding time by 90.26%, while increasing throughput by 4.20x. Ourwork demonstrates that intelligent streaming results in faster turnaroundtimes, and reduced overall cost of data and transmission, without negativelyimpacting clinical decision making using AI systems.", "output": "High-Throughput AI Inference for Medical Image Classification and Segmentation using Intelligent Streaming."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We introduce a two-stage probabilistic framework for statistical downscalingbetween unpaired data. Statistical downscaling seeks a probabilistic map totransform low-resolution data from a (possibly biased) coarse-grained numericalscheme to high-resolution data that is consistent with a high-fidelity scheme.Our framework tackles the problem by tandeming two transformations: a debiasingstep that is performed by an optimal transport map, and an upsampling step thatis achieved by a probabilistic diffusion model with textit{a posteriori}conditional sampling. This approach characterizes a conditional distributionwithout the need for paired data, and faithfully recovers relevant physicalstatistics from biased samples. We demonstrate the utility of the proposedapproach on one- and two-dimensional fluid flow problems, which arerepresentative of the core difficulties present in numerical simulations ofweather and climate. Our method produces realistic high-resolution outputs fromlow-resolution inputs, by upsampling resolutions of $8times$ and $16times$.Moreover, our procedure correctly matches the statistics of physicalquantities, even when the low-frequency content of the inputs and outputs donot match, a crucial but difficult-to-satisfy assumption needed by currentstate-of-the-art alternatives.", "output": "Debias Coarsely, Sample Conditionally: Statistical Downscaling through Optimal Transport and Probabilistic Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider offline Reinforcement Learning (RL), where the agent does notinteract with the environment and must rely on offline data collected using abehavior policy. Previous works provide policy evaluation guarantees when thetarget policy to be evaluated is covered by the behavior policy, that is,state-action pairs visited by the target policy must also be visited by thebehavior policy. We show that when the MDP has a latent low-rank structure,this coverage condition can be relaxed. Building on the connection to weightedmatrix completion with non-uniform observations, we propose an offline policyevaluation algorithm that leverages the low-rank structure to estimate thevalues of uncovered state-action pairs. Our algorithm does not require a knownfeature representation, and our finite-sample error bound involves a noveldiscrepancy measure quantifying the discrepancy between the behavior and targetpolicies in the spectral space. We provide concrete examples where ouralgorithm achieves accurate estimation while existing coverage conditions arenot satisfied. Building on the above evaluation algorithm, we further design anoffline policy optimization algorithm and provide non-asymptotic performanceguarantees.", "output": "Matrix Estimation for Offline Reinforcement Learning with Low-Rank Structure."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Given the growing concerns about fairness in machine learning and theimpressive performance of Graph Neural Networks (GNNs) on graph data learning,algorithmic fairness in GNNs has attracted significant attention. While manyexisting studies improve fairness at the group level, only a few works promoteindividual fairness, which renders similar outcomes for similar individuals. Adesirable framework that promotes individual fairness should (1) balancebetween fairness and performance, (2) accommodate two commonly-used individualsimilarity measures (externally annotated and computed from input features),(3) generalize across various GNN models, and (4) be computationally efficient.Unfortunately, none of the prior work achieves all the desirables. In thiswork, we propose a novel method, GFairHint, which promotes individual fairnessin GNNs and achieves all aforementioned desirables. GFairHint learns fairnessrepresentations through an auxiliary link prediction task, and thenconcatenates the representations with the learned node embeddings in originalGNNs as a \"fairness hint\". Through extensive experimental investigations onfive real-world graph datasets under three prevalent GNN models covering bothindividual similarity measures above, GFairHint achieves the best fairnessresults in almost all combinations of datasets with various backbone models,while generating comparable utility results, with much less computational costcompared to the previous state-of-the-art (SoTA) method.", "output": "GFairHint: Improving Individual Fairness for Graph Neural Networks via Fairness Hint."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Problem definition: Access to accurate predictions of patients' outcomes canenhance medical staff's decision-making, which ultimately benefits allstakeholders in the hospitals. A large hospital network in the US has beencollaborating with academics and consultants to predict short-term andlong-term outcomes for all inpatients across their seven hospitals.Methodology/results: We develop machine learning models that predict theprobabilities of next 24-hr/48-hr discharge and intensive care unit transfers,end-of-stay mortality and discharge dispositions. All models achieve highout-of-sample AUC (75.7%-92.5%) and are well calibrated. In addition, combining48-hr discharge predictions with doctors' predictions simultaneously enablesmore patient discharges (10%-28.7%) and fewer 7-day/30-day readmissions($p$-value $&lt;0.001$). We implement an automated pipeline that extracts data andupdates predictions every morning, as well as user-friendly software and acolor-coded alert system to communicate these patient-level predictions(alongside explanations) to clinical teams. Managerial implications: Since wehave been gradually deploying the tool, and training medical staff, over 200doctors, nurses, and case managers across seven hospitals use it in their dailypatient review process. We observe a significant reduction in the averagelength of stay (0.67 days per patient) following its adoption and anticipatesubstantial financial benefits (between $55 and $72 million annually) for thehealthcare system.", "output": "Patient Outcome Predictions Improve Operations at a Large Hospital Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work presents a comprehensive theoretical analysis of graph p-Laplacianbased framelet network (pL-UFG) to establish a solid understanding of itsproperties. We begin by conducting a convergence analysis of the p-Laplacianbased implicit layer integrated after the framelet convolution, providinginsights into the asymptotic behavior of pL-UFG. By exploring the generalizedDirichlet energy of pL-UFG, we demonstrate that the Dirichlet energy remainsnon-zero, ensuring the avoidance of over-smoothing issues in pL-UFG as itapproaches convergence. Furthermore, we elucidate the dynamic energyperspective through which the implicit layer in pL-UFG synergizes with graphframelets, enhancing the model's adaptability to both homophilic andheterophilic data. Remarkably, we establish that the implicit layer can beinterpreted as a generalized non-linear diffusion process, enabling trainingusing diverse schemes. These multifaceted analyses lead to unified conclusionsthat provide novel insights for understanding and implementing pL-UFG,contributing to advancements in the field of graph-based deep learning.", "output": "Revisiting Generalized p-Laplacian Regularized Framelet GCNs: Convergence, Energy Dynamic and Training with Non-Linear Diffusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Out-of-distribution (OOD) data poses serious challenges in deployed machinelearning models, so methods of predicting a model's performance on OOD datawithout labels are important for machine learning safety. While a number ofmethods have been proposed by prior work, they often underestimate the actualerror, sometimes by a large margin, which greatly impacts their applicabilityto real tasks. In this work, we identify pseudo-label shift, or the differencebetween the predicted and true OOD label distributions, as a key indicator tothis underestimation. Based on this observation, we introduce a novel methodfor estimating model performance by leveraging optimal transport theory,Confidence Optimal Transport (COT), and show that it provably provides morerobust error estimates in the presence of pseudo-label shift. Additionally, weintroduce an empirically-motivated variant of COT, Confidence Optimal Transportwith Thresholding (COTT), which applies thresholding to the individualtransport costs and further improves the accuracy of COT's error estimates. Weevaluate COT and COTT on a variety of standard benchmarks that induce varioustypes of distribution shift -- synthetic, novel subpopulation, and natural --and show that our approaches significantly outperform existing state-of-the-artmethods with an up to 3x lower prediction error.", "output": "Characterizing Out-of-Distribution Error via Optimal Transport."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The shift between the training and testing distributions is commonly due tosample selection bias, a type of bias caused by non-random sampling of examplesto be included in the training set. Although there are many approaches proposedto learn a classifier under sample selection bias, few address the case where asubset of labels in the training set are missing-not-at-random (MNAR) as aresult of the selection process. In statistics, Greene's method formulates thistype of sample selection with logistic regression as the prediction model.However, we find that simply integrating this method into a robustclassification framework is not effective for this bias setting. In this paper,we propose BiasCorr, an algorithm that improves on Greene's method by modifyingthe original training set in order for a classifier to learn under MNAR sampleselection bias. We provide theoretical guarantee for the improvement ofBiasCorr over Greene's method by analyzing its bias. Experimental results onreal-world datasets demonstrate that BiasCorr produces robust classifiers andcan be extended to outperform state-of-the-art classifiers that have beenproposed to train under sample selection bias.", "output": "A Robust Classifier Under Missing-Not-At-Random Sample Selection Bias."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning (FL) approaches for saddle point problems (SPP) haverecently gained in popularity due to the critical role they play in machinelearning (ML). Existing works mostly target smooth unconstrained objectives inEuclidean space, whereas ML problems often involve constraints or non-smoothregularization, which results in a need for composite optimization. Addressingthese issues, we propose Federated Dual Extrapolation (FeDualEx), an extra-stepprimal-dual algorithm, which is the first of its kind that encompasses bothsaddle point optimization and composite objectives under the FL paradigm. Boththe convergence analysis and the empirical evaluation demonstrate theeffectiveness of FeDualEx in these challenging settings. In addition, even forthe sequential version of FeDualEx, we provide rates for the stochasticcomposite saddle point setting which, to our knowledge, are not found in priorliterature.", "output": "Federated Composite Saddle Point Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "To ensure the out-of-distribution (OOD) generalization performance,traditional domain generalization (DG) methods resort to training on data frommultiple sources with different underlying distributions. And the success ofthose DG methods largely depends on the fact that there are diverse trainingdistributions. However, it usually needs great efforts to obtain enoughheterogeneous data due to the high expenses, privacy issues or the scarcity ofdata. Thus an interesting yet seldom investigated problem arises: how toimprove the OOD generalization performance when the perceived heterogeneity islimited. In this paper, we instantiate a new framework called few-domaingeneralization (FDG), which aims to learn a generalizable model from very fewdomains of novel tasks with the knowledge acquired from previous learningexperiences on base tasks. Moreover, we propose a Meta Adaptive Task Sampling(MATS) procedure to differentiate base tasks according to their semantic anddomain-shift similarity to the novel task. Empirically, we show that the newlyintroduced FDG framework can substantially improve the OOD generalizationperformance on the novel task and further combining MATS with episodic trainingcould outperform several state-of-the-art DG baselines on widely usedbenchmarks like PACS and DomainNet.", "output": "Meta Adaptive Task Sampling for Few-Domain Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Modern machine learning applications have seen a remarkable success ofoptimization algorithms that are designed to find flat minima. Motivated bythis paradigm, this work formulates and studies the algorithmic question of howto find flat minima. As an initial effort, this work adopts the trace ofhessian of the cost function as the measure of flatness, and formally definesthe notion of approximate flat minima. Under this notion, we then designalgorithms that find approximate flat minima efficiently. For general costfunctions, we present a gradient-based algorithm that finds an approximate flatlocal minimum efficiently. The main component of the algorithm is to usegradients computed from randomly perturbed iterates to estimate a directionthat leads to flatter minima. For the setting where the cost function is anempirical risk over training data, we present a faster algorithm that isinspired by a recently proposed practical algorithm called sharpness-awareminimization, supporting its success in practice.", "output": "How to escape sharp minima."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Offline-to-online reinforcement learning (RL), by combining the benefits ofoffline pretraining and online finetuning, promises enhanced sample efficiencyand policy performance. However, existing methods, effective as they are,suffer from suboptimal performance, limited adaptability, and unsatisfactorycomputational efficiency. We propose a novel framework, PROTO, which overcomesthe aforementioned limitations by augmenting the standard RL objective with aniteratively evolving regularization term. Performing a trust-region-styleupdate, PROTO yields stable initial finetuning and optimal final performance bygradually evolving the regularization term to relax the constraint strength. Byadjusting only a few lines of code, PROTO can bridge any offline policypretraining and standard off-policy RL finetuning to form a powerfuloffline-to-online RL pathway, birthing great adaptability to diverse methods.Simple yet elegant, PROTO imposes minimal additional computation and enableshighly efficient online finetuning. Extensive experiments demonstrate thatPROTO achieves superior performance over SOTA baselines, offering an adaptableand efficient offline-to-online RL framework.", "output": "PROTO: Iterative Policy Regularized Offline-to-Online Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In the early days of machine learning (ML), the emphasis was on developingcomplex algorithms to achieve best predictive performance. To understand andexplain the model results, one had to rely on post hoc explainabilitytechniques, which are known to have limitations. Recently, with the recognitionthat interpretability is just as important, researchers are compromising onsmall increases in predictive performance to develop algorithms that areinherently interpretable. While doing so, the ML community has rediscovered theuse of low-order functional ANOVA (fANOVA) models that have been known in thestatistical literature for some time. This paper starts with a description ofchallenges with post hoc explainability and reviews the fANOVA framework with afocus on main effects and second-order interactions. This is followed by anoverview of two recently developed techniques: Explainable Boosting Machines orEBM (Lou et al., 2013) and GAMI-Net (Yang et al., 2021b). The paper proposes anew algorithm, called GAMI-Lin-T, that also uses trees like EBM, but it doeslinear fits instead of piecewise constants within the partitions. There aremany other differences, including the development of a new interactionfiltering algorithm. Finally, the paper uses simulated and real datasets tocompare selected ML algorithms. The results show that GAMI-Lin-T and GAMI-Nethave comparable performances, and both are generally better than EBM.", "output": "Interpretable Machine Learning based on Functional ANOVA Framework: Algorithms and Comparisons."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a straightforward statistical test to detect certain violations ofthe assumption that the data are Independent and Identically Distributed (IID).The specific form of violation considered is common across real-worldapplications: whether the examples are ordered in the dataset such that almostadjacent examples tend to have more similar feature values (e.g. due todistributional drift, or attractive interactions between datapoints). Based ona k-Nearest Neighbors estimate, our approach can be used to audit anymultivariate numeric data as well as other data types (image, text, audio,etc.) that can be numerically represented, perhaps with model embeddings.Compared with existing methods to detect drift or auto-correlation, ourapproach is both applicable to more types of data and also able to detect awider variety of IID violations in practice. Code:", "output": "Detecting Dataset Drift and Non-IID Sampling via k-Nearest Neighbors."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While distributional reinforcement learning (RL) has demonstrated empiricalsuccess, the question of when and why it is beneficial has remained unanswered.In this work, we provide one explanation for the benefits of distributional RLthrough the lens of small-loss bounds, which scale with the instance-dependentoptimal cost. If the optimal cost is small, our bounds are stronger than thosefrom non-distributional approaches. As warmup, we show that learning the costdistribution leads to small-loss regret bounds in contextual bandits (CB), andwe find that distributional CB empirically outperforms the state-of-the-art onthree challenging tasks. For online RL, we propose a distributionalversion-space algorithm that constructs confidence sets using maximumlikelihood estimation, and we prove that it achieves small-loss regret in thetabular MDPs and enjoys small-loss PAC bounds in latent variable models.Building on similar insights, we propose a distributional offline RL algorithmbased on the pessimism principle and prove that it enjoys small-loss PACbounds, which exhibit a novel robustness property. For both online and offlineRL, our results provide the first theoretical benefits of learningdistributions even when we only need the mean for making decisions.", "output": "The Benefits of Being Distributional: Small-Loss Bounds for Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The federated learning (FL) paradigm emerges to preserve data privacy duringmodel training by only exposing clients' model parameters rather than originaldata. One of the biggest challenges in FL lies in the non-IID (not identicaland independently distributed) data (a.k.a., data heterogeneity) distributed onclients. To address this challenge, various personalized FL (pFL) methods areproposed such as similarity-based aggregation and model decoupling. The formerone aggregates models from clients of a similar data distribution. The laterone decouples a neural network (NN) model into a feature extractor and aclassifier. Personalization is captured by classifiers which are obtained bylocal training. To advance pFL, we propose a novel pFedSim (pFL based on modelsimilarity) algorithm in this work by combining these two kinds of methods.More specifically, we decouple a NN model into a personalized featureextractor, obtained by aggregating models from similar clients, and aclassifier, which is obtained by local training and used to estimate clientsimilarity. Compared with the state-of-the-art baselines, the advantages ofpFedSim include: 1) significantly improved model accuracy; 2) low communicationand computation overhead; 3) a low risk of privacy leakage; 4) no requirementfor any external public information. To demonstrate the superiority of pFedSim,extensive experiments are conducted on real datasets. The results validate thesuperb performance of our algorithm which can significantly outperformbaselines under various heterogeneous data settings.", "output": "pFedSim: Similarity-Aware Model Aggregation Towards Personalized Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multimodal Variational Autoencoders (VAEs) represent a promising group ofgenerative models that facilitate the construction of a tractable posteriorwithin the latent space, given multiple modalities. Daunhawer et al. (2022)demonstrate that as the number of modalities increases, the generative qualityof each modality declines. In this study, we explore an alternative approach toenhance the generative performance of multimodal VAEs by jointly modeling thelatent space of unimodal VAEs using score-based models (SBMs). The role of theSBM is to enforce multimodal coherence by learning the correlation among thelatent variables. Consequently, our model combines the superior generativequality of unimodal VAEs with coherent integration across different modalities.", "output": "Score-Based Multimodal Autoencoders."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent progress in music generation has been remarkably advanced by thestate-of-the-art MusicLM, which comprises a hierarchy of three LMs,respectively, for semantic, coarse acoustic, and fine acoustic modelings. Yet,sampling with the MusicLM requires processing through these LMs one by one toobtain the fine-grained acoustic tokens, making it computationally expensiveand prohibitive for a real-time generation. Efficient music generation with aquality on par with MusicLM remains a significant challenge. In this paper, wepresent MeLoDy (M for music; L for LM; D for diffusion), an LM-guided diffusionmodel that generates music audios of state-of-the-art quality meanwhilereducing 95.7% or 99.6% forward passes in MusicLM, respectively, for sampling10s or 30s music. MeLoDy inherits the highest-level LM from MusicLM forsemantic modeling, and applies a novel dual-path diffusion (DPD) model and anaudio VAE-GAN to efficiently decode the conditioning semantic tokens intowaveform. DPD is proposed to simultaneously model the coarse and fine acousticsby incorporating the semantic information into segments of latents effectivelyvia cross-attention at each denoising step. Our experimental results suggestthe superiority of MeLoDy, not only in its practical advantages on samplingspeed and infinitely continuable generation, but also in its state-of-the-artmusicality, audio quality, and text correlation.Our samples are available at ", "output": "Efficient Neural Music Generation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The term \"Code Mixed\" refers to the use of more than one language in the sametext. This phenomenon is predominantly observed on social media platforms, withan increasing amount of adaptation as time goes on. It is critical to detectforeign elements in a language and process them correctly, as a considerablenumber of individuals are using code-mixed languages that could not becomprehended by understanding one of those languages. In this work, we focus onlow-resource Hindi-English code-mixed language and enhancing the performance ofdifferent code-mixed natural language processing tasks such as sentimentanalysis, emotion recognition, and hate speech identification. We perform acomparative analysis of different Transformer-based language Models pre-trainedusing unsupervised approaches. We have included the code-mixed models likeHingBERT, HingRoBERTa, HingRoBERTa-Mixed, mBERT, and non-code-mixed models likeAlBERT, BERT, and RoBERTa for comparative analysis of code-mixed Hindi-Englishdownstream tasks. We report state-of-the-art results on respective datasetsusing HingBERT-based models which are specifically pre-trained on realcode-mixed text. Our HingBERT-based models provide significant improvementsthus highlighting the poor performance of vanilla BERT models on code-mixedtext.", "output": "Comparative Study of Pre-Trained BERT Models for Code-Mixed Hindi-English Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we study the setting in which data owners train machinelearning models collaboratively under a privacy notion called jointdifferential privacy [Kearns et al., 2018]. In this setting, the model trainedfor each data owner $j$ uses $j$'s data without privacy consideration and otherowners' data with differential privacy guarantees. This setting was initiatedin [Jain et al., 2021] with a focus on linear regressions. In this paper, westudy this setting for stochastic convex optimization (SCO). We present analgorithm that is a variant of DP-SGD [Song et al., 2013; Abadi et al., 2016]and provides theoretical bounds on its population loss. We compare ouralgorithm to several baselines and discuss for what parameter setups ouralgorithm is more preferred. We also empirically study joint differentialprivacy in the multi-class classification problem over two public datasets. Ourempirical findings are well-connected to the insights from our theoreticalresults.", "output": "Learning across Data Owners with Joint Differential Privacy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Several recent studies have elucidated why knowledge distillation (KD)improves model performance. However, few have researched the other advantagesof KD in addition to its improving model performance. In this study, we haveattempted to show that KD enhances the interpretability as well as the accuracyof models. We measured the number of concept detectors identified in networkdissection for a quantitative comparison of model interpretability. Weattributed the improvement in interpretability to the class-similarityinformation transferred from the teacher to student models. First, we confirmedthe transfer of class-similarity information from the teacher to student modelvia logit distillation. Then, we analyzed how class-similarity informationaffects model interpretability in terms of its presence or absence and degreeof similarity information. We conducted various quantitative and qualitativeexperiments and examined the results on different datasets, different KDmethods, and according to different measures of interpretability. Our researchshowed that KD models by large models could be used more reliably in variousfields.", "output": "On the Impact of Knowledge Distillation for Model Interpretability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Estimating average causal effects is a common practice to test newtreatments. However, the average effect ''masks'' important individualcharacteristics in the counterfactual distribution, which may lead to safety,fairness, and ethical concerns. This issue is exacerbated in the temporalsetting, where the treatment is sequential and time-varying, leading to anintricate influence on the counterfactual distribution. In this paper, wepropose a novel conditional generative modeling approach to capture the wholecounterfactual distribution, allowing efficient inference on certain statisticsof the counterfactual distribution. This makes the proposed approachparticularly suitable for healthcare and public policy making. Our generativemodeling approach carefully tackles the distribution mismatch in the observeddata and the targeted counterfactual distribution via a marginal structuralmodel. Our method outperforms state-of-the-art baselines on both synthetic andreal data.", "output": "Counterfactual Generative Models for Time-Varying Treatments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Explaining the decisions made by machine learning models for high-stakesapplications is critical for increasing transparency and guiding improvementsto these decisions. This is particularly true in the case of models for graphs,where decisions often depend on complex patterns combining rich structural andattribute data. While recent work has focused on designing so-called post-hocexplainers, the question of what constitutes a good explanation remains open.One intuitive property is that explanations should be sufficiently informativeto enable humans to approximately reproduce the predictions given the data.However, we show that post-hoc explanations do not achieve this goal as theirexplanations are highly dependent on fixed model parameters (e.g., learned GNNweights). To address this challenge, this paper proposes RAGE (Robust Ante-hocGraph Explainer), a novel and flexible ante-hoc explainer designed to discoverexplanations for a broad class of graph neural networks using bileveloptimization. RAGE is able to efficiently identify explanations that containthe full information needed for prediction while still enabling humans to rankthese explanations based on their influence. Our experiments, based on graphclassification and regression, show that RAGE explanations are more robust thanexisting post-hoc and ante-hoc approaches and often achieve similar or betteraccuracy than state-of-the-art models.", "output": "Robust Ante-hoc Graph Explainer using Bilevel Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The research explores the influence of preschool attendance (one year beforefull-time school) on the development of children during their first year ofschool. Using data collected by the Australian Early Development Census, thefindings show that areas with high proportions of preschool attendance tendedto have lower proportions of children with at least one developmentalvulnerability. Developmental vulnerablities include not being able to cope withthe school day (tired, hungry, low energy), unable to get along with others oraggressive behaviour, trouble with reading/writing or numbers. These findings,of course, vary by region. Using Data Analysis and Machine Learning, theresearchers were able to identify three distinct clusters within Queensland,each characterised by different socio-demographic variables influencing therelationship between preschool attendance and developmental vulnerability.These analyses contribute to understanding regions with high vulnerability andthe potential need for tailored policies or investments", "output": "Assessing the Spatial Structure of the Association between Attendance at Preschool and Childrens Developmental Vulnerabilities in Queensland Australia."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph Neural Networks (GNNs) are widely used for graph representationlearning in many application domains. The expressiveness of vanilla GNNs isupper-bounded by 1-dimensional Weisfeiler-Leman (1-WL) test as they operate onrooted subtrees through iterative message passing. In this paper, we empowerGNNs by injecting neighbor-connectivity information extracted from a new typeof substructure. We first investigate different kinds of connectivitiesexisting in a local neighborhood and identify a substructure called unionsubgraph, which is able to capture the complete picture of the 1-hopneighborhood of an edge. We then design a shortest-path-based substructuredescriptor that possesses three nice properties and can effectively encode thehigh-order connectivities in union subgraphs. By infusing the encoded neighborconnectivities, we propose a novel model, namely Union Subgraph Neural Network(UnionSNN), which is proven to be strictly more powerful than 1-WL indistinguishing non-isomorphic graphs. Additionally, the local encoding fromunion subgraphs can also be injected into arbitrary message-passing neuralnetworks (MPNNs) and Transformer-based models as a plugin. Extensiveexperiments on 17 benchmarks of both graph-level and node-level tasksdemonstrate that UnionSNN outperforms state-of-the-art baseline models, withcompetitive computational efficiency. The injection of our local encoding toexisting models is able to boost the performance by up to 11.09%.", "output": "Union Subgraph Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models (DMs) are widely used for generating high-quality imagedatasets. However, since they operate directly in the high-dimensional pixelspace, optimization of DMs is computationally expensive, requiring longtraining times. This contributes to large amounts of noise being injected intothe differentially private learning process, due to the composability propertyof differential privacy. To address this challenge, we propose training LatentDiffusion Models (LDMs) with differential privacy. LDMs use powerfulpre-trained autoencoders to reduce the high-dimensional pixel space to a muchlower-dimensional latent space, making training DMs more efficient and fast.Unlike [Ghalebikesabi et al., 2023] that pre-trains DMs with public data thenfine-tunes them with private data, we fine-tune only the attention modules ofLDMs at varying layers with privacy-sensitive data, reducing the number oftrainable parameters by approximately 96% compared to fine-tuning the entireDM. We test our algorithm on several public-private data pairs, such asImageNet as public data and CIFAR10 and CelebA as private data, and SVHN aspublic data and MNIST as private data. Our approach provides a promisingdirection for training more powerful, yet training-efficient differentiallyprivate DMs that can produce high-quality synthetic images.", "output": "Differentially Private Latent Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Time series prediction is a prevalent issue across various disciplines, suchas meteorology, traffic surveillance, investment, and energy production andconsumption. Many statistical and machine-learning strategies have beendeveloped to tackle this problem. However, these approaches either lackexplainability or exhibit less satisfactory performance when the predictionhorizon increases. To this end, we propose a novel plan for the designing ofnetworks' architecture based on transformations, possessing the potential toachieve an enhanced receptive field in learning which brings benefits to fusefeatures across scales. In this context, we introduce four differenttransformation mechanisms as bases to construct the learning model includingFourier Transform (FT), Singular Value Decomposition (SVD), matrixmultiplication and Conv block. Hence, we develop four learning models based onthe above building blocks, namely, FT-Matrix, FT-SVD, FT-Conv, and Conv-SVD.Note that the FT and SVD blocks are capable of learning global information,while the Conv blocks focus on learning local information. The matrix block issparsely designed to learn both global and local information simultaneously.The above Transformation Learning Networks (TLNets) have been extensivelytested and compared with multiple baseline models based on several real-worlddatasets and showed clear potential in long-range time-series forecasting.", "output": "TLNets: Transformation Learning Networks for long-range time-series prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Attention mechanisms have greatly improved the performance of deep-learningmodels on visual, NLP, and multimodal tasks while also providing tools to aidin the model's interpretability. In particular, attention scores over inputregions or concrete image features can be used to measure how much the attendedelements contribute to the model inference. The recently proposed ConceptTransformer (CT) generalizes the Transformer attention mechanism from suchlow-level input features to more abstract, intermediate-level latent conceptsthat better allow human analysts to more directly assess an explanation for thereasoning of the model about any particular output classification. However, theconcept learning employed by CT implicitly assumes that across every image in aclass, each image patch makes the same contribution to concepts thatcharacterize membership in that class. Instead of using the CT'simage-patch-centric concepts, object-centric concepts could lead to betterclassification performance as well as better explainability. Thus, we proposeConcept-Centric Transformers (CCT), a new family of concept transformers thatprovides more robust explanations and performance by integrating a novelconcept-extraction module based on object-centric learning. We test ourproposed CCT against the CT and several other existing approaches onclassification problems for MNIST (odd/even), CIFAR100 (super-classes), andCUB-200-2011 (bird species). Our experiments demonstrate that CCT not onlyachieves significantly better classification accuracy than all selectedbenchmark classifiers across all three of our test problems, but it generatesmore consistent concept-based explanations of classification output whencompared to CT.", "output": "Concept-Centric Transformers: Concept Transformers with Object-Centric Concept Learning for Interpretability."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Weakly supervised learning aims to empower machine learning when the perfectsupervision is unavailable, which has drawn great attention from researchers.Among various types of weak supervision, one of the most challenging cases isto learn from multiple unlabeled (U) datasets with only a little knowledge ofthe class priors, or U$^m$ learning for short. In this paper, we study theproblem of building an AUC (area under ROC curve) optimization model frommultiple unlabeled datasets, which maximizes the pairwise ranking ability ofthe classifier. We propose U$^m$-AUC, an AUC optimization approach thatconverts the U$^m$ data into a multi-label AUC optimization problem, and can betrained efficiently. We show that the proposed U$^m$-AUC is effectivetheoretically and empirically.", "output": "AUC Optimization from Multiple Unlabeled Datasets."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Medical image data are often limited due to the expensive acquisition andannotation process. Hence, training a deep-learning model with only raw datacan easily lead to overfitting. One solution to this problem is to augment theraw data with various transformations, improving the model's ability togeneralize to new data. However, manually configuring a generic augmentationcombination and parameters for different datasets is non-trivial due toinconsistent acquisition approaches and data distributions. Therefore,automatic data augmentation is proposed to learn favorable augmentationstrategies for different datasets while incurring large GPU overhead. To thisend, we present a novel method, called Dynamic Data Augmentation (DDAug), whichis efficient and has negligible computation cost. Our DDAug develops ahierarchical tree structure to represent various augmentations and utilizes anefficient Monte-Carlo tree searching algorithm to update, prune, and sample thetree. As a result, the augmentation pipeline can be optimized for each datasetautomatically. Experiments on multiple Prostate MRI datasets show that ourmethod outperforms the current state-of-the-art data augmentation strategies.", "output": "Dynamic Data Augmentation via MCTS for Prostate MRI Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Ensembling is among the most popular tools in machine learning (ML) due toits effectiveness in minimizing variance and thus improving generalization.Most ensembling methods for black-box base learners fall under the umbrella of\"stacked generalization,\" namely training an ML algorithm that takes theinferences from the base learners as input. While stacking has been widelyapplied in practice, its theoretical properties are poorly understood. In thispaper, we prove a novel result, showing that choosing the best stackedgeneralization from a (finite or finite-dimensional) family of stackedgeneralizations based on cross-validated performance does not perform \"muchworse\" than the oracle best. Our result strengthens and significantly extendsthe results in Van der Laan et al. (2007). Inspired by the theoreticalanalysis, we further propose a particular family of stacked generalizations inthe context of probabilistic forecasting, each one with a different sensitivityfor how much the ensemble weights are allowed to vary across items, timestampsin the forecast horizon, and quantiles. Experimental results demonstrate theperformance gain of the proposed method.", "output": "Theoretical Guarantees of Learning Ensembling Strategies with Applications to Time Series Forecasting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph neural networks (GNNs) have achieved remarkable success in varioustasks, however, their vulnerability to adversarial attacks raises concerns forthe real-world applications. Existing defense methods can resist some attacks,but suffer unbearable performance degradation under other unknown attacks. Thisis due to their reliance on either limited observed adversarial examples tooptimize (adversarial training) or specific heuristics to alter graph or modelstructures (graph purification or robust aggregation). In this paper, wepropose an Invariant causal DEfense method against adversarial Attacks (IDEA),providing a new perspective to address this issue. The method aims to learncausal features that possess strong predictability for labels and invariantpredictability across attacks, to achieve graph adversarial robustness. Throughmodeling and analyzing the causal relationships in graph adversarial attacks,we design two invariance objectives to learn the causal features. Extensiveexperiments demonstrate that our IDEA significantly outperforms all thebaselines under both poisoning and evasion attacks on five benchmark datasets,highlighting the strong and invariant predictability of IDEA. Theimplementation of IDEA is available at", "output": "IDEA: Invariant Causal Defense for Graph Adversarial Robustness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In recent years, numerous screening methods have been published forultrahigh-dimensional data that contain hundreds of thousands of features;however, most of these features cannot handle data with thousands of classes.Prediction models built to authenticate users based on multichannel biometricdata result in this type of problem. In this study, we present a novel methodknown as random forest-based multiround screening (RFMS) that can beeffectively applied under such circumstances. The proposed algorithm dividesthe feature space into small subsets and executes a series of partial modelbuilds. These partial models are used to implement tournament-based sorting andthe selection of features based on their importance. To benchmark RFMS, asynthetic biometric feature space generator known as BiometricBlender isemployed. Based on the results, the RFMS is on par with industry-standardfeature screening methods while simultaneously possessing many advantages overthese methods.", "output": "Feature space reduction method for ultrahigh-dimensional, multiclass data: Random forest-based multiround screening (RFMS)."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Exceptional text-to-image (T2I) generation results of Stable Diffusion models(SDMs) come with substantial computational demands. To resolve this issue,recent research on efficient SDMs has prioritized reducing the number ofsampling steps and utilizing network quantization. Orthogonal to thesedirections, this study highlights the power of classical architecturalcompression for general-purpose T2I synthesis by introducing block-removedknowledge-distilled SDMs (BK-SDMs). We eliminate several residual and attentionblocks from the U-Net of SDMs, obtaining over a 30% reduction in the number ofparameters, MACs per sampling step, and latency. We conduct distillation-basedpretraining with only 0.22M LAION pairs (fewer than 0.1% of the full trainingpairs) on a single A100 GPU. Despite being trained with limited resources, ourcompact models can imitate the original SDM by benefiting from transferredknowledge and achieve competitive results against larger multi-billionparameter models on the zero-shot MS-COCO benchmark. Moreover, we demonstratethe applicability of our lightweight pretrained models in personalizedgeneration with DreamBooth finetuning.", "output": "On Architectural Compression of Text-to-Image Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A successful tactic that is followed by the scientific community foradvancing AI is to treat games as problems, which has been proven to lead tovarious breakthroughs. We adapt this strategy in order to study Rocket League,a widely popular but rather under-explored 3D multiplayer video game with adistinct physics engine and complex dynamics that pose a significant challengein developing efficient and high-performance game-playing agents. In thispaper, we present Lucy-SKG, a Reinforcement Learning-based model that learnedhow to play Rocket League in a sample-efficient manner, outperforming by anotable margin the two highest-ranking bots in this game, namely Necto (2022bot champion) and its successor Nexto, thus becoming a state-of-the-art agent.Our contributions include: a) the development of a reward analysis andvisualization library, b) novel parameterizable reward shape functions thatcapture the utility of complex reward types via our proposed Kinesthetic RewardCombination (KRC) technique, and c) design of auxiliary neural architecturesfor training on reward prediction and state representation tasks in anon-policy fashion for enhanced efficiency in learning speed and performance. Byperforming thorough ablation studies for each component of Lucy-SKG, we showedtheir independent effectiveness in overall performance. In doing so, wedemonstrate the prospects and challenges of using sample-efficientReinforcement Learning techniques for controlling complex dynamical systemsunder competitive team-based multiplayer conditions.", "output": "Lucy-SKG: Learning to Play Rocket League Efficiently Using Deep Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autoregressive Transformers adopted in Large Language Models (LLMs) are hardto scale to long sequences. Despite several works trying to reduce theircomputational cost, most of LLMs still adopt attention layers between all pairsof tokens in the sequence, thus incurring a quadratic cost. In this study, wepresent a novel approach that dynamically prunes contextual information whilepreserving the model's expressiveness, resulting in reduced memory andcomputational requirements during inference. Our method employs a learnablemechanism that determines which uninformative tokens can be dropped from thecontext at any point across the generation process. By doing so, our approachnot only addresses performance concerns but also enhances interpretability,providing valuable insight into the model's decision-making process. Ourtechnique can be applied to existing pre-trained models through astraightforward fine-tuning process, and the pruning strength can be specifiedby a sparsity parameter. Notably, our empirical findings demonstrate that wecan effectively prune up to 80% of the context without significant performancedegradation on downstream tasks, offering a valuable tool for mitigatinginference costs. Our reference implementation achieves up to $2times$ increasein inference throughput and even greater memory savings.", "output": "Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider contextual bandit problems with knapsacks [CBwK], a problem whereat each round, a scalar reward is obtained and vector-valued costs aresuffered. The learner aims to maximize the cumulative rewards while ensuringthat the cumulative costs are lower than some predetermined cost constraints.We assume that contexts come from a continuous set, that costs can be signed,and that the expected reward and cost functions, while unknown, may beuniformly estimated -- a typical assumption in the literature. In this setting,total cost constraints had so far to be at least of order $T^{3/4}$, where $T$is the number of rounds, and were even typically assumed to depend linearly on$T$. We are however motivated to use CBwK to impose a fairness constraint ofequalized average costs between groups: the budget associated with thecorresponding cost constraints should be as close as possible to the naturaldeviations, of order $sqrt{T}$. To that end, we introduce a dual strategybased on projected-gradient-descent updates, that is able to deal withtotal-cost constraints of the order of $sqrt{T}$ up to poly-logarithmic terms.This strategy is more direct and simpler than existing strategies in theliterature. It relies on a careful, adaptive, tuning of the step size.", "output": "Small Total-Cost Constraints in Contextual Bandits with Knapsacks, with Application to Fairness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Heterogeneous Graph Neural Networks (HGNNs) are a class of powerful deeplearning methods widely used to learn representations of heterogeneous graphs.Despite the fast development of HGNNs, they still face some challenges such asover-smoothing, and non-robustness. Previous studies have shown that theseproblems can be reduced by using gradient regularization methods. However, theexisting gradient regularization methods focus on either graph topology or nodefeatures. There is no universal approach to integrate these features, whichseverely affects the efficiency of regularization. In addition, the inclusionof gradient regularization into HGNNs sometimes leads to some problems, such asan unstable training process, increased complexity and insufficient coverageregularized information. Furthermore, there is still short of a completetheoretical analysis of the effects of gradient regularization on HGNNs. Inthis paper, we propose a novel gradient regularization method called Grug,which iteratively applies regularization to the gradients generated by bothpropagated messages and the node features during the message-passing process.Grug provides a unified framework integrating graph topology and node features,based on which we conduct a detailed theoretical analysis of theireffectiveness. Specifically, the theoretical analyses elaborate the advantagesof Grug: 1) Decreasing sample variance during the training process (Stability);2) Enhancing the generalization of the model (Universality); 3) Reducing thecomplexity of the model (Simplicity); 4) Improving the integrity and diversityof graph information utilization (Diversity). As a result, Grug has thepotential to surpass the theoretical upper bounds set by DropMessage (AAAI-23Distinguished Papers). In addition, we evaluate Grug on five public real-worlddatasets with two downstream tasks.", "output": "Unifying gradient regularization for Heterogeneous Graph Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Lung cancer poses a significant global public health challenge, emphasizingthe importance of early detection for improved patient outcomes. Recentadvancements in deep learning algorithms have shown promising results inmedical image analysis. This study aims to explore the application of objectdetection particularly YOLOv5, an advanced object identification system, inmedical imaging for lung cancer identification. To train and evaluate thealgorithm, a dataset comprising chest X-rays and corresponding annotations wasobtained from Kaggle. The YOLOv5 model was employed to train an algorithmcapable of detecting cancerous lung lesions. The training process involvedoptimizing hyperparameters and utilizing augmentation techniques to enhance themodel's performance. The trained YOLOv5 model exhibited exceptional proficiencyin identifying lung cancer lesions, displaying high accuracy and recall rates.It successfully pinpointed malignant areas in chest radiographs, as validatedby a separate test set where it outperformed previous techniques. Additionally,the YOLOv5 model demonstrated computational efficiency, enabling real-timedetection and making it suitable for integration into clinical procedures. Thisproposed approach holds promise in assisting radiologists in the earlydiscovery and diagnosis of lung cancer, ultimately leading to prompt treatmentand improved patient outcomes.", "output": "Leveraging object detection for the identification of lung cancer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep Neural Networks (DNNs) generalization is known to be closely related tothe flatness of minima, leading to the development of Sharpness-AwareMinimization (SAM) for seeking flatter minima and better generalization. Inthis paper, we revisit the loss of SAM and propose a more general method,called WSAM, by incorporating sharpness as a regularization term. We prove itsgeneralization bound through the combination of PAC and Bayes-PAC techniques,and evaluate its performance on various public datasets. The resultsdemonstrate that WSAM achieves improved generalization, or is at least highlycompetitive, compared to the vanilla optimizer, SAM and its variants. The codeis available at", "output": "Sharpness-Aware Minimization Revisited: Weighted Sharpness as a Regularization Term."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Market making (MM) is an important research topic in quantitative finance,the agent needs to continuously optimize ask and bid quotes to provideliquidity and make profits. The limit order book (LOB) contains information onall active limit orders, which is an essential basis for decision-making. Themodeling of evolving, high-dimensional and low signal-to-noise ratio LOB datais a critical challenge. Traditional MM strategy relied on strong assumptionssuch as price process, order arrival process, etc. Previous reinforcementlearning (RL) works handcrafted market features, which is insufficient torepresent the market. This paper proposes a RL agent for market making with LOBdata. We leverage a neural network with convolutional filters and attentionmechanism (Attn-LOB) for feature extraction from LOB. We design a newcontinuous action space and a hybrid reward function for the MM task. Finally,we conduct comprehensive experiments on latency and interpretability, showingthat our agent has good applicability.", "output": "Market Making with Deep Reinforcement Learning from Limit Order Books."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph Neural Networks (GNNs) have emerged as a powerful tool forsemi-supervised node classification tasks. However, recent studies haverevealed various biases in GNNs stemming from both node features and graphtopology. In this work, we uncover a new bias - label position bias, whichindicates that the node closer to the labeled nodes tends to perform better. Weintroduce a new metric, the Label Proximity Score, to quantify this bias, andfind that it is closely related to performance disparities. To address thelabel position bias, we propose a novel optimization framework for learning alabel position unbiased graph structure, which can be applied to existing GNNs.Extensive experiments demonstrate that our proposed method not only outperformsbackbone methods but also significantly mitigates the issue of label positionbias in GNNs.", "output": "Towards Label Position Bias in Graph Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The generalization of neural networks is a central challenge in machinelearning, especially concerning the performance under distributions that differfrom training ones. Current methods, mainly based on the data-driven paradigmsuch as data augmentation, adversarial training, and noise injection, mayencounter limited generalization due to model non-smoothness. In this paper, wepropose to investigate generalization from a Partial Differential Equation(PDE) perspective, aiming to enhance it directly through the underlyingfunction of neural networks, rather than focusing on adjusting input data.Specifically, we first establish the connection between neural networkgeneralization and the smoothness of the solution to a specific PDE, namely``transport equation''. Building upon this, we propose a general framework thatintroduces adaptive distributional diffusion into transport equation to enhancethe smoothness of its solution, thereby improving generalization. In thecontext of neural networks, we put this theoretical framework into practice asPDE+ (textbf{PDE} with textbf{A}daptive textbf{D}istributionaltextbf{D}iffusion) which diffuses each sample into a distribution coveringsemantically similar inputs. This enables better coverage of potentiallyunobserved distributions in training, thus improving generalization beyondmerely data-driven methods. The effectiveness of PDE+ is validated in extensivesettings, including clean samples and various corruptions, demonstrating itssuperior performance compared to SOTA methods.", "output": "PDE+: Enhancing Generalization via PDE with Adaptive Distributional Diffusion."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Architectures that first convert point clouds to a grid representation andthen apply convolutional neural networks achieve good performance forradar-based object detection. However, the transfer from irregular point clouddata to a dense grid structure is often associated with a loss of information,due to the discretization and aggregation of points. In this paper, we proposea novel architecture, multi-scale KPPillarsBEV, that aims to mitigate thenegative effects of grid rendering. Specifically, we propose a novel gridrendering method, KPBEV, which leverages the descriptive power of kernel pointconvolutions to improve the encoding of local point cloud contexts during gridrendering. In addition, we propose a general multi-scale grid renderingformulation to incorporate multi-scale feature maps into convolutionalbackbones of detection networks with arbitrary grid rendering methods. Weperform extensive experiments on the nuScenes dataset and evaluate the methodsin terms of detection performance and computational complexity. The proposedmulti-scale KPPillarsBEV architecture outperforms the baseline by 5.37% and theprevious state of the art by 2.88% in Car AP4.0 (average precision for amatching threshold of 4 meters) on the nuScenes validation set. Moreover, theproposed single-scale KPBEV grid rendering improves the Car AP4.0 by 2.90% overthe baseline while maintaining the same inference speed.", "output": "Improved Multi-Scale Grid Rendering of Point Clouds for Radar Object Detection Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The approximation properties of infinitely wide shallow neural networksheavily depend on the choice of the activation function. To understand thisinfluence, we study embeddings between Barron spaces with different activationfunctions. These embeddings are proven by providing push-forward maps on themeasures $mu$ used to represent functions $f$. An activation function ofparticular interest is the rectified power unit ($operatorname{RePU}$) givenby $operatorname{RePU}_s(x)=max(0,x)^s$. For many commonly used activationfunctions, the well-known Taylor remainder theorem can be used to construct apush-forward map, which allows us to prove the embedding of the associatedBarron space into a Barron space with a $operatorname{RePU}$ as activationfunction. Moreover, the Barron spaces associated with the$operatorname{RePU}_s$ have a hierarchical structure similar to the Sobolevspaces $H^m$.", "output": "Embeddings between Barron spaces with higher order activation functions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This work presents a novel approach to tabular data prediction leveraginggraph structure learning and graph neural networks. Despite the prevalence oftabular data in real-world applications, traditional deep learning methodsoften overlook the potentially valuable associations between data instances.Such associations can offer beneficial insights for classification tasks, asinstances may exhibit similar patterns of correlations among features andtarget labels. This information can be exploited by graph neural networks,necessitating robust graph structures. However, existing studies primarilyfocus on improving graph structure from noisy data, largely neglecting thepossibility of deriving graph structures from tabular data. We present a novelsolution, Tabular Graph Structure Learning (TabGSL), to enhance tabular dataprediction by simultaneously learning instance correlation and featureinteraction within a unified framework. This is achieved through a proposedgraph contrastive learning module, along with transformer-based featureextractor and graph neural network. Comprehensive experiments conducted on 30benchmark tabular datasets demonstrate that TabGSL markedly outperforms bothtree-based models and recent deep learning-based tabular models. Visualizationsof the learned instance embeddings further substantiate the effectiveness ofTabGSL.", "output": "TabGSL: Graph Structure Learning for Tabular Data Prediction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dropout is a widely utilized regularization technique in the training ofneural networks, nevertheless, its underlying mechanism and its impact onachieving good generalization abilities remain poorly understood. In this work,we derive the stochastic modified equations for analyzing the dynamics ofdropout, where its discrete iteration process is approximated by a class ofstochastic differential equations. In order to investigate the underlyingmechanism by which dropout facilitates the identification of flatter minima, westudy the noise structure of the derived stochastic modified equation fordropout. By drawing upon the structural resemblance between the Hessian andcovariance through several intuitive approximations, we empirically demonstratethe universal presence of the inverse variance-flatness relation and theHessian-variance relation, throughout the training process of dropout. Thesetheoretical and empirical findings make a substantial contribution to ourunderstanding of the inherent tendency of dropout to locate flatter minima.", "output": "Stochastic Modified Equations and Dynamics of Dropout Algorithm."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "DPPs were introduced by Macchi as a model in quantum optics the 1970s. Sincethen, they have been widely used as models and subsampling tools in statisticsand computer science. Most applications require sampling from a DPP, and giventheir quantum origin, it is natural to wonder whether sampling a DPP on aquantum computer is easier than on a classical one. We focus here on DPPs overa finite state space, which are distributions over the subsets of${1,dots,N}$ parametrized by an $Ntimes N$ Hermitian kernel matrix. Vanillasampling consists in two steps, of respective costs $mathcal{O}(N^3)$ and$mathcal{O}(Nr^2)$ operations on a classical computer, where $r$ is the rankof the kernel matrix. A large first part of the current paper consists inexplaining why the state-of-the-art in quantum simulation of fermionic systemsalready yields quantum DPP sampling algorithms. We then modify existing quantumcircuits, and discuss their insertion in a full DPP sampling pipeline thatstarts from practical kernel specifications. The bottom line is that, with $P$(classical) parallel processors, we can divide the preprocessing cost by $P$and build a quantum circuit with $mathcal{O}(Nr)$ gates that sample a givenDPP, with depth varying from $mathcal{O}(N)$ to $mathcal{O}(rlog N)$depending on qubit-communication constraints on the target machine. We alsoconnect existing work on the simulation of superconductors to Pfaffian pointprocesses, which generalize DPPs and would be a natural addition to the machinelearner's toolbox. Finally, the circuits are empirically validated on aclassical simulator and on 5-qubit machines.", "output": "On sampling determinantal and Pfaffian point processes on a quantum computer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large language models (large LMs) are susceptible to producing text withhallucinated content. Self-contradiction, where the LM generates twocontradictory sentences within the same context, is an important form ofhallucination. In this work, we present a comprehensive analysis onself-contradiction for state-of-the-art, instruction-tuned LMs, includingevaluation, detection, and mitigation. To effectively triggerself-contradictions, we design a framework that constrains LMs to generateappropriate sentence pairs. Our evaluation on these sentence pairs reveals thatself-contradictions occur frequently across different LMs for both famous andlesser-known topics. Next, we prompt the LMs to detect self-contradictions. Ourresults indicate that ChatGPT and GPT-4 are able to accurately identifyself-contradictions, while Vicuna-13B struggles to do so. For example, with ourbest prompting method, ChatGPT achieves 91.0% precision and 80.5% recall on thesentence pairs generated by itself. To automatically mitigateself-contradictions, we develop an iterative algorithm that prompts the LMs toremove the detected self-contradictions from the generated text. Our algorithmsuccessfully revises the text such that self-contradictions are significantlyreduced, while maintaining its fluency and informativeness. Importantly, ourentire pipeline of triggering, detecting, and mitigating self-contradictions isapplicable to black-box LMs and does not require any external groundedknowledge.", "output": "Self-contradictory Hallucinations of Large Language Models: Evaluation, Detection and Mitigation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Several explanation methods such as Integrated Gradients (IG) can becharacterised as path-based methods, as they rely on a straight line betweenthe data and an uninformative baseline. However, when applied to languagemodels, these methods produce a path for each word of a sentencesimultaneously, which could lead to creating sentences from interpolated wordseither having no clear meaning, or having a significantly different meaningcompared to the original sentence. In order to keep the meaning of thesesentences as close as possible to the original one, we propose SequentialIntegrated Gradients (SIG), which computes the importance of each word in asentence by keeping fixed every other words, only creating interpolationsbetween the baseline and the word of interest. Moreover, inspired by thetraining procedure of several language models, we also propose to replace thebaseline token \"pad\" with the trained token \"mask\". While being a simpleimprovement over the original IG method, we show on various models and datasetsthat SIG proves to be a very effective method for explaining language models.", "output": "Sequential Integrated Gradients: a simple but effective method for explaining language models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, Unmanned Aerial Vehicles (UAVs) have shown impressive performancein many critical applications, such as surveillance, search and rescueoperations, environmental monitoring, etc. In many of these applications, theUAVs capture images as well as other sensory data and then send the dataprocessing requests to remote servers. Nevertheless, this approach is notalways practical in real-time-based applications due to unstable connections,limited bandwidth, limited energy, and strict end-to-end latency. One promisingsolution is to divide the inference requests into subtasks that can bedistributed among UAVs in a swarm based on the available resources. Moreover,these tasks create intermediate results that need to be transmitted reliably asthe swarm moves to cover the area. Our system model deals with real-timerequests, aiming to find the optimal transmission power that guarantees higherreliability and low latency. We formulate the Low Latency and High-Reliability(LLHR) distributed inference as an optimization problem, and due to thecomplexity of the problem, we divide it into three subproblems. In the firstsubproblem, we find the optimal transmit power of the connected UAVs withguaranteed transmission reliability. The second subproblem aims to find theoptimal positions of the UAVs in the grid, while the last subproblem finds theoptimal placement of the CNN layers in the available UAVs. We conduct extensivesimulations and compare our work to two baseline models demonstrating that ourmodel outperforms the competing models.", "output": "LLHR: Low Latency and High Reliability CNN Distributed Inference for Resource-Constrained UAV Swarms."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Extracting dense representations for terms and phrases is a task of greatimportance for knowledge discovery platforms targeting highly-technical fields.Dense representations are used as features for downstream components and havemultiple applications ranging from ranking results in search to summarization.Common approaches to create dense representations include trainingdomain-specific embeddings with self-supervised setups or using sentenceencoder models trained over similarity tasks. In contrast to static embeddings,sentence encoders do not suffer from the out-of-vocabulary (OOV) problem, butimpose significant computational costs. In this paper, we propose a fullyunsupervised approach to text encoding that consists of training smallcharacter-based models with the objective of reconstructing large pre-trainedembedding matrices. Models trained with this approach can not only match thequality of sentence encoders in technical domains, but are 5 times smaller andup to 10 times faster, even on high-end GPUs.", "output": "Extracting Text Representations for Terms and Phrases in Technical Domains."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Simulation-based inference (SBI) methods such as approximate Bayesiancomputation (ABC), synthetic likelihood, and neural posterior estimation (NPE)rely on simulating statistics to infer parameters of intractable likelihoodmodels. However, such methods are known to yield untrustworthy and misleadinginference outcomes under model misspecification, thus hindering theirwidespread applicability. In this work, we propose the first general approachto handle model misspecification that works across different classes of SBImethods. Leveraging the fact that the choice of statistics determines thedegree of misspecification in SBI, we introduce a regularized loss functionthat penalises those statistics that increase the mismatch between the data andthe model. Taking NPE and ABC as use cases, we demonstrate the superiorperformance of our method on high-dimensional time-series models that areartificially misspecified. We also apply our method to real data from the fieldof radio propagation where the model is known to be misspecified. We showempirically that the method yields robust inference in misspecified scenarios,whilst still being accurate when the model is well-specified.", "output": "Learning Robust Statistics for Simulation-based Inference under Model Misspecification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Off-policy learning (OPL) aims at finding improved policies from loggedbandit data, often by minimizing the inverse propensity scoring (IPS) estimatorof the risk. In this work, we investigate a smooth regularization for IPS, forwhich we derive a two-sided PAC-Bayes generalization bound. The bound istractable, scalable, interpretable and provides learning certificates. Inparticular, it is also valid for standard IPS without making the assumptionthat the importance weights are bounded. We demonstrate the relevance of ourapproach and its favorable performance through a set of learning tasks. Sinceour bound holds for standard IPS, we are able to provide insight into whenregularizing IPS is useful. Namely, we identify cases where regularizationmight not be needed. This goes against the belief that, in practice, clippedIPS often enjoys favorable performance than standard IPS in OPL.", "output": "Exponential Smoothing for Off-Policy Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Past research has identified a rich set of handcrafted linguistic featuresthat can potentially assist various tasks. However, their extensive numbermakes it difficult to effectively select and utilize existing handcraftedfeatures. Coupled with the problem of inconsistent implementation acrossresearch works, there has been no categorization scheme or generally-acceptedfeature names. This creates unwanted confusion. Also, most existing handcraftedfeature extraction libraries are not open-source or not actively maintained. Asa result, a researcher often has to build such an extraction system from theground up.We collect and categorize more than 220 popular handcrafted features groundedon past literature. Then, we conduct a correlation analysis study on severaltask-specific datasets and report the potential use cases of each feature.Lastly, we devise a multilingual handcrafted linguistic feature extractionsystem in a systematically expandable manner. We open-source our system forpublic access to a rich set of pre-implemented handcrafted features. Our systemis coined LFTK and is the largest of its kind. Find it atgithub.com/brucewlee/lftk.", "output": "LFTK: Handcrafted Features in Computational Linguistics."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we present GAROM, a new approach for reduced order modelling(ROM) based on generative adversarial networks (GANs). GANs have the potentialto learn data distribution and generate more realistic data. While widelyapplied in many areas of deep learning, little research is done on theirapplication for ROM, i.e. approximating a high-fidelity model with a simplerone. In this work, we combine the GAN and ROM framework, by introducing adata-driven generative adversarial model able to learn solutions to parametricdifferential equations. The latter is achieved by modelling the discriminatornetwork as an autoencoder, extracting relevant features of the input, andapplying a conditioning mechanism to the generator and discriminator networksspecifying the differential equation parameters. We show how to apply ourmethodology for inference, provide experimental evidence of the modelgeneralisation, and perform a convergence study of the method.", "output": "Generative Adversarial Reduced Order Modelling."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Domain generalization (DG) is a prevalent problem in real-world applications,which aims to train well-generalized models for unseen target domains byutilizing several source domains. Since domain labels, i.e., which domain eachdata point is sampled from, naturally exist, most DG algorithms treat them as akind of supervision information to improve the generalization performance.However, the original domain labels may not be the optimal supervision signaldue to the lack of domain heterogeneity, i.e., the diversity among domains. Forexample, a sample in one domain may be closer to another domain, its originallabel thus can be the noise to disturb the generalization learning. Althoughsome methods try to solve it by re-dividing domains and applying the newlygenerated dividing pattern, the pattern they choose may not be the mostheterogeneous due to the lack of the metric for heterogeneity. In this paper,we point out that domain heterogeneity mainly lies in variant features underthe invariant learning framework. With contrastive learning, we propose alearning potential-guided metric for domain heterogeneity by promoting learningvariant features. Then we notice the differences between seeking variance-basedheterogeneity and training invariance-based generalizable model. We thuspropose a novel method called Heterogeneity-based Two-stage ContrastiveLearning (HTCL) for the DG task. In the first stage, we generate the mostheterogeneous dividing pattern with our contrastive metric. In the secondstage, we employ an invariance-aimed contrastive learning by re-building pairswith the stable relation hinted by domains and classes, which better utilizesgenerated domain labels for generalization learning. Extensive experiments showHTCL better digs heterogeneity and yields great generalization performance.", "output": "Quantitatively Measuring and Contrastively Exploring Heterogeneity for Domain Generalization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Given samples from two joint distributions, we consider the problem ofOptimal Transportation (OT) between the corresponding distributions conditionedon a common variable. The objective of this work is to estimate the associatedtransport cost (Wasserstein distance) as well as the transport plan between theconditionals as a function of the conditioned value. Since matching conditionaldistributions is at the core of supervised training of discriminative modelsand (implicit) conditional-generative models, OT between conditionals has thepotential to be employed in diverse machine learning applications. However,since the conditionals involved in OT are implicitly specified via the jointsamples, it is challenging to formulate this problem, especially when (i) thevariable conditioned on is continuous and (ii) the marginal of this variable inthe two distributions is different. We overcome these challenges by employing aspecific kernel MMD (Maximum Mean Discrepancy) based regularizer that ensuresthe marginals of our conditional transport plan are close to the conditionalsspecified via the given joint samples. Under mild conditions, we prove that ourestimator for this regularized transport cost is statistically consistent andderive finite-sample bounds on the estimation error. Application-specificdetails for parameterizing our conditional transport plan are also presented.Furthermore, we empirically evaluate our methodology on benchmark datasets inapplications like classification, prompt learning for few-shot classification,and conditional-generation in the context of predicting cell responses tocancer treatment.", "output": "Empirical Optimal Transport between Conditional Distributions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Efficient utilisation of both intra- and extra-textual context remains one ofthe critical gaps between machine and human translation. Existing research hasprimarily focused on providing individual, well-defined types of context intranslation, such as the surrounding text or discrete external variables likethe speaker's gender. This work introduces MTCue, a novel neural machinetranslation (NMT) framework that interprets all context (including discretevariables) as text. MTCue learns an abstract representation of context,enabling transferability across different data settings and leveraging similarattributes in low-resource scenarios. With a focus on a dialogue domain withaccess to document and metadata context, we extensively evaluate MTCue in fourlanguage pairs in both translation directions. Our framework demonstratessignificant improvements in translation quality over a parameter-matchednon-contextual baseline, as measured by BLEU (+0.88) and Comet (+1.58).Moreover, MTCue significantly outperforms a \"tagging\" baseline at translatingEnglish text. Analysis reveals that the context encoder of MTCue learns arepresentation space that organises context based on specific attributes, suchas formality, enabling effective zero-shot control. Pre-training on contextembeddings also improves MTCue's few-shot performance compared to the \"tagging\"baseline. Finally, an ablation study conducted on model components andcontextual variables further supports the robustness of MTCue for context-basedNMT.", "output": "MTCue: Learning Zero-Shot Control of Extra-Textual Attributes by Leveraging Unstructured Context in Neural Machine Translation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we studied two identically-trained neural networks (i.e.networks with the same architecture, trained on the same dataset using the samealgorithm, but with different initialization) and found that their outputsdiscrepancy on the training dataset exhibits a \"double descent\" phenomenon. Wedemonstrated through extensive experiments across various tasks, datasets, andnetwork architectures that this phenomenon is prevalent. Leveraging thisphenomenon, we proposed a new early stopping criterion and developed a newmethod for data quality assessment. Our results show that a phenomenon-drivenapproach can benefit deep learning research both in theoretical understandingand practical applications.", "output": "Double Descent of Discrepancy: A Task-, Data-, and Model-Agnostic Phenomenon."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We examine the characteristic activation values of individual ReLU units inneural networks. We refer to the corresponding set for such characteristicactivation values in the input space as the characteristic activation set of aReLU unit. We draw an explicit connection between the characteristic activationset and learned features in ReLU networks. This connection leads to newinsights into why various neural network normalization techniques used inmodern deep learning architectures regularize and stabilize SGD optimization.Utilizing these insights, we propose a geometric approach to parameterize ReLUnetworks for improved feature learning. We empirically verify its usefulnesswith less carefully chosen initialization schemes and larger learning rates. Wereport improved optimization stability, faster convergence speed, and bettergeneralization performance.", "output": "Neural Characteristic Activation Value Analysis for Improved ReLU Network Feature Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative Adversarial Networks (GANs) have shown immense potential in fieldsfar from physics, such as in text and image generation. Here we use GANs tolearn a prototypical stochastic process on a lattice. By suitably adding noiseto the original data we succeed in bringing both the Generator and theDiscriminator loss functions close to their ideal value. However, as typicalfor adversarial approaches, oscillations persist. This undermines modelselection and the quality of the generated trajectory. We demonstrate that asuitable multi-model procedure where stochastic trajectories are advanced ateach step upon randomly selecting a Generator leads to a remarkable increase inaccuracy. Based on the reported findings GANs appears as a promising tool totackle complex statistical dynamics.", "output": "Learning and accurate generation of stochastic dynamics based on multi-model Generative Adversarial Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Unsupervised disentanglement is a long-standing challenge in representationlearning. Recently, self-supervised techniques achieved impressive results inthe sequential setting, where data is time-dependent. However, the lattermethods employ modality-based data augmentations and random sampling or solveauxiliary tasks. In this work, we propose to avoid that by generating,sampling, and comparing empirical distributions from the underlying variationalmodel. Unlike existing work, we introduce a self-supervised sequentialdisentanglement framework based on contrastive estimation with no externalsignals, while using common batch sizes and samples from the latent spaceitself. In practice, we propose a unified, efficient, and easy-to-code samplingstrategy for semantically similar and dissimilar views of the data. We evaluateour approach on video, audio, and time series benchmarks. Our method presentsstate-of-the-art results in comparison to existing techniques. The code isavailable at ", "output": "Sample and Predict Your Latent: Modality-free Sequential Disentanglement via Contrastive Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Identifiability of latent variable models has recently gained interest interms of its applications to interpretability or out of distributiongeneralisation. In this work, we study identifiability of Markov SwitchingModels as a first step towards extending recent results to sequential latentvariable models. We present identifiability conditions within first-orderMarkov dependency structures, and parametrise the transition distribution vianon-linear Gaussians. Our experiments showcase the applicability of ourapproach for regime-dependent causal discovery and high-dimensional time seriessegmentation.", "output": "On the Identifiability of Markov Switching Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Estimating the parameters of a probabilistic directed graphical model fromincomplete data remains a long-standing challenge. This is because, in thepresence of latent variables, both the likelihood function and posteriordistribution are intractable without further assumptions about structuraldependencies or model classes. While existing learning methods arefundamentally based on likelihood maximization, here we offer a new view of theparameter learning problem through the lens of optimal transport. Thisperspective licenses a framework that operates on many directed graphs withoutmaking unrealistic assumptions on the posterior over the latent variables orresorting to black-box variational approximations. We develop a theoreticalframework and support it with extensive empirical evidence demonstrating theflexibility and versatility of our approach. Across experiments, we show thatnot only can our method recover the ground-truth parameters but it alsoperforms competitively on downstream applications, notably the non-trivial taskof discrete representation learning.", "output": "Learning Directed Graphical Models with Optimal Transport."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Meta-Bayesian optimisation (meta-BO) aims to improve the sample efficiency ofBayesian optimisation by leveraging data from related tasks. While previousmethods successfully meta-learn either a surrogate model or an acquisitionfunction independently, joint training of both components remains an openchallenge. This paper proposes the first end-to-end differentiable meta-BOframework that generalises neural processes to learn acquisition functions viatransformer architectures. We enable this end-to-end framework withreinforcement learning (RL) to tackle the lack of labelled acquisition data.Early on, we notice that training transformer-based neural processes fromscratch with RL is challenging due to insufficient supervision, especially whenrewards are sparse. We formalise this claim with a combinatorial analysisshowing that the widely used notion of regret as a reward signal exhibits alogarithmic sparsity pattern in trajectory lengths. To tackle this problem, weaugment the RL objective with an auxiliary task that guides part of thearchitecture to learn a valid probabilistic model as an inductive bias. Wedemonstrate that our method achieves state-of-the-art regret results againstvarious baselines in experiments on standard hyperparameter optimisation tasksand also outperforms others in the real-world problems of mixed-integerprogramming tuning, antibody design, and logic synthesis for electronic designautomation.", "output": "End-to-End Meta-Bayesian Optimisation with Transformer Neural Processes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a novel perspective and algorithm for learning directed acyclicgraphs (DAGs) from data generated by a linear structural equation model (SEM).First, we show that a linear SEM can be viewed as a linear transform that, inprior work, computes the data from a dense input vector of random valued rootcauses (as we will call them) associated with the nodes. Instead, we considerthe case of (approximately) few root causes and also introduce noise in themeasurement of the data. Intuitively, this means that the DAG data is producedby few data-generating events whose effect percolates through the DAG. We proveidentifiability in this new setting and show that the true DAG is the globalminimizer of the $L^0$-norm of the vector of root causes. For data with fewroot causes, with and without noise, we show superior performance compared toprior DAG learning methods.", "output": "Learning DAGs from Data with Few Root Causes."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "This paper delves into stochastic optimization problems that involveMarkovian noise. We present a unified approach for the theoretical analysis offirst-order gradient methods for stochastic optimization and variationalinequalities. Our approach covers scenarios for both non-convex and stronglyconvex minimization problems. To achieve an optimal (linear) dependence on themixing time of the underlying noise sequence, we use the randomized batchingscheme, which is based on the multilevel Monte Carlo method. Moreover, ourtechnique allows us to eliminate the limiting assumptions of previous researchon Markov noise, such as the need for a bounded domain and uniformly boundedstochastic gradients. Our extension to variational inequalities under Markoviannoise is original. Additionally, we provide lower bounds that match the oraclecomplexity of our method in the case of strongly convex optimization problems.", "output": "First Order Methods with Markovian Noise: from Acceleration to Variational Inequalities."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Some of the most successful knowledge graph embedding (KGE) models for linkprediction -- CP, RESCAL, TuckER, ComplEx -- can be interpreted as energy-basedmodels. Under this perspective they are not amenable for exactmaximum-likelihood estimation (MLE), sampling and struggle to integrate logicalconstraints. This work re-interprets the score functions of these KGEs ascircuits -- constrained computational graphs allowing efficientmarginalisation. Then, we design two recipes to obtain efficient generativecircuit models by either restricting their activations to be non-negative orsquaring their outputs. Our interpretation comes with little or no loss ofperformance for link prediction, while the circuits framework unlocks exactlearning by MLE, efficient sampling of new triples, and guarantee that logicalconstraints are satisfied by design. Furthermore, our models scale moregracefully than the original KGEs on graphs with millions of entities.", "output": "How to Turn Your Knowledge Graph Embeddings into Generative Models via Probabilistic Circuits."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Online learning holds the promise of enabling efficient long-term creditassignment in recurrent neural networks. However, current algorithms fall shortof offline backpropagation by either not being scalable or failing to learnlong-range dependencies. Here we present a high-performance online learningalgorithm that merely doubles the memory and computational requirements of asingle inference pass. We achieve this by leveraging independent recurrentmodules in multi-layer networks, an architectural motif that has recently beenshown to be particularly powerful. Experiments on synthetic memory problems andon the challenging long-range arena benchmark suite reveal that our algorithmperforms competitively, establishing a new standard for what can be achievedthrough online learning. This ability to learn long-range dependencies offers anew perspective on learning in the brain and opens a promising avenue inneuromorphic computing.", "output": "Online learning of long-range dependencies."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite the increasing relevance of explainable AI, assessing the quality ofexplanations remains a challenging issue. Due to the high costs associated withhuman-subject experiments, various proxy metrics are often used toapproximately quantify explanation quality. Generally, one possibleinterpretation of the quality of an explanation is its inherent value forteaching a related concept to a student. In this work, we extend artificialsimulatability studies to the domain of graph neural networks. Instead ofcostly human trials, we use explanation-supervisable graph neural networks toperform simulatability studies to quantify the inherent usefulness ofattributional graph explanations. We perform an extensive ablation study toinvestigate the conditions under which the proposed analyses are mostmeaningful. We additionally validate our methods applicability on real-worldgraph classification and regression datasets. We find that relevantexplanations can significantly boost the sample efficiency of graph neuralnetworks and analyze the robustness towards noise and bias in the explanations.We believe that the notion of usefulness obtained from our proposedsimulatability analysis provides a dimension of explanation quality that islargely orthogonal to the common practice of faithfulness and has greatpotential to expand the toolbox of explanation quality assessments,specifically for graph explanations.", "output": "Quantifying the Intrinsic Usefulness of Attributional Explanations for Graph Neural Networks with Artificial Simulatability Studies."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing heterogeneous treatment effects learners, also known as conditionalaverage treatment effects (CATE) learners, lack a general mechanism forend-to-end inter-treatment information sharing, and data have to be split amongpotential outcome functions to train CATE learners which can lead to biasedestimates with limited observational datasets. To address this issue, wepropose a novel deep learning-based framework to train CATE learners thatfacilitates dynamic end-to-end information sharing among treatment groups. Theframework is based on textit{soft weight sharing} of textit{hypernetworks},which offers advantages such as parameter efficiency, faster training, andimproved results. The proposed framework complements existing CATE learners andintroduces a new class of uncertainty-aware CATE learners that we refer to astextit{HyperCATE}. We develop HyperCATE versions of commonly used CATElearners and evaluate them on IHDP, ACIC-2016, and Twins benchmarks. Ourexperimental results show that the proposed framework improves the CATEestimation error via counterfactual inference, with increasing effectivenessfor smaller datasets.", "output": "Dynamic Inter-treatment Information Sharing for Heterogeneous Treatment Effects Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recurrent neural networks are a powerful means to cope with time series. Weshow how autoregressive linear, i.e., linearly activated recurrent neuralnetworks (LRNNs) can approximate any time-dependent function f(t) given by anumber of function values. The approximation can effectively be learned bysimply solving a linear equation system; no backpropagation or similar methodsare needed. Furthermore, and this is probably the main contribution of thisarticle, the size of an LRNN can be reduced significantly in one step afterinspecting the spectrum of the network transition matrix, i.e., itseigenvalues, by taking only the most relevant components. Therefore, incontrast to other approaches, we do not only learn network weights but also thenetwork architecture. LRNNs have interesting properties: They end up in ellipsetrajectories in the long run and allow the prediction of further values andcompact representations of functions. We demonstrate this by severalexperiments, among them multiple superimposed oscillators (MSO), roboticsoccer, and predicting stock prices. LRNNs outperform the previousstate-of-the-art for the MSO task with a minimal number of units.", "output": "The Power of Linear Recurrent Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated learning algorithms are developed both for efficiency reasons andto ensure the privacy and confidentiality of personal and business data,respectively. Despite no data being shared explicitly, recent studies showedthat the mechanism could still leak sensitive information. Hence, secureaggregation is utilized in many real-world scenarios to prevent attribution tospecific participants. In this paper, we focus on the quality of individualtraining datasets and show that such quality information could be inferred andattributed to specific participants even when secure aggregation is applied.Specifically, through a series of image recognition experiments, we infer therelative quality ordering of participants. Moreover, we apply the inferredquality information to detect misbehaviours, to stabilize training performance,and to measure the individual contributions of participants.", "output": "Quality Inference in Federated Learning with Secure Aggregation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "I study a stochastic multi-arm bandit problem where rewards are subject toadversarial corruption. I propose a novel attack strategy that manipulates alearner employing the UCB algorithm into pulling some non-optimal target arm $T- o(T)$ times with a cumulative cost that scales as $widehat{O}(sqrt{logT})$, where $T$ is the number of rounds. I also prove the first lower bound onthe cumulative attack cost. The lower bound matches the upper bound up to$O(log log T)$ factors, showing the proposed attack strategy to be nearoptimal.", "output": "Near Optimal Adversarial Attack on UCB Bandits."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Data assimilation (DA) aims at forecasting the state of a dynamical system bycombining a mathematical representation of the system with noisy observationstaking into account their uncertainties. State of the art methods are based onthe Gaussian error statistics and the linearization of the non-linear dynamicswhich may lead to sub-optimal methods. In this respect, there are still openquestions how to improve these methods. In this paper, we propose a fully datadriven deep learning architecture generalizing recurrent Elman networks anddata assimilation algorithms which approximate a sequence of prior andposterior densities conditioned on noisy observations. By construction ourapproach can be used for general nonlinear dynamics and non-Gaussian densities.On numerical experiments based on the well-known Lorenz-95 system and withGaussian error statistics, our architecture achieves comparable performance toEnKF on both the analysis and the propagation of probability density functionsof the system state at a given time without using any explicit regularizationtechnique.", "output": "Data Assimilation Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "With the freight delivery demands and shipping costs increasing rapidly,intelligent control of fleets to enable efficient and cost-conscious solutionsbecomes an important problem. In this paper, we propose DeepFreight, amodel-free deep-reinforcement-learning-based algorithm for multi-transferfreight delivery, which includes two closely-collaborative components:truck-dispatch and package-matching. Specifically, a deep multi-agentreinforcement learning framework called QMIX is leveraged to learn a dispatchpolicy, with which we can obtain the multi-step joint vehicle dispatchdecisions for the fleet with respect to the delivery requests. Then anefficient multi-transfer matching algorithm is executed to assign the deliveryrequests to the trucks. Also, DeepFreight is integrated with a Mixed-IntegerLinear Programming optimizer for further optimization. The evaluation resultsshow that the proposed system is highly scalable and ensures a 100% deliverysuccess while maintaining low delivery-time and fuel consumption. The codes areavailable at ", "output": "DeepFreight: Integrating Deep Reinforcement Learning and Mixed Integer Programming for Multi-transfer Truck Freight Delivery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The successes of modern deep machine learning methods are founded on theirability to transform inputs across multiple layers to build good high-levelrepresentations. It is therefore critical to understand this process ofrepresentation learning. However, standard theoretical approaches (formallyNNGPs) involving infinite width limits eliminate representation learning. Wetherefore develop a new infinite width limit, the Bayesian representationlearning limit, that exhibits representation learning mirroring that infinite-width models, yet at the same time, retains some of the simplicity ofstandard infinite-width limits. In particular, we show that Deep Gaussianprocesses (DGPs) in the Bayesian representation learning limit have exactlymultivariate Gaussian posteriors, and the posterior covariances can be obtainedby optimizing an interpretable objective combining a log-likelihood to improveperformance with a series of KL-divergences which keep the posteriors close tothe prior. We confirm these results experimentally in wide but finite DGPs.Next, we introduce the possibility of using this limit and objective as aflexible, deep generalisation of kernel methods, that we call deep kernelmachines (DKMs). Like most naive kernel methods, DKMs scale cubically in thenumber of datapoints. We therefore use methods from the Gaussian processinducing point literature to develop a sparse DKM that scales linearly in thenumber of datapoints. Finally, we extend these approaches to NNs (which havenon-Gaussian posteriors) in the Appendices.", "output": "A theory of representation learning gives a deep generalisation of kernel methods."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Domain generalization (DG) aims to learn from multiple source domains a modelthat can generalize well on unseen target domains. Existing DG methods mainlylearn the representations with invariant marginal distribution of the inputfeatures, however, the invariance of the conditional distribution of the labelsgiven the input features is more essential for unknown domain prediction.Meanwhile, the existing of unobserved confounders which affect the inputfeatures and labels simultaneously cause spurious correlation and hinder thelearning of the invariant relationship contained in the conditionaldistribution. Interestingly, with a causal view on the data generating process,we find that the input features of one domain are valid instrumental variablesfor other domains. Inspired by this finding, we propose an instrumentalvariable-driven DG method (IV-DG) by removing the bias of the unobservedconfounders with two-stage learning. In the first stage, it learns theconditional distribution of the input features of one domain given inputfeatures of another domain. In the second stage, it estimates the relationshipby predicting labels with the learned conditional distribution. Theoreticalanalyses and simulation experiments show that it accurately captures theinvariant relationship. Extensive experiments on real-world datasetsdemonstrate that IV-DG method yields state-of-the-art results.", "output": "Instrumental Variable-Driven Domain Generalization with Unobserved Confounders."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We investigate the convergence of stochastic mirror descent (SMD) underinterpolation in relatively smooth and smooth convex optimization. Inrelatively smooth convex optimization we provide new convergence guarantees forSMD with a constant stepsize. For smooth convex optimization we propose a newadaptive stepsize scheme -- the mirror stochastic Polyak stepsize (mSPS).Notably, our convergence results in both settings do not make bounded gradientassumptions or bounded variance assumptions, and we show convergence to aneighborhood that vanishes under interpolation. Consequently, these resultscorrespond to the first convergence guarantees under interpolation for theexponentiated gradient algorithm for fixed or adaptive stepsizes. mSPSgeneralizes the recently proposed stochastic Polyak stepsize (SPS) (Loizou etal. 2021) to mirror descent and remains both practical and efficient for modernmachine learning applications while inheriting the benefits of mirror descent.We complement our results with experiments across various supervised learningtasks and different instances of SMD, demonstrating the effectiveness of mSPS.", "output": "Stochastic Mirror Descent: Convergence Analysis and Adaptive Variants via the Mirror Stochastic Polyak Stepsize."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Arrays of quantum dots (QDs) are a promising candidate system to realizescalable, coupled qubit systems and serve as a fundamental building block forquantum computers. In such semiconductor quantum systems, devices now have tensof individual electrostatic and dynamical voltages that must be carefully setto localize the system into the single-electron regime and to realize goodqubit operational performance. The mapping of requisite QD locations andcharges to gate voltages presents a challenging classical control problem. Withan increasing number of QD qubits, the relevant parameter space growssufficiently to make heuristic control unfeasible. In recent years, there hasbeen considerable effort to automate device control that combines script-basedalgorithms with machine learning (ML) techniques. In this Colloquium, acomprehensive overview of the recent progress in the automation of QD devicecontrol is presented, with a particular emphasis on silicon- and GaAs-based QDsformed in two-dimensional electron gases. Combining physics-based modeling withmodern numerical optimization and ML has proven effective in yieldingefficient, scalable control. Further integration of theoretical, computational,and experimental efforts with computer science and ML holds vast potential inadvancing semiconductor and other platforms for quantum computing.", "output": "Colloquium: Advances in automation of quantum dot devices control."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Methods for training models on graphs distributed across multiple clientshave recently grown in popularity, due to the size of these graphs as well asregulations on keeping data where it is generated. However, a single connectedgraph cannot be disjointly partitioned onto multiple clients due to thecross-client edges connecting graph nodes. Thus, distributed methods fortraining a model on a single graph incur either significant communicationoverhead between clients or a loss of available information to the training. Weintroduce the Federated Graph Convolutional Network (FedGCN) algorithm, whichuses federated learning to train GCN models for semi-supervised nodeclassification with fast convergence and little communication. Compared toprior methods that require communication among clients at each training round,FedGCN clients only communicate with the central server in one pre-trainingstep, greatly reducing communication costs and allowing the use of homomorphicencryption to further enhance privacy. We theoretically analyze the tradeoffbetween FedGCN's convergence rate and communication cost under different datadistributions. Experimental results show that our FedGCN algorithm achievesbetter model accuracy with 51.7% faster convergence on average and at least100X less communication compared to prior work.", "output": "FedGCN: Convergence and Communication Tradeoffs in Federated Training of Graph Convolutional Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large pre-trained language models based on transformer architecture havedrastically changed the natural language processing (NLP) landscape. However,deploying those models for on-device applications in constrained devices suchas smart watches is completely impractical due to their size and inferencecost. As an alternative to transformer-based architectures, recent work onefficient NLP has shown that weight-efficient models can attain competitiveperformance for simple tasks, such as slot filling and intent classification,with model sizes in the order of the megabyte. This work introduces thepNLP-Mixer architecture, an embedding-free MLP-Mixer model for on-device NLPthat achieves high weight-efficiency thanks to a novel projection layer. Weevaluate a pNLP-Mixer model of only one megabyte in size on two multi-lingualsemantic parsing datasets, MTOP and multiATIS. Our quantized model achieves99.4% and 97.8% the performance of mBERT on MTOP and multi-ATIS, while using170x fewer parameters. Our model consistently beats the state-of-the-art oftiny models (pQRNN), which is twice as large, by a margin up to 7.8% on MTOP.", "output": "pNLP-Mixer: an Efficient all-MLP Architecture for Language."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Knowledge distillation aims to enhance the performance of a lightweightstudent model by exploiting the knowledge from a pre-trained cumbersome teachermodel. However, in the traditional knowledge distillation, teacher predictionsare only used to provide the supervisory signal for the last layer of thestudent model, which may result in those shallow student layers lackingaccurate training guidance in the layer-by-layer back propagation and thushinders effective knowledge transfer. To address this issue, we proposeDeeply-Supervised Knowledge Distillation (DSKD), which fully utilizes classpredictions and feature maps of the teacher model to supervise the training ofshallow student layers. A loss-based weight allocation strategy is developed inDSKD to adaptively balance the learning process of each shallow layer, so as tofurther improve the student performance. Extensive experiments on CIFAR-100 andTinyImageNet with various teacher-student models show significantlyperformance, confirming the effectiveness of our proposed method. Code isavailable at:$href{", "output": "Knowledge Distillation with Deep Supervision."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We consider a version of actor-critic which uses proportional step-sizes andonly one critic update with a single sample from the stationary distributionper actor step. We provide an analysis of this method using the small-gaintheorem. Specifically, we prove that this method can be used to find astationary point, and that the resulting sample complexity improves the stateof the art for actor-critic methods to $O left(mu^{-2} epsilon^{-2} right)$to find an $epsilon$-approximate stationary point where $mu$ is the conditionnumber associated with the critic.", "output": "A Small Gain Analysis of Single Timescale Actor Critic."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transformer-based architectures are the model of choice for natural languageunderstanding, but they come at a significant cost, as they have quadraticcomplexity in the input length, require a lot of training data, and can bedifficult to tune. In the pursuit of lower costs, we investigate simpleMLP-based architectures. We find that existing architectures such as MLPMixer,which achieves token mixing through a static MLP applied to each featureindependently, are too detached from the inductive biases required for naturallanguage understanding. In this paper, we propose a simple variant, HyperMixer,which forms the token mixing MLP dynamically using hypernetworks. Empirically,we demonstrate that our model performs better than alternative MLP-basedmodels, and on par with Transformers. In contrast to Transformers, HyperMixerachieves these results at substantially lower costs in terms of processingtime, training data, and hyperparameter tuning.", "output": "HyperMixer: An MLP-based Low Cost Alternative to Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous exploration has many important applications. However, classicinformation gain-based or frontier-based exploration only relies on the robotcurrent state to determine the immediate exploration goal, which lacks thecapability of predicting the value of future states and thus leads toinefficient exploration decisions. This paper presents a method to learn how\"good\" states are, measured by the state value function, to provide a guidancefor robot exploration in real-world challenging environments. We formulate ourwork as an off-policy evaluation (OPE) problem for robot exploration (OPERE).It consists of offline Monte-Carlo training on real-world data and performsTemporal Difference (TD) online adaptation to optimize the trained valueestimator. We also design an intrinsic reward function based on sensorinformation coverage to enable the robot to gain more information with sparseextrinsic rewards. Results show that our method enables the robot to predictthe value of future states so as to better guide robot exploration. Theproposed algorithm achieves better prediction and exploration performancecompared with the state-of-the-arts. To the best of our knowledge, this workfor the first time demonstrates value function prediction on real-world datasetfor robot exploration in challenging subterranean and urban environments. Moredetails and demo videos can be found at ", "output": "Off-Policy Evaluation with Online Adaptation for Robot Exploration in Challenging Environments."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "State-of-the-art deep learning models have a parameter count that reachesinto the billions. Training, storing and transferring such models is energy andtime consuming, thus costly. A big part of these costs is caused by trainingthe network. Model compression lowers storage and transfer costs, and canfurther make training more efficient by decreasing the number of computationsin the forward and/or backward pass. Thus, compressing networks also attraining time while maintaining a high performance is an important researchtopic. This work is a survey on methods which reduce the number of trainedweights in deep learning models throughout the training. Most of the introducedmethods set network parameters to zero which is called pruning. The presentedpruning approaches are categorized into pruning at initialization, lotterytickets and dynamic sparse training. Moreover, we discuss methods that freezeparts of a network at its random initialization. By freezing weights, thenumber of trainable parameters is shrunken which reduces gradient computationsand the dimensionality of the model's optimization space. In this survey wefirst propose dimensionality reduced training as an underlying mathematicalmodel that covers pruning and freezing during training. Afterwards, we presentand discuss different dimensionality reduced training methods.", "output": "Dimensionality Reduced Training by Pruning and Freezing Parts of a Deep Neural Network, a Survey."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Continuous monitoring with an ever-increasing number of sensors has becomeubiquitous across many application domains. However, acquired time series aretypically high-dimensional and difficult to interpret. Expressive deep learning(DL) models have gained popularity for dimensionality reduction, but theresulting latent space often remains difficult to interpret. In this work wepropose SOM-CPC, a model that visualizes data in an organized 2D manifold,while preserving higher-dimensional information. We address a largelyunexplored and challenging set of scenarios comprising high-rate time series,and show on both synthetic and real-life data (physiological data and audiorecordings) that SOM-CPC outperforms strong baselines like DL-based featureextraction, followed by conventional dimensionality reduction techniques, andmodels that jointly optimize a DL model and a Self-Organizing Map (SOM).SOM-CPC has great potential to acquire a better understanding of latentpatterns in high-rate data streams.", "output": "SOM-CPC: Unsupervised Contrastive Learning with Self-Organizing Maps for Structured Representations of High-Rate Time Series."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Tree ensembles are powerful models that achieve excellent predictiveperformances, but can grow to unwieldy sizes. These ensembles are oftenpost-processed (pruned) to reduce memory footprint and improveinterpretability. We present ForestPrune, a novel optimization framework topost-process tree ensembles by pruning depth layers from individual trees.Since the number of nodes in a decision tree increases exponentially with treedepth, pruning deep trees drastically compactifies ensembles. We develop aspecialized optimization algorithm to efficiently obtain high-quality solutionsto problems under ForestPrune. Our algorithm typically reaches good solutionsin seconds for medium-size datasets and ensembles, with 10000s of rows and 100sof trees, resulting in significant speedups over existing approaches. Ourexperiments demonstrate that ForestPrune produces parsimonious models thatoutperform models extracted by existing post-processing algorithms.", "output": "ForestPrune: Compact Depth-Controlled Tree Ensembles."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Major Depressive Disorder (MDD) is a common worldwide mental health issuewith high associated socioeconomic costs. The prediction and automaticdetection of MDD can, therefore, make a huge impact on society. Speech, as anon-invasive, easy to collect signal, is a promising marker to aid thediagnosis and assessment of MDD. In this regard, speech samples were collectedas part of the Remote Assessment of Disease and Relapse in Major DepressiveDisorder (RADAR-MDD) research programme. RADAR-MDD was an observational cohortstudy in which speech and other digital biomarkers were collected from a cohortof individuals with a history of MDD in Spain, United Kingdom and theNetherlands. In this paper, the RADAR-MDD speech corpus was taken as anexperimental framework to test the efficacy of a Sequence-to-Sequence modelwith a local attention mechanism in a two-class depression severityclassification paradigm. Additionally, a novel training method, HARD-Training,is proposed. It is a methodology based on the selection of more ambiguoussamples for the model training, and inspired by the curriculum learningparadigm. HARD-Training was found to consistently improve - with an averageincrement of 8.6% - the performance of our classifiers for both of two speechelicitation tasks used and each collection site of the RADAR-MDD speech corpus.With this novel methodology, our Sequence-to-Sequence model was able toeffectively detect MDD severity regardless of language. Finally, recognisingthe need for greater awareness of potential algorithmic bias, we conduct anadditional analysis of our results separately for each gender.", "output": "Detecting the Severity of Major Depressive Disorder from Speech: A Novel HARD-Training Methodology."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Randomized controlled trials (RCTs) are considered the gold standard forestimating the average treatment effect (ATE) of interventions. One use of RCTsis to study the causes of global poverty -- a subject explicitly cited in the2019 Nobel Memorial Prize awarded to Duflo, Banerjee, and Kremer \"for theirexperimental approach to alleviating global poverty.\" Because the ATE is apopulation summary, anti-poverty experiments often seek to unpack the effectvariation around the ATE by conditioning (CATE) on tabular variables such asage and ethnicity that were measured during the RCT data collection. Althoughsuch variables are key to unpacking CATE, using only such variables may fail tocapture historical, geographical, or neighborhood-specific contributors toeffect variation, as tabular RCT data are often only observed near the time ofthe experiment. In global poverty research, when the location of the experimentunits is approximately known, satellite imagery can provide a window into suchfactors important for understanding heterogeneity. However, there is no methodthat specifically enables applied researchers to analyze CATE from images. Inthis paper, using a deep probabilistic modeling framework, we develop such amethod that estimates latent clusters of images by identifying images withsimilar treatment effects distributions. Our interpretable image CATE modelalso includes a sensitivity factor that quantifies the importance of imagesegments contributing to the effect cluster prediction. We compare the proposedmethods against alternatives in simulation; also, we show how the model worksin an actual RCT, estimating the effects of an anti-poverty intervention innorthern Uganda and obtaining a posterior predictive distribution over effectsfor the rest of the country where no experimental data was collected. We makeall models available in open-source software.", "output": "Image-based Treatment Effect Heterogeneity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mixed integer problems are ubiquitous in decision making, from discretedevice settings and design parameters, unit production, and on/off or yes/nodecision in switches, routing, and social networks. Despite their prevalence,classical optimization approaches for combinatorial optimization remainprohibitively slow for fast and accurate decision making in dynamic andsafety-critical environments with hard constraints. To address this gap, wepropose SiPhyR (pronounced: cipher), a physics-informed machine learningframework for end-to-end learning to optimize for combinatorial problems.SiPhyR employs a novel physics-informed rounding approach to tackle thechallenge of combinatorial optimization within a differentiable framework thathas certified satisfiability of safety-critical constraints. We demonstrate theeffectiveness of SiPhyR on an emerging paradigm for clean energy systems:dynamic reconfiguration, where the topology of the electric grid and power floware optimized so as to maintain a safe and reliable power grid in the presenceof intermittent renewable generation. Offline training of the unsupervisedframework on representative load and generation data makes dynamic decisionmaking via the online application of Grid-SiPhyR computationally feasible.", "output": "Grid-SiPhyR: An end-to-end learning to optimize framework for combinatorial problems in power systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Interest in understanding and factorizing learned embedding spaces throughconceptual explanations is steadily growing. When no human concept labels areavailable, concept discovery methods search trained embedding spaces forinterpretable concepts like object shape or color that can be used to providepost-hoc explanations for decisions. Unlike previous work, we argue thatconcept discovery should be identifiable, meaning that a number of knownconcepts can be provably recovered to guarantee reliability of theexplanations. As a starting point, we explicitly make the connection betweenconcept discovery and classical methods like Principal Component Analysis andIndependent Component Analysis by showing that they can recover independentconcepts with non-Gaussian distributions. For dependent concepts, we proposetwo novel approaches that exploit functional compositionality properties ofimage-generating processes. Our provably identifiable concept discovery methodssubstantially outperform competitors on a battery of experiments includinghundreds of trained models and dependent concepts, where they exhibit up to 29% better alignment with the ground truth. Our results provide a rigorousfoundation for reliable concept discovery without human labels.", "output": "When are Post-hoc Conceptual Explanations Identifiable?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing approaches for generating multitrack music with transformer modelshave been limited in terms of the number of instruments, the length of themusic segments and slow inference. This is partly due to the memoryrequirements of the lengthy input sequences necessitated by existingrepresentations. In this work, we propose a new multitrack music representationthat allows a diverse set of instruments while keeping a short sequence length.Our proposed Multitrack Music Transformer (MMT) achieves comparable performancewith state-of-the-art systems, landing in between two recently proposed modelsin a subjective listening test, while achieving substantial speedups and memoryreductions over both, making the method attractive for real time improvisationor near real time creative applications. Further, we propose a new measure foranalyzing musical self-attention and show that the trained model attends moreto notes that form a consonant interval with the current note and to notes thatare 4N beats away from the current step.", "output": "Multitrack Music Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Neural audio/speech coding has recently demonstrated its capability todeliver high quality at much lower bitrates than traditional methods. However,existing neural audio/speech codecs employ either acoustic features or learnedblind features with a convolutional neural network for encoding, by which thereare still temporal redundancies within encoded features. This paper introduceslatent-domain predictive coding into the VQ-VAE framework to fully remove suchredundancies and proposes the TF-Codec for low-latency neural speech coding inan end-to-end manner. Specifically, the extracted features are encodedconditioned on a prediction from past quantized latent frames so that temporalcorrelations are further removed. Moreover, we introduce a learnablecompression on the time-frequency input to adaptively adjust the attention paidto main frequencies and details at different bitrates. A differentiable vectorquantization scheme based on distance-to-soft mapping and Gumbel-Softmax isproposed to better model the latent distributions with rate constraint.Subjective results on multilingual speech datasets show that, with low latency,the proposed TF-Codec at 1 kbps achieves significantly better quality than Opusat 9 kbps, and TF-Codec at 3 kbps outperforms both EVS at 9.6 kbps and Opus at12 kbps. Numerous studies are conducted to demonstrate the effectiveness ofthese techniques.", "output": "Latent-Domain Predictive Neural Speech Coding."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep Learning (DL) methods have dramatically increased in popularity inrecent years, with significant growth in their application to supervisedlearning problems in the biomedical sciences. However, the greater prevalenceand complexity of missing data in modern biomedical datasets presentsignificant challenges for DL methods. Here, we provide a formal treatment ofmissing data in the context of deeply learned generalized linear models, asupervised DL architecture for regression and classification problems. Wepropose a new architecture, textit{dlglm}, that is one of the first to be ableto flexibly account for both ignorable and non-ignorable patterns ofmissingness in input features and response at training time. We demonstratethrough statistical simulation that our method outperforms existing approachesfor supervised learning tasks in the presence of missing not at random (MNAR)missingness. We conclude with a case study of a Bank Marketing dataset from theUCI Machine Learning Repository, in which we predict whether clients subscribedto a product based on phone survey data. Supplementary materials for thisarticle are available online.", "output": "Deeply-Learned Generalized Linear Models with Missing Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Event extraction for the clinical domain is an under-explored research area.The lack of training data along with the high volume of domain-specificterminologies with vague entity boundaries makes the task especiallychallenging. In this paper, we introduce DICE, a robust and data-efficientgenerative model for clinical event extraction. DICE frames event extraction asa conditional generation problem and introduces a contrastive learningobjective to accurately decide the boundaries of biomedical mentions. DICE alsotrains an auxiliary mention identification task jointly with event extractiontasks to better identify entity mention boundaries, and further introducesspecial markers to incorporate identified entity mentions as trigger andargument candidates for their respective tasks. To benchmark clinical eventextraction, we compose MACCROBAT-EE, the first clinical event extractiondataset with argument annotation, based on an existing clinical informationextraction dataset MACCROBAT. Our experiments demonstrate state-of-the-artperformances of DICE for clinical and news domain event extraction, especiallyunder low data settings.", "output": "DICE: Data-Efficient Clinical Event Extraction with Generative Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a deep importance sampling method that is suitable for estimatingrare event probabilities in high-dimensional problems. We approximate theoptimal importance distribution in a general importance sampling problem as thepushforward of a reference distribution under a composition of order-preservingtransformations, in which each transformation is formed by a squaredtensor-train decomposition. The squared tensor-train decomposition provides ascalable ansatz for building order-preserving high-dimensional transformationsvia density approximations. The use of composition of maps moving along asequence of bridging densities alleviates the difficulty of directlyapproximating concentrated density functions. To compute expectations overunnormalized probability distributions, we design a ratio estimator thatestimates the normalizing constant using a separate importance distribution,again constructed via a composition of transformations in tensor-train format.This offers better theoretical variance reduction compared with self-normalizedimportance sampling, and thus opens the door to efficient computation of rareevent probabilities in Bayesian inference problems. Numerical experiments onproblems constrained by differential equations show little to no increase inthe computational complexity with the event probability going to zero, andallow to compute hitherto unattainable estimates of rare event probabilitiesfor complex, high-dimensional posterior densities.", "output": "Deep importance sampling using tensor trains with application to a priori and a posteriori rare event estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent works have shown that tackling offline reinforcement learning (RL)with a conditional policy produces promising results. The Decision Transformer(DT) combines the conditional policy approach and a transformer architecture,showing competitive performance against several benchmarks. However, DT lacksstitching ability -- one of the critical abilities for offline RL to learn theoptimal policy from sub-optimal trajectories. This issue becomes particularlysignificant when the offline dataset only contains sub-optimal trajectories. Onthe other hand, the conventional RL approaches based on Dynamic Programming(such as Q-learning) do not have the same limitation; however, they suffer fromunstable learning behaviours, especially when they rely on functionapproximation in an off-policy learning setting. In this paper, we propose theQ-learning Decision Transformer (QDT) to address the shortcomings of DT byleveraging the benefits of Dynamic Programming (Q-learning). It utilises theDynamic Programming results to relabel the return-to-go in the training data tothen train the DT with the relabelled data. Our approach efficiently exploitsthe benefits of these two approaches and compensates for each other'sshortcomings to achieve better performance. We empirically show these in bothsimple toy environments and the more complex D4RL benchmark, showingcompetitive performance gains.", "output": "Q-learning Decision Transformer: Leveraging Dynamic Programming for Conditional Sequence Modelling in Offline RL."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Controlling artificial agents from visual sensory data is an arduous task.Reinforcement learning (RL) algorithms can succeed but require large amounts ofinteractions between the agent and the environment. To alleviate the issue,unsupervised RL proposes to employ self-supervised interaction and learning,for adapting faster to future tasks. Yet, as shown in the Unsupervised RLBenchmark (URLB; Laskin et al. 2021), whether current unsupervised strategiescan improve generalization capabilities is still unclear, especially in visualcontrol settings. In this work, we study the URLB and propose a new method tosolve it, using unsupervised model-based RL, for pre-training the agent, and atask-aware fine-tuning strategy combined with a new proposed hybrid planner,Dyna-MPC, to adapt the agent for downstream tasks. On URLB, our method obtains93.59% overall normalized performance, surpassing previous baselines by astaggering margin. The approach is empirically evaluated through a large-scaleempirical study, which we use to validate our design choices and analyze ourmodels. We also show robust performance on the Real-Word RL benchmark, hintingat resiliency to environment perturbations during adaptation. Project website:", "output": "Mastering the Unsupervised Reinforcement Learning Benchmark from Pixels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "One way to interpret the reasoning power of transformer-based language modelsis to describe the types of logical rules they can resolve over some inputtext. Recently, Chiang et al. (2023) showed that finite-precision transformerscan be equivalently expressed in a generalization of first-order logic.However, finite-precision transformers are a weak transformer variant because,as we show, a single head can only attend to a constant number of tokens and,in particular, cannot represent uniform attention. Since attending broadly is acore capability for transformers, we ask whether a minimally more expressivemodel that can attend universally can also be characterized in logic. To thisend, we analyze transformers whose forward pass is computed in $log n$precision on contexts of length $n$. We prove that any log-precisiontransformer can be equivalently expressed as a first-order logic sentence that,in addition to standard universal and existential quantifiers, may also containmajority-vote quantifiers. This is the tightest known upper bound and firstlogical characterization of log-precision transformers.", "output": "A Logic for Expressing Log-Precision Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Convolutional Neural Network (CNN) is one of the most important architecturesin deep learning. The fundamental building block of a CNN is a trainablefilter, represented as a discrete grid, used to perform convolution on discreteinput data. In this work, we propose a continuous version of a trainableconvolutional filter able to work also with unstructured data. This newframework allows exploring CNNs beyond discrete domains, enlarging the usage ofthis important learning technique for many more complex problems. Ourexperiments show that the continuous filter can achieve a level of accuracycomparable to the state-of-the-art discrete filter, and that it can be used incurrent deep learning architectures as a building block to solve problems withunstructured domains as well.", "output": "A Continuous Convolutional Trainable Filter for Modelling Unstructured Data."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we propose an efficient two-stage algorithm solving a jointproblem of correlation detection and partial alignment recovery between twoGaussian databases. Correlation detection is a hypothesis testing problem;under the null hypothesis, the databases are independent, and under thealternate hypothesis, they are correlated, under an unknown row permutation. Wedevelop bounds on the type-I and type-II error probabilities, and show that theanalyzed detector performs better than a recently proposed detector, at leastfor some specific parameter choices. Since the proposed detector relies on astatistic, which is a sum of dependent indicator random variables, then inorder to bound the type-I probability of error, we develop a novelgraph-theoretic technique for bounding the $k$-th order moments of suchstatistics. When the databases are accepted as correlated, the algorithm alsorecovers some partial alignment between the given databases. We also proposetwo more algorithms: (i) One more algorithm for partial alignment recovery,whose reliability and computational complexity are both higher than those ofthe first proposed algorithm. (ii) An algorithm for full alignment recovery,which has a reduced amount of calculations and a not much lower errorprobability, when compared to the optimal recovery procedure.", "output": "On Correlation Detection and Alignment Recovery of Gaussian Databases."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dynamic networks, e.g., Dynamic Convolution (DY-Conv) and the Mixture ofExperts (MoE), have been extensively explored as they can considerably improvethe model's representation power with acceptable computational cost. The commonpractice in implementing dynamic networks is to convert the given static layersinto fully dynamic ones where all parameters are dynamic (at least within asingle layer) and vary with the input. However, such a fully dynamic settingmay cause redundant parameters and high deployment costs, limiting theapplicability of dynamic networks to a broader range of tasks and models. Themain contributions of our work are challenging the basic commonsense in dynamicnetworks and proposing a partially dynamic network, namely PAD-Net, totransform the redundant dynamic parameters into static ones. Also, we furtherdesign Iterative Mode Partition to partition dynamic and static parametersefficiently. Our method is comprehensively supported by large-scale experimentswith two typical advanced dynamic architectures, i.e., DY-Conv and MoE, on bothimage classification and GLUE benchmarks. Encouragingly, we surpass the fullydynamic networks by $+0.7%$ top-1 acc with only $30%$ dynamic parameters forResNet-50 and $+1.9%$ average score in language understanding with only $50%$dynamic parameters for BERT. Code will be released at:url{", "output": "PAD-Net: An Efficient Framework for Dynamic Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, Montasser et al. [2019] showed that finite VC dimension is notsufficient for proper adversarially robust PAC learning. In light of thishardness, there is a growing effort to study what type of relaxations to theadversarially robust PAC learning setup can enable proper learnability. In thiswork, we initiate the study of proper learning under relaxations of theworst-case robust loss. We give a family of robust loss relaxations under whichVC classes are properly PAC learnable with sample complexity close to what onewould require in the standard PAC learning setup. On the other hand, we showthat for an existing and natural relaxation of the worst-case robust loss,finite VC dimension is not sufficient for proper learning. Lastly, we give newgeneralization guarantees for the adversarially robust empirical riskminimizer.", "output": "On Proper Learnability between Average- and Worst-case Robustness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The spread of misinformation is a prominent problem in today's society, andmany researchers in academia and industry are trying to combat it. Due to thevast amount of misinformation that is created every day, it is unrealistic toleave this task to human fact-checkers. Data scientists and researchers havebeen working on automated misinformation detection for years, and it is still achallenging problem today. The goal of our research is to add a new level toautomated misinformation detection; classifying segments of text withpersuasive writing techniques in order to produce interpretable reasoning forwhy an article can be marked as misinformation. To accomplish this, we presenta novel annotation scheme containing many common persuasive writing tactics,along with a dataset with human annotations accordingly. For this task, we makeuse of a RoBERTa model for text classification, due to its high performance inNLP. We develop several language model-based baselines and present the resultsof our persuasive strategy label predictions as well as the improvements theseintermediate labels make in detecting misinformation and producinginterpretable results.", "output": "Using Persuasive Writing Strategies to Explain and Detect Health Misinformation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated Learning (FL) is a decentralized learning method used to trainmachine learning algorithms. In FL, a global model iteratively collects theparameters of local models without accessing their local data. However, asignificant challenge in FL is handling the heterogeneity of local datadistribution, which often results in a drifted global model that is difficultto converge. To address this issue, current methods employ different strategiessuch as knowledge distillation, weighted model aggregation, and multi-tasklearning. These approaches are referred to as asynchronous FL, as they alignuser models either locally or post-hoc, where model drift has already occurredor has been underestimated. In this paper, we propose an active and synchronouscorrelation approach to address the challenge of user heterogeneity in FL.Specifically, our approach aims to approximate FL as standard deep learning byactively and synchronously scheduling user learning pace in each round with adynamic multi-phase curriculum. A global curriculum is formed by anauto-regressive auto-encoder that integrates all user curricula on the server.This global curriculum is then divided into multiple phases and broadcast tousers to measure and align the domain-agnostic learning pace. Empirical studiesdemonstrate that our approach outperforms existing asynchronous approaches interms of generalization performance, even in the presence of severe userheterogeneity.", "output": "FedCL: Federated Multi-Phase Curriculum Learning to Synchronously Correlate User Heterogeneity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Estimating causal effects from observational data is a central problem inmany domains. A general approach is to balance covariates with weights suchthat the distribution of the data mimics randomization. We present generalizedbalancing weights, Neural Balancing Weights (NBW), to estimate the causaleffects of an arbitrary mixture of discrete and continuous interventions. Theweights were obtained through direct estimation of the density ratio betweenthe source and balanced distributions by optimizing the variationalrepresentation of $f$-divergence. For this, we selected $alpha$-divergence asit presents efficient optimization because it has an estimator whose samplecomplexity is independent of its ground truth value and unbiased mini-batchgradients; moreover, it is advantageous for the vanishing-gradient problem. Inaddition, we provide the following two methods for estimating the balancingweights: improving the generalization performance of the balancing weights andchecking the balance of the distribution changed by the weights. Finally, wediscuss the sample size requirements for the weights as a general problem of acurse of dimensionality when balancing multidimensional data. Our studyprovides a basic approach for estimating the balancing weights ofmultidimensional data using variational $f$-divergences.", "output": "Generalized Balancing Weights via Deep Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Just like weights, bias terms are the learnable parameters of many popularmachine learning models, including neural networks. Biases are believed toeffectively increase the representational power of neural networks to solve awide range of tasks in computer vision. However, we argue that if we considerthe intrinsic distribution of images in the input space as well as some desiredproperties a model should have from the first principles, biases can becompletely ignored in addressing many image-related tasks, such as imageclassification. Our observation indicates that zero-bias neural networks couldperform comparably to neural networks with bias at least on practical imageclassification tasks. In addition, we prove that zero-bias neural networkspossess a nice property called scalar (multiplication) invariance, which allowsthe prediction of neural networks remains the same when altering the contrastof the input image. We then extend scalar invariance to more general cases thatallow us to formally verify certain convex regions of the input space. Besidesthat, we show the fairness of zero-bias neural networks in predicting the zeroimage. In contrast to the state-of-art models which lean towards certainlabels, zero-bias neural networks have a uniform belief in all labels. Based onthose merits, we believe dropping bias terms can be considered as a prior indesigning neural network architecture for some CV tasks, which shares thespirit of adapting convolutions as the transnational invariance prior.", "output": "Scalar Invariant Networks with Zero Bias."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We propose a multi-explanation graph attention network (MEGAN). Unlikeexisting graph explainability methods, our network can produce node and edgeattributional explanations along multiple channels, the number of which isindependent of task specifications. This proves crucial to improve theinterpretability of graph regression predictions, as explanations can be splitinto positive and negative evidence w.r.t to a reference value. Additionally,our attention-based network is fully differentiable and explanations canactively be trained in an explanation-supervised manner. We first validate ourmodel on a synthetic graph regression dataset with known ground-truthexplanations. Our network outperforms existing baseline explainability methodsfor the single- as well as the multi-explanation case, achieving near-perfectexplanation accuracy during explanation supervision. Finally, we demonstrateour model's capabilities on multiple real-world datasets. We find that ourmodel produces sparse high-fidelity explanations consistent with humanintuition about those tasks.", "output": "MEGAN: Multi-Explanation Graph Attention Network."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mobility systems often suffer from a high price of anarchy due to theuncontrolled behavior of selfish users. This may result in societal costs thatare significantly higher compared to what could be achieved by a centralizedsystem-optimal controller. Monetary tolling schemes can effectively align thebehavior of selfish users with the system-optimum. Yet, they inevitablydiscriminate the population in terms of income. Artificial currencies wererecently presented as an effective alternative that can achieve the sameperformance, whilst guaranteeing fairness among the population. However, thosestudies were based on behavioral models that may differ from practicalimplementations. This paper presents a data-driven approach to automaticallyadapt artificial-currency tolls within repetitive-game settings. We firstconsider a parallel-arc setting whereby users commute on a daily basis from anindividual origin to an individual destination, choosing a route in exchange ofan artificial-currency price or reward, while accounting for the impact of thechoices of the other users on travel discomfort. Second, we devise amodel-based reinforcement learning controller that autonomously learns theoptimal pricing policy by interacting with the proposed framework consideringthe closeness of the observed aggregate flows to a desired system-optimaldistribution as a reward function. Our numerical results show that the proposeddata-driven pricing scheme can effectively align the users' flows with thesystem optimum, significantly reducing the societal costs with respect to theuncontrolled flows (by about 15% and 25% depending on the scenario), andrespond to environmental changes in a robust and efficient manner.", "output": "A Data-driven Pricing Scheme for Optimal Routing through Artificial Currencies."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While modern Text-to-Speech (TTS) systems can produce natural-soundingspeech, they remain unable to reproduce the full diversity found in naturalspeech data. We consider the distribution of all possible real speech samplesthat could be generated by these speakers alongside the distribution of allsynthetic samples that could be generated for the same set of speakers, using aparticular TTS system. We set out to quantify the distance between real andsynthetic speech via a range of utterance-level statistics related toproperties of the speaker, speech prosody and acoustic environment. Differencesin the distribution of these statistics are evaluated using the Wassersteindistance. We reduce these distances by providing ground-truth values atgeneration time, and quantify the improvements to the overall distributiondistance, approximated using an automatic speech recognition system. Our bestsystem achieves a 10% reduction in distribution distance.", "output": "Evaluating and reducing the distance between synthetic and real speech distributions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, RNN-Transducers have achieved remarkable results on variousautomatic speech recognition tasks. However, lattice-free sequencediscriminative training methods, which obtain superior performance in hybridmodels, are rarely investigated in RNN-Transducers. In this work, we proposethree lattice-free training objectives, namely lattice-free maximum mutualinformation, lattice-free segment-level minimum Bayes risk, and lattice-freeminimum Bayes risk, which are used for the final posterior output of thephoneme-based neural transducer with a limited context dependency. Compared tocriteria using N-best lists, lattice-free methods eliminate the decoding stepfor hypotheses generation during training, which leads to more efficienttraining. Experimental results show that lattice-free methods gain up to 6.5%relative improvement in word error rate compared to a sequence-levelcross-entropy trained model. Compared to the N-best-list based minimum Bayesrisk objectives, lattice-free methods gain 40% - 70% relative training timespeedup with a small degradation in performance.", "output": "Lattice-Free Sequence Discriminative Training for Phoneme-Based Neural Transducers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A significant drawback of eXplainable Artificial Intelligence (XAI)approaches is the assumption of feature independence. This paper focuses onintegrating causal knowledge in XAI methods to increase trust and help usersassess explanations' quality. We propose a novel extension to a widely usedlocal and model-agnostic explainer that explicitly encodes causal relationshipsin the data generated around the input instance to explain. Extensiveexperiments show that our method achieves superior performance comparing theinitial one for both the fidelity in mimicking the black-box and the stabilityof the explanations.", "output": "CALIME: Causality-Aware Local Interpretable Model-Agnostic Explanations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Optimal transport (OT) has become exceedingly popular in machine learning,data science, and computer vision. The core assumption in the OT problem is theequal total amount of mass in source and target measures, which limits itsapplication. Optimal Partial Transport (OPT) is a recently proposed solution tothis limitation. Similar to the OT problem, the computation of OPT relies onsolving a linear programming problem (often in high dimensions), which canbecome computationally prohibitive. In this paper, we propose an efficientalgorithm for calculating the OPT problem between two non-negative measures inone dimension. Next, following the idea of sliced OT distances, we utilizeslicing to define the sliced OPT distance. Finally, we demonstrate thecomputational and accuracy benefits of the sliced OPT-based method in variousnumerical experiments. In particular, we show an application of our proposedSliced-OPT in noisy point cloud registration.", "output": "Sliced Optimal Partial Transport."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The BLOOM model is a large publicly available multilingual language model,but its pretraining was limited to 46 languages. To extend the benefits ofBLOOM to other languages without incurring prohibitively large costs, it isdesirable to adapt BLOOM to new languages not seen during pretraining. In thiswork, we apply existing language adaptation strategies to BLOOM and benchmarkits zero-shot prompting performance on eight new languages in aresource-constrained setting. We find language adaptation to be effective atimproving zero-shot performance in new languages. Surprisingly, we find thatadapter-based finetuning is more effective than continued pretraining for largemodels. In addition, we discover that prompting performance is notsignificantly affected by language specifics, such as the writing system. It isprimarily determined by the size of the language adaptation data. We also addnew languages to BLOOMZ, which is a multitask finetuned version of BLOOMcapable of following task instructions zero-shot. We find including a newlanguage in the multitask fine-tuning mixture to be the most effective methodto teach BLOOMZ a new language. We conclude that with sufficient training datalanguage adaptation can generalize well to diverse languages. Our code isavailable at ", "output": "BLOOM+1: Adding Language Support to BLOOM for Zero-Shot Prompting."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this work, we propose a novel framework for estimating the dimension ofthe data manifold using a trained diffusion model. A diffusion modelapproximates the score function i.e. the gradient of the log density of anoise-corrupted version of the target distribution for varying levels ofcorruption. We prove that, if the data concentrates around a manifold embeddedin the high-dimensional ambient space, then as the level of corruptiondecreases, the score function points towards the manifold, as this directionbecomes the direction of maximal likelihood increase. Therefore, for smalllevels of corruption, the diffusion model provides us with access to anapproximation of the normal bundle of the data manifold. This allows us toestimate the dimension of the tangent space, thus, the intrinsic dimension ofthe data manifold. To the best of our knowledge, our method is the firstestimator of the data manifold dimension based on diffusion models and itoutperforms well established statistical estimators in controlled experimentson both Euclidean and image data.", "output": "Your diffusion model secretly knows the dimension of the data manifold."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Reinforcement learning (RL) problems can be challenging without well-shapedrewards. Prior work on provably efficient RL methods generally proposes toaddress this issue with dedicated exploration strategies. However, another wayto tackle this challenge is to reformulate it as a multi-task RL problem, wherethe task space contains not only the challenging task of interest but alsoeasier tasks that implicitly function as a curriculum. Such a reformulationopens up the possibility of running existing multi-task RL methods as a moreefficient alternative to solving a single challenging task from scratch. Inthis work, we provide a theoretical framework that reformulates a single-taskRL problem as a multi-task RL problem defined by a curriculum. Under mildregularity conditions on the curriculum, we show that sequentially solving eachtask in the multi-task RL problem is more computationally efficient thansolving the original single-task problem, without any explicit explorationbonuses or other exploration strategies. We also show that our theoreticalinsights can be translated into an effective practical learning algorithm thatcan accelerate curriculum learning on simulated robotic tasks.", "output": "Understanding the Complexity Gains of Single-Task RL with a Curriculum."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deploying reliable deep learning techniques in interdisciplinary applicationsneeds learned models to output accurate and (even more importantly) explainablepredictions. Existing approaches typically explicate network outputs in apost-hoc fashion, under an implicit assumption that faithful explanations comefrom accurate predictions/classifications. We have an opposite claim thatexplanations boost (or even determine) classification. That is, end-to-endlearning of explanation factors to augment discriminative representationextraction could be a more intuitive strategy to inversely assure fine-grainedexplainability, e.g., in those neuroimaging and neuroscience studies withhigh-dimensional data containing noisy, redundant, and task-irrelevantinformation. In this paper, we propose such an explainable geometric deepnetwork dubbed as NeuroExplainer, with applications to uncover altered infantcortical development patterns associated with preterm birth. Given fundamentalcortical attributes as network input, our NeuroExplainer adopts a hierarchicalattention-decoding framework to learn fine-grained attentions and respectivediscriminative representations to accurately recognize preterm infants fromterm-born infants at term-equivalent age. NeuroExplainer learns thehierarchical attention-decoding modules under subject-level weak supervisioncoupled with targeted regularizers deduced from domain knowledge regardingbrain development. These prior-guided constraints implicitly maximizes theexplainability metrics (i.e., fidelity, sparsity, and stability) in networktraining, driving the learned network to output detailed explanations andaccurate classifications. Experimental results on the public dHCP benchmarksuggest that NeuroExplainer led to quantitatively reliable explanation resultsthat are qualitatively consistent with representative neuroimaging studies.", "output": "NeuroExplainer: Fine-Grained Attention Decoding to Uncover Cortical Development Patterns of Preterm Infants."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We analyse quantile temporal-difference learning (QTD), a distributionalreinforcement learning algorithm that has proven to be a key component inseveral successful large-scale applications of reinforcement learning. Despitethese empirical successes, a theoretical understanding of QTD has provenelusive until now. Unlike classical TD learning, which can be analysed withstandard stochastic approximation tools, QTD updates do not approximatecontraction mappings, are highly non-linear, and may have multiple fixedpoints. The core result of this paper is a proof of convergence to the fixedpoints of a related family of dynamic programming procedures with probability1, putting QTD on firm theoretical footing. The proof establishes connectionsbetween QTD and non-linear differential inclusions through stochasticapproximation theory and non-smooth analysis.", "output": "An Analysis of Quantile Temporal-Difference Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graphs have a superior ability to represent relational data, like chemicalcompounds, proteins, and social networks. Hence, graph-level learning, whichtakes a set of graphs as input, has been applied to many tasks includingcomparison, regression, classification, and more. Traditional approaches tolearning a set of graphs heavily rely on hand-crafted features, such assubstructures. But while these methods benefit from good interpretability, theyoften suffer from computational bottlenecks as they cannot skirt the graphisomorphism problem. Conversely, deep learning has helped graph-level learningadapt to the growing scale of graphs by extracting features automatically andencoding graphs into low-dimensional representations. As a result, these deepgraph learning methods have been responsible for many successes. Yet, there isno comprehensive survey that reviews graph-level learning starting withtraditional learning and moving through to the deep learning approaches. Thisarticle fills this gap and frames the representative algorithms into asystematic taxonomy covering traditional learning, graph-level deep neuralnetworks, graph-level graph neural networks, and graph pooling. To ensure athoroughly comprehensive survey, the evolutions, interactions, andcommunications between methods from four different branches of development arealso examined. This is followed by a brief review of the benchmark data sets,evaluation metrics, and common downstream applications. The survey concludeswith a broad overview of 12 current and future directions in this boomingfield.", "output": "State of the Art and Potentialities of Graph-level Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Gate-defined quantum dots (QDs) have appealing attributes as a quantumcomputing platform. However, near-term devices possess a range of possibleimperfections that need to be accounted for during the tuning and operation ofQD devices. One such problem is the capacitive cross-talk between the metallicgates that define and control QD qubits. A way to compensate for the capacitivecross-talk and enable targeted control of specific QDs independent of couplingis by the use of virtual gates. Here, we demonstrate a reliable automatedcapacitive coupling identification method that combines machine learning withtraditional fitting to take advantage of the desirable properties of each. Wealso show how the cross-capacitance measurement may be used for theidentification of spurious QDs sometimes formed during tuning experimentaldevices. Our systems can autonomously flag devices with spurious dots near theoperating regime, which is crucial information for reliable tuning to a regimesuitable for qubit operations.", "output": "Automated extraction of capacitive coupling for quantum dot systems."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While a vast collection of explainable AI (XAI) algorithms have beendeveloped in recent years, they are often criticized for significant gaps withhow humans produce and consume explanations. As a result, current XAItechniques are often found to be hard to use and lack effectiveness. In thiswork, we attempt to close these gaps by making AI explanations selective -- afundamental property of human explanations -- by selectively presenting asubset from a large set of model reasons based on what aligns with therecipient's preferences. We propose a general framework for generatingselective explanations by leveraging human input on a small sample. Thisframework opens up a rich design space that accounts for different selectivitygoals, types of input, and more. As a showcase, we use a decision-support taskto explore selective explanations based on what the decision-maker wouldconsider relevant to the decision task. We conducted two experimental studiesto examine three out of a broader possible set of paradigms based on ourproposed framework: in Study 1, we ask the participants to provide their owninput to generate selective explanations, with either open-ended orcritique-based input. In Study 2, we show participants selective explanationsbased on input from a panel of similar users (annotators). Our experimentsdemonstrate the promise of selective explanations in reducing over-reliance onAI and improving decision outcomes and subjective perceptions of the AI, butalso paint a nuanced picture that attributes some of these positive effects tothe opportunity to provide one's own input to augment AI explanations. Overall,our work proposes a novel XAI framework inspired by human communicationbehaviors and demonstrates its potentials to encourage future work to betteralign AI explanations with human production and consumption of explanations.", "output": "Selective Explanations: Leveraging Human Input to Align Explainable AI."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent ODE/SDE-based generative models, such as diffusion models, rectifiedflows, and flow matching, define a generative process as a time reversal of afixed forward process. Even though these models show impressive performance onlarge-scale datasets, numerical simulation requires multiple evaluations of aneural network, leading to a slow sampling speed. We attribute the reason tothe high curvature of the learned generative trajectories, as it is directlyrelated to the truncation error of a numerical solver. Based on therelationship between the forward process and the curvature, here we present anefficient method of training the forward process to minimize the curvature ofgenerative trajectories without any ODE/SDE simulation. Experiments show thatour method achieves a lower curvature than previous models and, therefore,decreased sampling costs while maintaining competitive performance. Code isavailable at ", "output": "Minimizing Trajectory Curvature of ODE-based Generative Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative flow networks (GFlowNets) are amortized variational inferencealgorithms that are trained to sample from unnormalized target distributionsover compositional objects. A key limitation of GFlowNets until this time hasbeen that they are restricted to discrete spaces. We present a theory forgeneralized GFlowNets, which encompasses both existing discrete GFlowNets andones with continuous or hybrid state spaces, and perform experiments with twogoals in mind. First, we illustrate critical points of the theory and theimportance of various assumptions. Second, we empirically demonstrate howobservations about discrete GFlowNets transfer to the continuous case and showstrong results compared to non-GFlowNet baselines on several previously studiedtasks. This work greatly widens the perspectives for the application ofGFlowNets in probabilistic inference and various modeling settings.", "output": "A theory of continuous generative flow networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Archetypal analysis is a matrix factorization method with convexityconstraints. Due to local minima, a good initialization is essential, butfrequently used initialization methods yield either sub-optimal starting pointsor are prone to get stuck in poor local minima. In this paper, we proposearchetypal analysis++ (AA++), a probabilistic initialization strategy forarchetypal analysis that sequentially samples points based on their influenceon the objective, similar to $k$-means++. In fact, we argue that $k$-means++already approximates the proposed initialization method. Furthermore, wesuggest to adapt an efficient Monte Carlo approximation of $k$-means++ to AA++.In an extensive empirical evaluation of 13 real-world data sets of varyingsizes and dimensionalities and considering two pre-processing strategies, weshow that AA++ nearly always outperforms all baselines, including the mostfrequently used ones.", "output": "Archetypal Analysis++: Rethinking the Initialization Strategy."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present a deep Graph Convolutional Kernel Machine (GCKM) forsemi-supervised node classification in graphs. First, we introduce anunsupervised kernel machine propagating the node features in a one-hopneighbourhood. Then, we specify a semi-supervised classification kernel machinethrough the lens of the Fenchel-Young inequality. The deep graph convolutionalkernel machine is obtained by stacking multiple shallow kernel machines. Aftershowing that unsupervised and semi-supervised layer corresponds to aneigenvalue problem and a linear system on the aggregated node features,respectively, we derive an efficient end-to-end training algorithm in the dualvariables. Numerical experiments demonstrate that our approach is competitivewith state-of-the-art graph neural networks for homophilious and heterophiliousbenchmark datasets. Notably, GCKM achieves superior performance when very fewlabels are available.", "output": "Semi-Supervised Classification with Graph Convolutional Kernel Machines."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Hyperparameter optimization is an important subfield of machine learning thatfocuses on tuning the hyperparameters of a chosen algorithm to achieve peakperformance. Recently, there has been a stream of methods that tackle the issueof hyperparameter optimization, however, most of the methods do not exploit thescaling law property of learning curves. In this work, we propose Deep PowerLaws (DPL), an ensemble of neural network models conditioned to yieldpredictions that follow a power-law scaling pattern. Our method dynamicallydecides which configurations to pause and train incrementally by making use ofgray-box evaluations. We compare our method against 7 state-of-the-artcompetitors on 3 benchmarks related to tabular, image, and NLP datasetscovering 59 diverse tasks. Our method achieves the best results across allbenchmarks by obtaining the best any-time results compared to all competitors.", "output": "Power Laws for Hyperparameter Optimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In practical compressed sensing (CS), the obtained measurements typicallynecessitate quantization to a limited number of bits prior to transmission orstorage. This nonlinear quantization process poses significant recoverychallenges, particularly with extreme coarse quantization such as 1-bit.Recently, an efficient algorithm called QCS-SGM was proposed for quantized CS(QCS) which utilizes score-based generative models (SGM) as an implicit prior.Due to the adeptness of SGM in capturing the intricate structures of naturalsignals, QCS-SGM substantially outperforms previous QCS methods. However,QCS-SGM is constrained to (approximately) row-orthogonal sensing matrices asthe computation of the likelihood score becomes intractable otherwise. Toaddress this limitation, we introduce an advanced variant of QCS-SGM, termedQCS-SGM+, capable of handling general matrices effectively. The key idea is aBayesian inference perspective on the likelihood score computation, wherein anexpectation propagation algorithm is employed for its approximate computation.We conduct extensive experiments on various settings, demonstrating thesubstantial superiority of QCS-SGM+ over QCS-SGM for general sensing matricesbeyond mere row-orthogonality.", "output": "QCM-SGM+: Improved Quantized Compressed Sensing With Score-Based Generative Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We study online learning problems in which a decision maker has to make asequence of costly decisions, with the goal of maximizing their expected rewardwhile adhering to budget and return-on-investment (ROI) constraints. Previouswork requires the decision maker to know beforehand some specific parametersrelated to the degree of strict feasibility of the offline problem. Moreover,when inputs are adversarial, it requires the existence of a strictly feasiblesolution to the offline optimization problem at each round. Both requirementsare unrealistic for practical applications such as bidding in online adauctions. We propose a best-of-both-worlds primal-dual framework whichcircumvents both assumptions by exploiting the notion of interval regret,providing guarantees under both stochastic and adversarial inputs. Our prooftechniques can be applied to both input models with minimal modifications,thereby providing a unified perspective on the two problems. Finally, we showhow to instantiate the framework to optimally bid in various mechanisms ofpractical relevance, such as first- and second-price auctions.", "output": "Online Learning under Budget and ROI Constraints and Applications to Bidding in Non-Truthful Auctions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Display Ads and the generalized assignment problem are two well-studiedonline packing problems with important applications in ad allocation and otherareas. In both problems, ad impressions arrive online and have to be allocatedimmediately to budget-constrained advertisers. Worst-case algorithms thatachieve the ideal competitive ratio are known, but might act overlyconservative given the predictable and usually tame nature of real-world input.Given this discrepancy, we develop an algorithm for both problems thatincorporate machine-learned predictions and can thus improve the performancebeyond the worst-case. Our algorithm is based on the work of Feldman et al.(2009) and similar in nature to Mahdian et al. (2007) who were the first todevelop a learning-augmented algorithm for the related, but more structured AdWords problem. We use a novel analysis to show that our algorithm is able tocapitalize on a good prediction, while being robust against poor predictions.We experimentally evaluate our algorithm on synthetic and real-world data on awide range of predictions. Our algorithm is consistently outperforming theworst-case algorithm without predictions.", "output": "Online Ad Allocation with Predictions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "While leveraging additional training data is well established to improveadversarial robustness, it incurs the unavoidable cost of data collection andthe heavy computation to train models. To mitigate the costs, we propose GuidedAdversarial Training (GAT), a novel adversarial training technique thatexploits auxiliary tasks under a limited set of training data. Our approachextends single-task models into multi-task models during the min-maxoptimization of adversarial training, and drives the loss optimization with aregularization of the gradient curvature across multiple tasks. GAT leveragestwo types of auxiliary tasks: self-supervised tasks, where the labels aregenerated automatically, and domain-knowledge tasks, where human expertsprovide additional labels. Experimentally, GAT increases the robust AUC ofCheXpert medical imaging dataset from 50% to 83% and On CIFAR-10, GAToutperforms eight state-of-the-art adversarial training and achieves 56.21%robust accuracy with Resnet-50. Overall, we demonstrate that guided multi-tasklearning is an actionable and promising avenue to push further the boundariesof model robustness.", "output": "GAT: Guided Adversarial Training with Pareto-optimal Auxiliary Tasks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Federated Learning offers a way to train deep neural networks in adistributed fashion. While this addresses limitations related to distributeddata, it incurs a communication overhead as the model parameters or gradientsneed to be exchanged regularly during training. This can be an issue with largescale distribution of learning tasks and negate the benefit of the respectiveresource distribution. In this paper, we we propose to utilise parallelAdapters for Federated Learning. Using various datasets, we show that Adapterscan be incorporated to different Federated Learning techniques. We highlightthat our approach can achieve similar inference performance compared totraining the full model while reducing the communication overhead by roughly90%. We further explore the applicability of Adapters in cross-silo andcross-device settings, as well as different non-IID data distributions.", "output": "Adaptive Parameterization of Deep Learning Models for Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Universality is a key hypothesis in mechanistic interpretability -- thatdifferent models learn similar features and circuits when trained on similartasks. In this work, we study the universality hypothesis by examining howsmall neural networks learn to implement group composition. We present a novelalgorithm by which neural networks may implement composition for any finitegroup via mathematical representation theory. We then show that networksconsistently learn this algorithm by reverse engineering model logits andweights, and confirm our understanding using ablations. By studying networks ofdiffering architectures trained on various groups, we find mixed evidence foruniversality: using our algorithm, we can completely characterize the family ofcircuits and features that networks learn on this task, but for a given networkthe precise circuits learned -- as well as the order they develop -- arearbitrary.", "output": "A Toy Model of Universality: Reverse Engineering How Networks Learn Group Operations."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Memory-based meta-learning is a technique for approximating Bayes-optimalpredictors. Under fairly general conditions, minimizing sequential predictionerror, measured by the log loss, leads to implicit meta-learning. The goal ofthis work is to investigate how far this interpretation can be realized bycurrent sequence prediction models and training regimes. The focus is onpiecewise stationary sources with unobserved switching-points, which arguablycapture an important characteristic of natural language and action-observationsequences in partially observable environments. We show that various types ofmemory-based neural models, including Transformers, LSTMs, and RNNs can learnto accurately approximate known Bayes-optimal algorithms and behave as ifperforming Bayesian inference over the latent switching-points and the latentparameters governing the data distribution within each segment.", "output": "Memory-Based Meta-Learning on Non-Stationary Distributions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we study risk-sensitive Reinforcement Learning (RL), focusingon the objective of Conditional Value at Risk (CVaR) with risk tolerance$tau$. Starting with multi-arm bandits (MABs), we show the minimax CVaR regretrate is $Omega(sqrt{tau^{-1}AK})$, where $A$ is the number of actions and$K$ is the number of episodes, and that it is achieved by an Upper ConfidenceBound algorithm with a novel Bernstein bonus. For online RL in tabular MarkovDecision Processes (MDPs), we show a minimax regret lower bound of$Omega(sqrt{tau^{-1}SAK})$ (with normalized cumulative rewards), where $S$is the number of states, and we propose a novel bonus-driven Value Iterationprocedure. We show that our algorithm achieves the optimal regret of$widetilde O(sqrt{tau^{-1}SAK})$ under a continuity assumption and ingeneral attains a near-optimal regret of $widetilde O(tau^{-1}sqrt{SAK})$,which is minimax-optimal for constant $tau$. This improves on the bestavailable bounds. By discretizing rewards appropriately, our algorithms arecomputationally efficient.", "output": "Near-Minimax-Optimal Risk-Sensitive Reinforcement Learning with CVaR."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The primary goal in recommendation is to suggest relevant content to users,but optimizing for accuracy often results in recommendations that lackdiversity. To remedy this, conventional approaches such as re-ranking improvediversity by presenting more diverse items. Here we argue that to promoteinherent and prolonged diversity, the system must encourage its creation.Towards this, we harness the performative nature of recommendation, and showhow learning can incentivize strategic content creators to create diversecontent. Our approach relies on a novel form of regularization that anticipatesstrategic changes to content, and penalizes for content homogeneity. We provideanalytic and empirical results that demonstrate when and how diversity can beincentivized, and experimentally demonstrate the utility of our approach onsynthetic and semi-synthetic data.", "output": "Performative Recommendation: Diversifying Content via Strategic Incentives."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Transfer learning with a small amount of target data is an effective andcommon approach to adapting a pre-trained model to distribution shifts. In somesituations, target data labels may be expensive to obtain, so we may only haveaccess to a limited number of target data points. To make the most of a verysmall target dataset, we propose a lightweight, sample-efficient approach thatlearns a diverse set of features and adapts to a target distribution byinterpolating these features. Our approach, Project and Probe (Pro$^2$), firstlearns a linear projection that maps a pre-trained embedding onto orthogonaldirections while being predictive of labels in the source dataset. The goal ofthis step is to learn a variety of predictive features, so that at least someof them remain useful after distribution shift. Pro$^2$ then learns a linearclassifier on top of these projected features using a small target dataset.Theoretically, we find that Pro$^2$ results in more sample-efficientgeneralization by inducing a favorable bias-variance tradeoff. Our experimentson four datasets, with multiple distribution shift settings for each, show thatPro$^2$ improves performance by 5-15% when given limited target data comparedto prior methods such as standard linear probing.", "output": "Project and Probe: Sample-Efficient Domain Adaptation by Interpolating Orthogonal Features."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Instrumental variable (IV) methods are used to estimate causal effects insettings with unobserved confounding, where we cannot directly experiment onthe treatment variable. Instruments are variables which only affect the outcomeindirectly via the treatment variable(s). Most IV applications focus onlow-dimensional treatments and crucially require at least as many instrumentsas treatments. This assumption is restrictive: in the natural sciences we oftenseek to infer causal effects of high-dimensional treatments (e.g., the effectof gene expressions or microbiota on health and disease), but can only run fewexperiments with a limited number of instruments (e.g., drugs or antibiotics).In such underspecified problems, the full treatment effect is not identifiablein a single experiment even in the linear case. We show that one can stillreliably recover the projection of the treatment effect onto the instrumentedsubspace and develop techniques to consistently combine such partial estimatesfrom different sets of instruments. We then leverage our combined estimators inan algorithm that iteratively proposes the most informative instruments at eachround of experimentation to maximize the overall information about the fullcausal effect.", "output": "Sequential Underspecified Instrument Selection for Cause-Effect Estimation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Nonstationary phenomena, such as satiation effects in recommendations, havemostly been modeled using bandits with finitely many arms. However, the richeraction space provided by linear bandits is often preferred in practice. In thiswork, we introduce a novel nonstationary linear bandit model, where currentrewards are influenced by the learner's past actions in a fixed-size window.Our model, which recovers stationary linear bandits as a special case,leverages two parameters: the window size $m ge 0$, and an exponent $gamma$that captures the rotting ($gamma &lt; 0)$ or rising ($gamma &gt; 0$) nature of thephenomenon. When both $m$ and $gamma$ are known, we propose and analyze avariant of OFUL which minimizes regret against cycling policies. By choosingthe cycle length so as to trade-off approximation and estimation errors, wethen prove a bound of order$sqrt{d},(m+1)^{frac{1}{2}+max{gamma,0}},T^{3/4}$ (ignoring logfactors) on the regret against the optimal sequence of actions, where $T$ isthe horizon and $d$ is the dimension of the linear action space. Through abandit model selection approach, our results are extended to the case where $m$and $gamma$ are unknown. Finally, we complement our theoretical results withexperiments against natural baselines.", "output": "Linear Bandits with Memory: from Rotting to Rising."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we present InstructABSA, Aspect Based Sentiment Analysis(ABSA) using the instruction learning paradigm for the ABSA subtasks: AspectTerm Extraction (ATE), Aspect Term Sentiment Classification (ATSC), and JointTask modeling. Our method introduces positive, negative, and neutral examplesto each training sample, and instruction tunes the model (Tk-Instruct) the ABSAsubtasks, yielding significant performance improvements. Experimental resultson the Sem Eval 2014, 15, and 16 datasets demonstrate that InstructABSAoutperforms the previous state-of-the-art (SOTA) approaches on the three ABSAsubtasks (ATE, ATSC, and Joint Task) by a significant margin, outperforming 7xlarger models. In particular, InstructABSA surpasses the SOTA on the Rest14 ATEsubtask by 5.69% points, Rest15 ATSC subtask by 9.59% points, and on the Lapt14Joint Task by 3.37% points. Our results also suggest a strong generalizationability to new domains across all three subtasks", "output": "InstructABSA: Instruction Learning for Aspect Based Sentiment Analysis."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In federated learning, a large number of users collaborate to learn a globalmodel. They alternate local computations and communication with a distantserver. Communication, which can be slow and costly, is the main bottleneck inthis setting. In addition to communication-efficiency, a robust algorithmshould allow for partial participation, the desirable feature that not allclients need to participate to every round of the training process. To reducethe communication load and therefore accelerate distributed gradient descent,two strategies are popular: 1) communicate less frequently; that is, performseveral iterations of local computations between the communication rounds; and2) communicate compressed information instead of full-dimensional vectors. Wepropose TAMUNA, the first algorithm for distributed optimization and federatedlearning, which harnesses these two strategies jointly and allows for partialparticipation. TAMUNA converges linearly to an exact solution in the stronglyconvex setting, with a doubly accelerated rate: it provably benefits from thetwo acceleration mechanisms provided by local training and compression, namelya better dependency on the condition number of the functions and on the modeldimension, respectively.", "output": "TAMUNA: Doubly Accelerated Federated Learning with Local Training, Compression, and Partial Participation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper, we propose an efficient secure aggregation scheme forfederated learning that is protected against Byzantine attacks and privacyleakages. Processing individual updates to manage adversarial behavior, whilepreserving privacy of data against colluding nodes, requires some sort ofsecure secret sharing. However, communication load for secret sharing of longvectors of updates can be very high. To resolve this issue, in the proposedscheme, local updates are partitioned into smaller sub-vectors and shared usingramp secret sharing. However, this sharing method does not admit bi-linearcomputations, such as pairwise distance calculations, needed byoutlier-detection algorithms. To overcome this issue, each user runs anotherround of ramp sharing, with different embedding of data in the sharingpolynomial. This technique, motivated by ideas from coded computing, enablessecure computation of pairwise distance. In addition, to maintain the integrityand privacy of the local update, the proposed scheme also uses a vectorcommitment method, in which the commitment size remains constant (i.e. does notincrease with the length of the local update), while simultaneously allowingverification of the secret sharing process.", "output": "ByzSecAgg: A Byzantine-Resistant Secure Aggregation Scheme for Federated Learning Based on Coded Computing and Vector Commitment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Diffusion models have enabled high-quality, conditional image editingcapabilities. We propose to expand their arsenal, and demonstrate thatoff-the-shelf diffusion models can be used for a wide range of cross-domaincompositing tasks. Among numerous others, these include image blending, objectimmersion, texture-replacement and even CG2Real translation or stylization. Weemploy a localized, iterative refinement scheme which infuses the injectedobjects with contextual information derived from the background scene, andenables control over the degree and types of changes the object may undergo. Weconduct a range of qualitative and quantitative comparisons to prior work, andexhibit that our method produces higher quality and realistic results withoutrequiring any annotations or training. Finally, we demonstrate how our methodmay be used for data augmentation of downstream tasks.", "output": "Cross-domain Compositing with Pretrained Diffusion Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We design replicable algorithms in the context of statistical clusteringunder the recently introduced notion of replicability from Impagliazzo et al.[2022]. According to this definition, a clustering algorithm is replicable if,with high probability, its output induces the exact same partition of thesample space after two executions on different inputs drawn from the samedistribution, when its internal randomness is shared across the executions. Wepropose such algorithms for the statistical $k$-medians, statistical $k$-means,and statistical $k$-centers problems by utilizing approximation routines fortheir combinatorial counterparts in a black-box manner. In particular, wedemonstrate a replicable $O(1)$-approximation algorithm for statisticalEuclidean $k$-medians ($k$-means) with $operatorname{poly}(d)$ samplecomplexity. We also describe an $O(1)$-approximation algorithm with anadditional $O(1)$-additive error for statistical Euclidean $k$-centers, albeitwith $exp(d)$ sample complexity. In addition, we provide experiments onsynthetic distributions in 2D using the $k$-means++ implementation from sklearnas a black-box that validate our theoretical results.", "output": "Replicable Clustering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Although we have witnessed great success of pre-trained models in naturallanguage processing (NLP) and computer vision (CV), limited progress has beenmade for general time series analysis. Unlike NLP and CV where a unified modelcan be used to perform different tasks, specially designed approach stilldominates in each time series analysis task such as classification, anomalydetection, forecasting, and few-shot learning. The main challenge that blocksthe development of pre-trained model for time series analysis is the lack of alarge amount of data for training. In this work, we address this challenge byleveraging language or CV models, pre-trained from billions of tokens, for timeseries analysis. Specifically, we refrain from altering the self-attention andfeedforward layers of the residual blocks in the pre-trained language or imagemodel. This model, known as the Frozen Pretrained Transformer (FPT), isevaluated through fine-tuning on all major types of tasks involving timeseries. Our results demonstrate that pre-trained models on natural language orimages can lead to a comparable or state-of-the-art performance in all maintime series analysis tasks, as illustrated in Figure 1. We also found boththeoretically and empirically that the self-attention module behaviorssimilarly to principle component analysis (PCA), an observation that helpsexplains how transformer bridges the domain gap and a crucial step towardsunderstanding the universality of a pre-trained transformer.", "output": "One Fits All:Power General Time Series Analysis by Pretrained LM."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Counterfactual Risk Minimization (CRM) is a framework for dealing with thelogged bandit feedback problem, where the goal is to improve a logging policyusing offline data. In this paper, we explore the case where it is possible todeploy learned policies multiple times and acquire new data. We extend the CRMprinciple and its theory to this scenario, which we call \"SequentialCounterfactual Risk Minimization (SCRM).\" We introduce a novel counterfactualestimator and identify conditions that can improve the performance of CRM interms of excess risk and regret rates, by using an analysis similar to restartstrategies in accelerated optimization methods. We also provide an empiricalevaluation of our method in both discrete and continuous action settings, anddemonstrate the benefits of multiple deployments of CRM.", "output": "Sequential Counterfactual Risk Minimization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Energy markets can provide incentives for undesired behavior of marketparticipants. Multi-agent Reinforcement learning (MARL) is a promising newapproach to predicting the expected behavior of energy market participants.However, reinforcement learning requires many interactions with the system toconverge, and the power system environment often consists of extensivecomputations, e.g., optimal power flow (OPF) calculation for market clearing.To tackle this complexity, we provide a model of the energy market to a basicMARL algorithm in the form of a learned OPF approximation and explicit marketrules. The learned OPF surrogate model makes an explicit solving of the OPFcompletely unnecessary. Our experiments demonstrate that the model additionallyreduces training time by about one order of magnitude but at the cost of aslightly worse approximation of the Nash equilibrium. Potential applications ofour method are market design, more realistic modeling of market participants,and analysis of manipulative behavior.", "output": "Approximating Energy Market Clearing and Bidding With Model-Based Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deep Learning (DL) systems have proliferated in many applications, requiringspecialized hardware accelerators and chips. In the nano-era, devices havebecome increasingly more susceptible to permanent and transient faults.Therefore, we need an efficient methodology for analyzing the resilience ofadvanced DL systems against such faults, and understand how the faults inneural accelerator chips manifest as errors at the DL application level, wherefaults can lead to undetectable and unrecoverable errors. Using faultinjection, we can perform resilience investigations of the DL system bymodifying neuron weights and outputs at the software-level, as if the hardwarehad been affected by a transient fault. Existing fault models reduce the searchspace, allowing faster analysis, but requiring a-priori knowledge on the model,and not allowing further analysis of the filtered-out search space. Therefore,we propose ISimDL, a novel methodology that employs neuron sensitivity togenerate importance sampling-based fault-scenarios. Without any a-prioriknowledge of the model-under-test, ISimDL provides an equivalent reduction ofthe search space as existing works, while allowing long simulations to coverall the possible faults, improving on existing model requirements. Ourexperiments show that the importance sampling provides up to 15x higherprecision in selecting critical faults than the random uniform sampling,reaching such precision in less than 100 faults. Additionally, we showcaseanother practical use-case for importance sampling for reliable DNN design,namely Fault Aware Training (FAT). By using ISimDL to select the faults leadingto errors, we can insert the faults during the DNN training process to hardenthe DNN against such faults. Using importance sampling in FAT reduces theoverhead required for finding faults that lead to a predetermined drop inaccuracy by more than 12x.", "output": "ISimDL: Importance Sampling-Driven Acceleration of Fault Injection Simulations for Evaluating the Robustness of Deep Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Graph neural networks (GNNs) for temporal graphs have recently attractedincreasing attentions, where a common assumption is that the class set fornodes is closed. However, in real-world scenarios, it often faces the open setproblem with the dynamically increased class set as the time passes by. Thiswill bring two big challenges to the existing dynamic GNN methods: (i) How todynamically propagate appropriate information in an open temporal graph, wherenew class nodes are often linked to old class nodes. This case will lead to asharp contradiction. This is because typical GNNs are prone to make theembeddings of connected nodes become similar, while we expect the embeddings ofthese two interactive nodes to be distinguishable since they belong todifferent classes. (ii) How to avoid catastrophic knowledge forgetting over oldclasses when learning new classes occurred in temporal graphs. In this paper,we propose a general and principled learning approach for open temporal graphs,called OTGNet, with the goal of addressing the above two challenges. We assumethe knowledge of a node can be disentangled into class-relevant andclass-agnostic one, and thus explore a new message passing mechanism byextending the information bottleneck principle to only propagate class-agnosticknowledge between nodes of different classes, avoiding aggregating conflictiveinformation. Moreover, we devise a strategy to select both important anddiverse triad sub-graph structures for effective class-incremental learning.Extensive experiments on three real-world datasets of different domainsdemonstrate the superiority of our method, compared to the baselines.", "output": "Towards Open Temporal Graph Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "PCA-Net is a recently proposed neural operator architecture which combinesprincipal component analysis (PCA) with neural networks to approximateoperators between infinite-dimensional function spaces. The present workdevelops approximation theory for this approach, improving and significantlyextending previous work in this direction: First, a novel universalapproximation result is derived, under minimal assumptions on the underlyingoperator and the data-generating distribution. Then, two potential obstacles toefficient operator learning with PCA-Net are identified, and made precisethrough lower complexity bounds; the first relates to the complexity of theoutput distribution, measured by a slow decay of the PCA eigenvalues. The otherobstacle relates to the inherent complexity of the space of operators betweeninfinite-dimensional input and output spaces, resulting in a rigorous andquantifiable statement of the curse of dimensionality. In addition to theselower bounds, upper complexity bounds are derived. A suitable smoothnesscriterion is shown to ensure an algebraic decay of the PCA eigenvalues.Furthermore, it is shown that PCA-Net can overcome the general curse ofdimensionality for specific operators of interest, arising from the Darcy flowand the Navier-Stokes equations.", "output": "Operator learning with PCA-Net: upper and lower complexity bounds."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Mathematical notions of privacy, such as differential privacy, are oftenstated as probabilistic guarantees that are difficult to interpret. It isimperative, however, that the implications of data sharing be effectivelycommunicated to the data principal to ensure informed decision-making and offerfull transparency with regards to the associated privacy risks. To this end,our work presents a rigorous quantitative evaluation of the protectionconferred by private learners by investigating their resilience to trainingdata reconstruction attacks. We accomplish this by deriving non-asymptoticlower bounds on the reconstruction error incurred by any adversary against$(epsilon, delta)$ differentially private learners for target samples thatbelong to any compact metric space. Working with a generalization ofdifferential privacy, termed metric privacy, we remove boundedness assumptionson the input space prevalent in prior work, and prove that our results hold forgeneral locally compact metric spaces. We extend the analysis to cover the highdimensional regime, wherein, the input data dimensionality may be larger thanthe adversary's query budget, and demonstrate that our bounds are minimaxoptimal under certain regimes.", "output": "Non-Asymptotic Lower Bounds For Training Data Reconstruction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Pillars are important structural units used to ensure mining safety inunderground hard rock mines. Therefore, precise predictions regarding thestability of underground pillars are required. One common index that is oftenused to assess pillar stability is the Safety Factor (SF). Unfortunately, suchcrisp boundaries in pillar stability assessment using SF are unreliable. Thispaper presents a novel application of Artificial Neural Network-Backpropagation(ANN-BP) and Deep Ensemble Learning for pillar stability classification. Thereare three types of ANN-BP used for the classification of pillar stabilitydistinguished by their activation functions: ANN-BP ReLU, ANN-BP ELU, andANN-BP GELU. This research also presents a new labeling alternative for pillarstability by considering its suitability with the SF. Thus, pillar stability isexpanded into four categories: failed with a suitable safety factor, intactwith a suitable safety factor, failed without a suitable safety factor, andintact without a suitable safety factor. There are five inputs used for eachmodel: pillar width, mining height, bord width, depth to floor, and ratio. Theresults showed that the ANN-BP model with Ensemble Learning could improveANN-BP performance with an average accuracy of 86.48% and an F_2-score of96.35% for the category of failed with a suitable safety factor.", "output": "Ensemble Learning Model on Artificial Neural Network-Backpropagation (ANN-BP) Architecture for Coal Pillar Stability Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Solving complicated AI tasks with different domains and modalities is a keystep toward artificial general intelligence. While there are abundant AI modelsavailable for different domains and modalities, they cannot handle complicatedAI tasks. Considering large language models (LLMs) have exhibited exceptionalability in language understanding, generation, interaction, and reasoning, weadvocate that LLMs could act as a controller to manage existing AI models tosolve complicated AI tasks and language could be a generic interface to empowerthis. Based on this philosophy, we present HuggingGPT, a framework thatleverages LLMs (e.g., ChatGPT) to connect various AI models in machine learningcommunities (e.g., Hugging Face) to solve AI tasks. Specifically, we useChatGPT to conduct task planning when receiving a user request, select modelsaccording to their function descriptions available in Hugging Face, executeeach subtask with the selected AI model, and summarize the response accordingto the execution results. By leveraging the strong language capability ofChatGPT and abundant AI models in Hugging Face, HuggingGPT is able to covernumerous sophisticated AI tasks in different modalities and domains and achieveimpressive results in language, vision, speech, and other challenging tasks,which paves a new way towards artificial general intelligence.", "output": "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Large Language Models (LLMs) have revolutionized natural language processingand demonstrated impressive capabilities in various tasks. Unfortunately, theyare prone to hallucinations, where the model exposes incorrect or falseinformation in its responses, which renders diligent evaluation approachesmandatory. While LLM performance in specific knowledge fields is oftenevaluated based on question and answer (Q&amp;A) datasets, such evaluations usuallyreport only a single accuracy number for the entire field, a procedure which isproblematic with respect to transparency and model improvement. A stratifiedevaluation could instead reveal subfields, where hallucinations are more likelyto occur and thus help to better assess LLMs' risks and guide their furtherdevelopment. To support such stratified evaluations, we propose LLMMaps as anovel visualization technique that enables users to evaluate LLMs' performancewith respect to Q&amp;A datasets. LLMMaps provide detailed insights into LLMs'knowledge capabilities in different subfields, by transforming Q&amp;A datasets aswell as LLM responses into our internal knowledge structure. An extension forcomparative visualization furthermore, allows for the detailed comparison ofmultiple LLMs. To assess LLMMaps we use them to conduct a comparative analysisof several state-of-the-art LLMs, such as BLOOM, GPT-2, GPT-3, ChatGPT andLLaMa-13B, as well as two qualitative user evaluations. All necessary sourcecode and data for generating LLMMaps to be used in scientific publications andelsewhere will be available on GitHub.", "output": "LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Quantum many-body physics simulation has important impacts on understandingfundamental science and has applications to quantum materials design andquantum technology. However, due to the exponentially growing size of theHilbert space with respect to the particle number, a direct simulation isintractable. While representing quantum states with tensor networks and neuralnetworks are the two state-of-the-art methods for approximate simulations, eachhas its own limitations in terms of expressivity and inductive bias. To addressthese challenges, we develop a novel architecture, Autoregressive NeuralTensorNet (ANTN), which bridges tensor networks and autoregressive neuralnetworks. We show that Autoregressive Neural TensorNet parameterizes normalizedwavefunctions, allows for exact sampling, generalizes the expressivity oftensor networks and autoregressive neural networks, and inherits a variety ofsymmetries from autoregressive neural networks. We demonstrate our approach onquantum state learning as well as finding the ground state of the challenging2D $J_1$-$J_2$ Heisenberg model with different systems sizes and couplingparameters, outperforming both tensor networks and autoregressive neuralnetworks. Our work opens up new opportunities for scientific simulations ofquantum many-body physics and quantum technology.", "output": "ANTN: Bridging Autoregressive Neural Networks and Tensor Networks for Quantum Many-Body Simulation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multilabel ranking is a central task in machine learning. However, the mostfundamental question of learnability in a multilabel ranking setting withrelevance-score feedback remains unanswered. In this work, we characterize thelearnability of multilabel ranking problems in both batch and online settingsfor a large family of ranking losses. Along the way, we give two equivalenceclasses of ranking losses based on learnability that capture most, if not all,losses used in practice.", "output": "On the Learnability of Multilabel Ranking."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We provide a unified framework for characterizing pure and approximatedifferentially private (DP) learnabiliity. The framework uses the language ofgraph theory: for a concept class $mathcal{H}$, we define the contradictiongraph $G$ of $mathcal{H}$. It vertices are realizable datasets, and twodatasets $S,S'$ are connected by an edge if they contradict each other (i.e.,there is a point $x$ that is labeled differently in $S$ and $S'$). Our mainfinding is that the combinatorial structure of $G$ is deeply related tolearning $mathcal{H}$ under DP. Learning $mathcal{H}$ under pure DP iscaptured by the fractional clique number of $G$. Learning $mathcal{H}$ underapproximate DP is captured by the clique number of $G$. Consequently, weidentify graph-theoretic dimensions that characterize DP learnability: theclique dimension and fractional clique dimension. Along the way, we revealproperties of the contradiction graph which may be of independent interest. Wealso suggest several open questions and directions for future research.", "output": "A Unified Characterization of Private Learnability via Graph Theory."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automated computational analysis of the vast chemical space is critical fornumerous fields of research such as drug discovery and material science.Representation learning techniques have recently been employed with the primaryobjective of generating compact and informative numerical expressions ofcomplex data. One approach to efficiently learn molecular representations isprocessing string-based notations of chemicals via natural language processing(NLP) algorithms. Majority of the methods proposed so far utilize SMILESnotations for this purpose; however, SMILES is associated with numerousproblems related to validity and robustness, which may prevent the model fromeffectively uncovering the knowledge hidden in the data. In this study, wepropose SELFormer, a transformer architecture-based chemical language modelthat utilizes a 100% valid, compact and expressive notation, SELFIES, as input,in order to learn flexible and high-quality molecular representations.SELFormer is pre-trained on two million drug-like compounds and fine-tuned fordiverse molecular property prediction tasks. Our performance evaluation hasrevealed that, SELFormer outperforms all competing methods, including graphlearning-based approaches and SMILES-based chemical language models, onpredicting aqueous solubility of molecules and adverse drug reactions. We alsovisualized molecular representations learned by SELFormer via dimensionalityreduction, which indicated that even the pre-trained model can discriminatemolecules with differing structural properties. We shared SELFormer as aprogrammatic tool, together with its datasets and pre-trained models. Overall,our research demonstrates the benefit of using the SELFIES notations in thecontext of chemical language modeling and opens up new possibilities for thedesign and discovery of novel drug candidates with desired features.", "output": "SELFormer: Molecular Representation Learning via SELFIES Language Models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Generative foundation models are susceptible to implicit biases that canarise from extensive unsupervised training data. Such biases can producesuboptimal samples, skewed outcomes, and unfairness, with potentiallysignificant repercussions. Consequently, aligning these models with humanethics and preferences is an essential step toward ensuring their responsibleand effective deployment in real-world applications. Prior research hasprimarily employed Reinforcement Learning from Human Feedback (RLHF) as a meansof addressing this problem, wherein generative models are fine-tuned using RLalgorithms guided by a human-feedback-informed reward model. However, theinefficiencies and instabilities associated with RL algorithms frequentlypresent substantial obstacles to the successful alignment of generative models,necessitating the development of a more robust and streamlined approach. Tothis end, we introduce a new framework, Reward rAnked FineTuning (RAFT),designed to align generative models more effectively. Utilizing a reward modeland a sufficient number of samples, our approach selects the high-qualitysamples, discarding those that exhibit undesired behavior, and subsequentlyassembles a streaming dataset. This dataset serves as the basis for aligningthe generative model and can be employed under both offline and onlinesettings. Notably, the sample generation process within RAFT is gradient-free,rendering it compatible with black-box generators. Through extensiveexperiments, we demonstrate that our proposed algorithm exhibits strongperformance in the context of both large language models and diffusion models.", "output": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "A key challenge for a reinforcement learning (RL) agent is to incorporateexternal/expert1 advice in its learning. The desired goals of an algorithm thatcan shape the learning of an RL agent with external advice include (a)maintaining policy invariance; (b) accelerating the learning of the agent; and(c) learning from arbitrary advice [3]. To address this challenge this paperformulates the problem of incorporating external advice in RL as a multi-armedbandit called shaping-bandits. The reward of each arm of shaping banditscorresponds to the return obtained by following the expert or by following adefault RL algorithm learning on the true environment reward.We show thatdirectly applying existing bandit and shaping algorithms that do not reasonabout the non-stationary nature of the underlying returns can lead to poorresults. Thus we propose UCB-PIES (UPIES), Racing-PIES (RPIES), and Lazy PIES(LPIES) three different shaping algorithms built on different assumptions thatreason about the long-term consequences of following the expert policy or thedefault RL algorithm. Our experiments in four different settings show thatthese proposed algorithms achieve the above-mentioned goals whereas the otheralgorithms fail to do so.", "output": "Bandit-Based Policy Invariant Explicit Shaping for Incorporating External Advice in Reinforcement Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Dimensionality reduction (DR) algorithms compress high-dimensional data intoa lower dimensional representation while preserving important features of thedata. DR is a critical step in many analysis pipelines as it enablesvisualisation, noise reduction and efficient downstream processing of the data.In this work, we introduce the ProbDR variational framework, which interprets awide range of classical DR algorithms as probabilistic inference algorithms inthis framework. ProbDR encompasses PCA, CMDS, LLE, LE, MVU, diffusion maps,kPCA, Isomap, (t-)SNE, and UMAP. In our framework, a low-dimensional latentvariable is used to construct a covariance, precision, or a graph Laplacianmatrix, which can be used as part of a generative model for the data. Inferenceis done by optimizing an evidence lower bound. We demonstrate the internalconsistency of our framework and show that it enables the use of probabilisticprogramming languages (PPLs) for DR. Additionally, we illustrate that theframework facilitates reasoning about unseen data and argue that our generativemodels approximate Gaussian processes (GPs) on manifolds. By providing aunified view of DR, our framework facilitates communication, reasoning aboutuncertainties, model composition, and extensions, particularly when domainknowledge is present.", "output": "Dimensionality Reduction as Probabilistic Inference."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In a federated learning (FL) system, distributed clients upload their localmodels to a central server to aggregate into a global model. Malicious clientsmay plant backdoors into the global model through uploading poisoned localmodels, causing images with specific patterns to be misclassified into sometarget labels. Backdoors planted by current attacks are not durable, and vanishquickly once the attackers stop model poisoning. In this paper, we investigatethe connection between the durability of FL backdoors and the relationshipsbetween benign images and poisoned images (i.e., the images whose labels areflipped to the target label during local training). Specifically, benign imageswith the original and the target labels of the poisoned images are found tohave key effects on backdoor durability. Consequently, we propose a novelattack, Chameleon, which utilizes contrastive learning to further amplify sucheffects towards a more durable backdoor. Extensive experiments demonstrate thatChameleon significantly extends the backdoor lifespan over baselines by$1.2times sim 4times$, for a wide range of image datasets, backdoor types,and model architectures.", "output": "Chameleon: Adapting to Peer Images for Planting Durable Backdoors in Federated Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Simulation-free methods for training continuous-time generative modelsconstruct probability paths that go between noise distributions and individualdata samples. Recent works, such as Flow Matching, derived paths that areoptimal for each data sample. However, these algorithms rely on independentdata and noise samples, and do not exploit underlying structure in the datadistribution for constructing probability paths. We propose Multisample FlowMatching, a more general framework that uses non-trivial couplings between dataand noise samples while satisfying the correct marginal constraints. At verysmall overhead costs, this generalization allows us to (i) reduce gradientvariance during training, (ii) obtain straighter flows for the learned vectorfield, which allows us to generate high-quality samples using fewer functionevaluations, and (iii) obtain transport maps with lower cost in highdimensions, which has applications beyond generative modeling. Importantly, wedo so in a completely simulation-free manner with a simple minimizationobjective. We show that our proposed methods improve sample consistency ondownsampled ImageNet data sets, and lead to better low-cost sample generation.", "output": "Multisample Flow Matching: Straightening Flows with Minibatch Couplings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "As the field of automated machine learning (AutoML) advances, it becomesincreasingly important to incorporate domain knowledge into these systems. Wepresent an approach for doing so by harnessing the power of large languagemodels (LLMs). Specifically, we introduce Context-Aware Automated FeatureEngineering (CAAFE), a feature engineering method for tabular datasets thatutilizes an LLM to iteratively generate additional semantically meaningfulfeatures for tabular datasets based on the description of the dataset. Themethod produces both Python code for creating new features and explanations forthe utility of the generated features.Despite being methodologically simple, CAAFE improves performance on 11 outof 14 datasets - boosting mean ROC AUC performance from 0.798 to 0.822 acrossall dataset - similar to the improvement achieved by using a random forestinstead of logistic regression on our datasets.Furthermore, CAAFE is interpretable by providing a textual explanation foreach generated feature. CAAFE paves the way for more extensive semi-automationin data science tasks and emphasizes the significance of context-awaresolutions that can extend the scope of AutoML systems to semantic AutoML. Werelease our $href{ a simple$href{ a $href{ package}$.", "output": "LLMs for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Code based Language Models (LMs) have shown very promising results in thefield of software engineering with applications such as code refinement, codecompletion and generation. However, the task of time and space complexityclassification from code has not been extensively explored due to a lack ofdatasets, with prior endeavors being limited to Java. In this project, we aimto address these gaps by creating a labelled dataset of code snippets spanningmultiple languages (Python and C++ datasets currently, with C, C#, andJavaScript datasets being released shortly). We find that existing timecomplexity calculation libraries and tools only apply to a limited number ofuse-cases. The lack of a well-defined rule based system motivates theapplication of several recently proposed code-based LMs. We demonstrate theeffectiveness of dead code elimination and increasing the maximum sequencelength of LMs. In addition to time complexity, we propose to use LMs to findspace complexities from code, and to the best of our knowledge, this is thefirst attempt to do so. Furthermore, we introduce a novel code comprehensiontask, called cross-language transfer, where we fine-tune the LM on one languageand run inference on another. Finally, we visualize the activation of theattention fed classification head of our LMs using Non-negative MatrixFactorization (NMF) to interpret our results.", "output": "TASTY: A Transformer based Approach to Space and Time complexity."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "In this paper I propose a concept of a correct loss function in a generativemodel of supervised learning for an input space $mathcal{X}$ and a label space$mathcal{Y}$, which are measurable spaces. A correct loss function in agenerative model of supervised learning must correctly measure the discrepancybetween elements of a hypothesis space $mathcal{H}$ of possible predictors andthe supervisor operator, which may not belong to $mathcal{H}$. To definecorrect loss functions, I propose a characterization of a regular conditionalprobability measure $mu_{mathcal{Y}|mathcal{X}}$ for a probability measure$mu$ on $mathcal{X} times mathcal{Y}$ relative to the projection$Pi_{mathcal{X}}: mathcal{X}timesmathcal{Y}to mathcal{X}$ as a solutionof a linear operator equation. If $mathcal{Y}$ is a separable metrizabletopological space with the Borel $sigma$-algebra $ mathcal{B} (mathcal{Y})$,I propose another characterization of a regular conditional probability measure$mu_{mathcal{Y}|mathcal{X}}$ as a minimizer of a mean square error on thespace of Markov kernels, called probabilistic morphisms, from $mathcal{X}$ to$mathcal{Y}$, using kernel mean embeddings. Using these results and usinginner measure to quantify generalizability of a learning algorithm, I give ageneralization of a result due to Cucker-Smale, which concerns the learnabilityof a regression model, to a setting of a conditional probability estimationproblem. I also give a variant of Vapnik's regularization method for solvingstochastic ill-posed problems, using inner measure, and present itsapplications.", "output": "Supervised learning with probabilistic morphisms and kernel mean embeddings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Activation functions are essential to deep learning networks. Popular andversatile activation functions are mostly monotonic functions, somenon-monotonic activation functions are being explored and show promisingperformance. But by introducing non-monotonicity, they also alter the positiveinput, which is proved to be unnecessary by the success of ReLU and itsvariants. In this paper, we double down on the non-monotonic activationfunctions' development and propose the Saturated Gaussian Error Linear Units bycombining the characteristics of ReLU and non-monotonic activation functions.We present three new activation functions built with our proposed method:SGELU, SSiLU, and SMish, which are composed of the negative portion of GELU,SiLU, and Mish, respectively, and ReLU's positive portion. The results of imageclassification experiments on CIFAR-100 indicate that our proposed activationfunctions are highly effective and outperform state-of-the-art baselines acrossmultiple deep learning architectures.", "output": "Saturated Non-Monotonic Activation Functions."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Masked Language Models (MLMs) have proven to be effective for second-passrescoring in Automatic Speech Recognition (ASR) systems. In this work, wepropose Masked Audio Text Encoder (MATE), a multi-modal masked language modelrescorer which incorporates acoustic representations into the input space ofMLM. We adopt contrastive learning for effectively aligning the modalities bylearning shared representations. We show that using a multi-modal rescorer isbeneficial for domain generalization of the ASR system when target domain datais unavailable. MATE reduces word error rate (WER) by 4%-16% on in-domain, and3%-7% on out-of-domain datasets, over the text-only baseline. Additionally,with very limited amount of training data (0.8 hours), MATE achieves a WERreduction of 8%-23% over the first-pass baseline.", "output": "Masked Audio Text Encoders are Effective Multi-Modal Rescorers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Language models (LMs) are powerful tools for natural language processing, butthey often struggle to produce coherent and fluent text when they are small.Models with around 125M parameters such as GPT-Neo (small) or GPT-2 (small) canrarely generate coherent and consistent English text beyond a few words evenafter extensive training. This raises the question of whether the emergence ofthe ability to produce coherent English text only occurs at larger scales (withhundreds of millions of parameters or more) and complex architectures (withmany layers of global attention).In this work, we introduce TinyStories, a synthetic dataset of short storiesthat only contain words that a typical 3 to 4-year-olds usually understand,generated by GPT-3.5 and GPT-4. We show that TinyStories can be used to trainand evaluate LMs that are much smaller than the state-of-the-art models (below10 million total parameters), or have much simpler architectures (with only onetransformer block), yet still produce fluent and consistent stories withseveral paragraphs that are diverse and have almost perfect grammar, anddemonstrate reasoning capabilities.We also introduce a new paradigm for the evaluation of language models: Wesuggest a framework which uses GPT-4 to grade the content generated by thesemodels as if those were stories written by students and graded by a (human)teacher. This new paradigm overcomes the flaws of standard benchmarks whichoften requires the model's output to be very structures, and moreover providesa multidimensional score for the model, providing scores for differentcapabilities such as grammar, creativity and consistency.We hope that TinyStories can facilitate the development, analysis andresearch of LMs, especially for low-resource or specialized domains, and shedlight on the emergence of language capabilities in LMs.", "output": "TinyStories: How Small Can Language Models Be and Still Speak Coherent English?."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Convolutional neural networks have made significant strides in medical imageanalysis in recent years. However, the local nature of the convolution operatorinhibits the CNNs from capturing global and long-range interactions. Recently,Transformers have gained popularity in the computer vision community and alsomedical image segmentation. But scalability issues of self-attention mechanismand lack of the CNN like inductive bias have limited their adoption. In thiswork, we present MaxViT-UNet, an Encoder-Decoder based hybrid visiontransformer for medical image segmentation. The proposed hybrid decoder, alsobased on MaxViT-block, is designed to harness the power of convolution andself-attention mechanism at each decoding stage with minimal computationalburden. The multi-axis self-attention in each decoder stage helps indifferentiating between the object and background regions much moreefficiently. The hybrid decoder block initially fuses the lower level featuresupsampled via transpose convolution, with skip-connection features coming fromhybrid encoder, then fused features are refined using multi-axis attentionmechanism. The proposed decoder block is repeated multiple times to accuratelysegment the nuclei regions. Experimental results on MoNuSeg dataset proves theeffectiveness of the proposed technique. Our MaxViT-UNet outperformed theprevious CNN only (UNet) and Transformer only (Swin-UNet) techniques by a largemargin of 2.36% and 5.31% on Dice metric respectively.", "output": "MaxViT-UNet: Multi-Axis Attention for Medical Image Segmentation."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Customer experience plays a critical role for a profitable organisation orcompany. A satisfied customer for a company corresponds to higher rates ofcustomer retention, and better representation in the market. One way to improvecustomer experience is to optimize the functionality of its call center. Inthis work, we have collaborated with the largest provider of telecommunicationsand Internet access in the country, and we formulate the customer-agent pairingproblem as a machine learning problem. The proposed learning-based methodcauses a significant improvement in performance of about $215%$ compared to arule-based method.", "output": "Improving Customer Experience in Call Centers with Intelligent Customer-Agent Pairing."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present $text{sustain.AI}$, an intelligent, context-aware recommendersystem that assists auditors and financial investors as well as the generalpublic to efficiently analyze companies' sustainability reports. The toolleverages an end-to-end trainable architecture that couples a BERT-basedencoding module with a multi-label classification head to match relevant textpassages from sustainability reports to their respective law regulations fromthe Global Reporting Initiative (GRI) standards. We evaluate our model on twonovel German sustainability reporting data sets and consistently achieve asignificantly higher recommendation performance compared to multiple strongbaselines. Furthermore, $text{sustain.AI}$ is publicly available for everyoneat ", "output": "sustain.AI: a Recommender System to analyze Sustainability Reports."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The recent popularity of large language models (LLMs) has brought asignificant impact to boundless fields, particularly through their open-endedecosystem such as the APIs, open-sourced models, and plugins. However, withtheir widespread deployment, there is a general lack of research thatthoroughly discusses and analyzes the potential risks concealed. In that case,we intend to conduct a preliminary but pioneering study covering therobustness, consistency, and credibility of LLMs systems. With most of therelated literature in the era of LLM uncharted, we propose an automatedworkflow that copes with an upscaled number of queries/responses. Overall, weconduct over a million queries to the mainstream LLMs including ChatGPT, LLaMA,and OPT. Core to our workflow consists of a data primitive, followed by anautomated interpreter that evaluates these LLMs under different adversarialmetrical systems. As a result, we draw several, and perhaps unfortunate,conclusions that are quite uncommon from this trendy community. Briefly, theyare: (i)-the minor but inevitable error occurrence in the user-generated queryinput may, by chance, cause the LLM to respond unexpectedly; (ii)-LLMs possesspoor consistency when processing semantically similar query input. In addition,as a side finding, we find that ChatGPT is still capable to yield the correctanswer even when the input is polluted at an extreme level. While thisphenomenon demonstrates the powerful memorization of the LLMs, it raisesserious concerns about using such data for LLM-involved evaluation in academicdevelopment. To deal with it, we propose a novel index associated with adataset that roughly decides the feasibility of using such data forLLM-involved evaluation. Extensive empirical studies are tagged to support theaforementioned claims.", "output": "Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Autonomous experimentation has emerged as an efficient approach to acceleratethe pace of materials discovery. Although instruments for autonomous synthesishave become popular in molecular and polymer science, solution processing ofhybrid materials and nanoparticles, examples of autonomous tools for physicalvapour deposition are scarce yet important for the semiconductor industry.Here, we report the design and implementation of an autonomous instrument forsputter deposition of thin films with controlled composition, leveraging ahighly automated sputtering reactor custom-controlled by Python, opticalemission spectroscopy (OES), and Bayesian optimization algorithm. We modeledfilm composition, measured by x-ray fluorescence, as a linear function ofemission lines monitored during the co-sputtering from elemental Zn and Titargets in N$_2$ atmosphere. A Bayesian control algorithm, informed by OES,navigates the space of sputtering power to fabricate films with user-definedcomposition, by minimizing the absolute error between desired and measuredemission signals. We validated our approach by autonomously fabricatingZn$_x$Ti$_{1-x}$N$_y$ films with deviations from the targeted cationcomposition within relative 3.5 %, even for 15 nm thin films, demonstratingthat the proposed approach can reliably synthesize thin films with specificcomposition and minimal human interference. Moreover, the proposed method canbe extended to more difficult synthesis experiments where plasma intensitydepends non-linearly on pressure, or the elemental sticking coefficientsstrongly depend on the substrate temperature.", "output": "Autonomous sputter synthesis of thin film nitrides with composition controlled by Bayesian optimization of optical plasma emission."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Optimizing for humans' latent preferences is a grand challenge in routerecommendation, where globally-scalable solutions remain an open problem.Although past work created increasingly general solutions for the applicationof inverse reinforcement learning (IRL), these have not been successfullyscaled to world-sized MDPs, large datasets, and highly parameterized models;respectively hundreds of millions of states, trajectories, and parameters. Inthis work, we surpass previous limitations through a series of advancementsfocused on graph compression, parallelization, and problem initialization basedon dominant eigenvectors. We introduce Receding Horizon Inverse Planning(RHIP), which generalizes existing work and enables control of key performancetrade-offs via its planning horizon. Our policy achieves a 16-24% improvementin global route quality, and, to our knowledge, represents the largest instanceof IRL in a real-world setting to date. Our results show critical benefits tomore sustainable modes of transportation (e.g. two-wheelers), where factorsbeyond journey time (e.g. route safety) play a substantial role. We concludewith ablations of key components, negative results on state-of-the-arteigenvalue solvers, and identify future opportunities to improve scalabilityvia IRL-specific batching strategies.", "output": "Massively Scalable Inverse Reinforcement Learning in Google Maps."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Existing solutions to zero-shot text classification either conduct promptingwith pre-trained language models, which is sensitive to the choices oftemplates, or rely on large-scale annotated data of relevant tasks formeta-tuning. In this work, we propose a new paradigm based on self-supervisedlearning to solve zero-shot text classification tasks by tuning the languagemodels with unlabeled data, called self-supervised tuning. By exploring theinherent structure of free texts, we propose a new learning objective calledfirst sentence prediction to bridge the gap between unlabeled data and textclassification tasks. After tuning the model to learn to predict the firstsentence in a paragraph based on the rest, the model is able to conductzero-shot inference on unseen tasks such as topic classification and sentimentanalysis. Experimental results show that our model outperforms thestate-of-the-art baselines on 7 out of 10 tasks. Moreover, the analysis revealsthat our model is less sensitive to the prompt design. Our code and pre-trainedmodels are publicly available at  .", "output": "Zero-Shot Text Classification via Self-Supervised Tuning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent studies have demonstrated the great power of deep learning methods,particularly Transformer and MLP, for time series forecasting. Despite itssuccess in NLP and CV, many studies found that Transformer is less effectivethan MLP for time series forecasting. In this work, we design a specialTransformer, i.e., channel-aligned robust dual Transformer (CARD for short),that addresses key shortcomings of Transformer in time series forecasting.First, CARD introduces a dual Transformer structure that allows it to captureboth temporal correlations among signals and dynamical dependence amongmultiple variables over time. Second, we introduce a robust loss function fortime series forecasting to alleviate the potential overfitting issue. This newloss function weights the importance of forecasting over a finite horizon basedon prediction uncertainties. Our evaluation of multiple long-term andshort-term forecasting datasets demonstrates that CARD significantlyoutperforms state-of-the-art time series forecasting methods, including bothTransformer and MLP-based models.", "output": "Make Transformer Great Again for Time Series Forecasting: Channel Aligned Robust Dual Transformer."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Despite substantial advancements, Natural Language Processing (NLP) modelsoften require post-training adjustments to enforce business rules, rectifyundesired behavior, and align with user values. These adjustments involveoperationalizing \"concepts\"--dictating desired model responses to certaininputs. However, it's difficult for a single entity to enumerate and define allpossible concepts, indicating a need for a multi-user, collaborative modelalignment framework. Moreover, the exhaustive delineation of a concept ischallenging, and an improper approach can create shortcuts or interfere withoriginal data or other concepts.To address these challenges, we introduce CoDev, a framework that enablesmulti-user interaction with the model, thereby mitigating individuallimitations. CoDev aids users in operationalizing their concepts using LargeLanguage Models, and relying on the principle that NLP models exhibit simplerbehaviors in local regions. Our main insight is learning a emph{local} modelfor each concept, and a emph{global} model to integrate the original data withall concepts. We then steer a large language model to generate instances withinconcept boundaries where local and global disagree. Our experiments show CoDevis effective at helping multiple users operationalize concepts and avoidinterference for a variety of scenarios, tasks, and models.", "output": "Collaborative Development of NLP models."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The great learning ability of deep learning models facilitates us tocomprehend the real physical world, making learning to simulate complicatedparticle systems a promising endeavour. However, the complex laws of thephysical world pose significant challenges to the learning based simulations,such as the varying spatial dependencies between interacting particles andvarying temporal dependencies between particle system states in different timestamps, which dominate particles' interacting behaviour and the physicalsystems' evolution patterns. Existing learning based simulation methods fail tofully account for the complexities, making them unable to yield satisfactorysimulations. To better comprehend the complex physical laws, this paperproposes a novel learning based simulation model- Graph Networks withSpatial-Temporal neural Ordinary Equations (GNSTODE)- that characterizes thevarying spatial and temporal dependencies in particle systems using a unitedend-to-end framework. Through training with real-world particle-particleinteraction observations, GNSTODE is able to simulate any possible particlesystems with high precisions. We empirically evaluate GNSTODE's simulationperformance on two real-world particle systems, Gravity and Coulomb, withvarying levels of spatial and temporal dependencies. The results show that theproposed GNSTODE yields significantly better simulations than state-of-the-artlearning based simulation methods, which proves that GNSTODE can serve as aneffective solution to particle simulations in real-world application.", "output": "Towards Complex Dynamic Physics System Simulation with Graph Neural ODEs."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "We present FIT: a transformer-based architecture with efficientself-attention and adaptive computation. Unlike original transformers, whichoperate on a single sequence of data tokens, we divide the data tokens intogroups, with each group being a shorter sequence of tokens. We employ two typesof transformer layers: local layers operate on data tokens within each group,while global layers operate on a smaller set of introduced latent tokens. Theselayers, comprising the same set of self-attention and feed-forward layers asstandard transformers, are interleaved, and cross-attention is used tofacilitate information exchange between data and latent tokens within the samegroup. The attention complexity is $O(n^2)$ locally within each group of size$n$, but can reach $O(L^{{4}/{3}})$ globally for sequence length of $L$. Theefficiency can be further enhanced by relying more on global layers thatperform adaptive computation using a smaller set of latent tokens. FIT is aversatile architecture and can function as an encoder, diffusion decoder, orautoregressive decoder. We provide initial evidence demonstrating itseffectiveness in high-resolution image understanding and generation tasks.Notably, FIT exhibits potential in performing end-to-end training ongigabit-scale data, such as 6400$times$6400 images, or 160K tokens (afterpatch tokenization), within a memory capacity of 16GB, without requiringspecific optimizations or model parallelism.", "output": "FIT: Far-reaching Interleaved Transformers."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Privacy-Preserving machine learning (PPML) can help us train and deploymodels that utilize private information. In particular, on-device MachineLearning allows us to completely avoid sharing information with a third-partyserver during inference. However, on-device models are typically less accuratewhen compared to the server counterparts due to the fact that (1) theytypically only rely on a small set of on-device features and (2) they need tobe small enough to run efficiently on end-user devices. Split Learning (SL) isa promising approach that can overcome these limitations. In SL, a largemachine learning model is divided into two parts, with the bigger part residingon the server-side and a smaller part executing on-device, aiming toincorporate the private features. However, end-to-end training of such modelsrequires exchanging gradients at the cut layer, which might encode privatefeatures or labels. In this paper, we provide insights into potential privacyrisks associated with SL and introduce a novel attack method, EXACT, toreconstruct private information. Furthermore, we also investigate theeffectiveness of various mitigation strategies. Our results indicate that thegradients significantly improve the attacker's effectiveness in all threedatasets reaching almost 100% reconstruction accuracy for some features.However, a small amount of differential privacy (DP) is quite effective inmitigating this risk without causing significant training degradation.", "output": "EXACT: Extensive Attack for Split Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The increasing adoption of text-to-speech technologies has led to a growingdemand for natural and emotive voices that adapt to a conversation's contextand emotional tone. The Emotive Narrative Storytelling (EMNS) corpus is aunique speech dataset created to enhance conversations' expressiveness andemotive quality in interactive narrative-driven systems. The corpus consists ofa 2.3-hour recording featuring a female speaker delivering labelled utterances.It encompasses eight acted emotional states, evenly distributed with a varianceof 0.68%, along with expressiveness levels and natural language descriptionswith word emphasis labels. The evaluation of audio samples from differentdatasets revealed that the EMNS corpus achieved the highest average scores inaccurately conveying emotions and demonstrating expressiveness. It outperformedother datasets in conveying shared emotions and achieved comparable levels ofgenuineness. A classification task confirmed the accurate representation ofintended emotions in the corpus, with participants recognising the recordingsas genuine and expressive. Additionally, the availability of the datasetcollection tool under the Apache 2.0 License simplifies remote speech datacollection for researchers.", "output": "EMNS /Imz/ Corpus: An emotive single-speaker dataset for narrative storytelling in games, television and graphic novels."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Overfitting remains a prevalent challenge in deep neural networks, leading tosuboptimal real-world performance. Employing regularization techniques is acommon strategy to counter this challenge, improving model generalization. Thispaper proposes Simultaneous Learning, a novel regularization approach drawingon Transfer Learning and Multi-task Learning principles, applied specificallyto the classification of hop varieties - an integral component of beerproduction. Our approach harnesses the power of auxiliary datasets in synergywith the target dataset to amplify the acquisition of highly relevant features.Through a strategic modification of the model's final layer, we enable thesimultaneous classification of both datasets without the necessity to treatthem as disparate tasks. To realize this, we formulate a loss function thatincludes an inter-group penalty. We conducted experimental evaluations usingthe InceptionV3 and ResNet50 models, designating the UFOP-HVD hop leaf datasetas the target and ImageNet and PlantNet as auxiliary datasets. Our proposedmethod exhibited a substantial performance advantage over models withoutregularization and those adopting dropout regularization, with accuracyimprovements ranging from 5 to 22 percentage points. Additionally, we introducea technique for interpretability devised to assess the quality of features byanalyzing correlations among class features in the network's convolutionallayers.", "output": "Regularization Through Simultaneous Learning: A Case Study for Hop Classification."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The superconducting linear accelerator is a highly flexiable facility formodern scientific discoveries, necessitating weekly reconfiguration and tuning.Accordingly, minimizing setup time proves essential in affording users withample experimental time. We propose a trend-based soft actor-critic(TBSAC) beamcontrol method with strong robustness, allowing the agents to be trained in asimulated environment and applied to the real accelerator directly withzero-shot. To validate the effectiveness of our method, two different typicalbeam control tasks were performed on China Accelerator Facility for SuperheavyElements (CAFe II) and a light particle injector(LPI) respectively. The orbitcorrection tasks were performed in three cryomodules in CAFe II seperately, thetime required for tuning has been reduced to one-tenth of that needed by humanexperts, and the RMS values of the corrected orbit were all less than 1mm. Theother transmission efficiency optimization task was conducted in the LPI, ouragent successfully optimized the transmission efficiency of radio-frequencyquadrupole(RFQ) to over $85%$ within 2 minutes. The outcomes of these twoexperiments offer substantiation that our proposed TBSAC approach canefficiently and effectively accomplish beam commissioning tasks while upholdingthe same standard as skilled human experts. As such, our method exhibitspotential for future applications in other accelerator commissioning fields.", "output": "Trend-Based SAC Beam Control Method with Zero-Shot in Superconducting Linear Accelerator."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Concerns regarding unfairness and discrimination in the context of artificialintelligence (AI) systems have recently received increased attention from bothlegal and computer science scholars. Yet, the degree of overlap between notionsof algorithmic bias and fairness on the one hand, and legal notions ofdiscrimination and equality on the other, is often unclear, leading tomisunderstandings between computer science and law. What types of bias andunfairness does the law address when it prohibits discrimination? What role canfairness metrics play in establishing legal compliance? In this paper, we aimto illustrate to what extent European Union (EU) non-discrimination lawcoincides with notions of algorithmic fairness proposed in computer scienceliterature and where they differ. The contributions of this paper are asfollows. First, we analyse seminal examples of algorithmic unfairness throughthe lens of EU non-discrimination law, drawing parallels with EU case law.Second, we set out the normative underpinnings of fairness metrics andtechnical interventions and compare these to the legal reasoning of the Courtof Justice of the EU. Specifically, we show how normative assumptions oftenremain implicit in both disciplinary approaches and explain the ensuinglimitations of current AI practice and non-discrimination law. We conclude withimplications for AI practitioners and regulators.", "output": "Algorithmic Unfairness through the Lens of EU Non-Discrimination Law: Or Why the Law is not a Decision Tree."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Automated Machine Learning (AutoML) is a promising direction fordemocratizing AI by automatically deploying Machine Learning systems withminimal human expertise. The core technical challenge behind AutoML isoptimizing the pipelines of Machine Learning systems (e.g. the choice ofpreprocessing, augmentations, models, optimizers, etc.). Existing PipelineOptimization techniques fail to explore deep interactions between pipelinestages/components. As a remedy, this paper proposes a novel neural architecturethat captures the deep interaction between the components of a Machine Learningpipeline. We propose embedding pipelines into a latent representation through anovel per-component encoder mechanism. To search for optimal pipelines, suchpipeline embeddings are used within deep-kernel Gaussian Process surrogatesinside a Bayesian Optimization setup. Furthermore, we meta-learn the parametersof the pipeline embedding network using existing evaluations of pipelines ondiverse collections of related datasets (a.k.a. meta-datasets). Throughextensive experiments on three large-scale meta-datasets, we demonstrate thatpipeline embeddings yield state-of-the-art results in Pipeline Optimization.", "output": "Deep Pipeline Embeddings for AutoML."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recently, methods for learning diverse skills to generate various behaviorswithout external rewards have been actively studied as a form of unsupervisedreinforcement learning. However, most of the existing methods learn a finitenumber of discrete skills, and thus the variety of behaviors that can beexhibited with the learned skills is limited. In this paper, we propose a novelmethod for learning potentially an infinite number of different skills, whichis named discovery of continuous skills on a sphere (DISCS). In DISCS, skillsare learned by maximizing mutual information between skills and states, andeach skill corresponds to a continuous value on a sphere. Because therepresentations of skills in DISCS are continuous, infinitely diverse skillscould be learned. We examine existing methods and DISCS in the MuJoCo Ant robotcontrol environments and show that DISCS can learn much more diverse skillsthan the other methods.", "output": "Unsupervised Discovery of Continuous Skills on a Sphere."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Multi-level methods are widely used for the solution of large-scale problems,because of their computational advantages and exploitation of thecomplementarity between the involved sub-problems. After a re-interpretation ofmulti-level methods from a block-coordinate point of view, we propose amulti-level algorithm for the solution of nonlinear optimization problems andanalyze its evaluation complexity. We apply it to the solution of partialdifferential equations using physics-informed neural networks (PINNs) and showon a few test problems that the approach results in better solutions andsignificant computational savings", "output": "A Block-Coordinate Approach of Multi-level Optimization with an Application to Physics-Informed Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Deformable Image Registration (DIR) plays a significant role in quantifyingdeformation in medical data. Recent Deep Learning methods have shown promisingaccuracy and speedup for registering a pair of medical images. However, in 4D(3D + time) medical data, organ motion, such as respiratory motion and heartbeating, can not be effectively modeled by pair-wise methods as they wereoptimized for image pairs but did not consider the organ motion patternsnecessary when considering 4D data. This paper presents ORRN, an OrdinaryDifferential Equations (ODE)-based recursive image registration network. Ournetwork learns to estimate time-varying voxel velocities for an ODE that modelsdeformation in 4D image data. It adopts a recursive registration strategy toprogressively estimate a deformation field through ODE integration of voxelvelocities. We evaluate the proposed method on two publicly available lung 4DCTdatasets, DIRLab and CREATIS, for two tasks: 1) registering all images to theextreme inhale image for 3D+t deformation tracking and 2) registering extremeexhale to inhale phase images. Our method outperforms other learning-basedmethods in both tasks, producing the smallest Target Registration Error of1.24mm and 1.26mm, respectively. Additionally, it produces less than 0.001%unrealistic image folding, and the computation speed is less than 1 second foreach CT volume. ORRN demonstrates promising registration accuracy, deformationplausibility, and computation efficiency on group-wise and pair-wiseregistration tasks. It has significant implications in enabling fast andaccurate respiratory motion estimation for treatment planning in radiationtherapy or robot motion planning in thoracic needle insertion.", "output": "ORRN: An ODE-based Recursive Registration Network for Deformable Respiratory Motion Estimation with Lung 4DCT Images."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "emph{Consistent teaching} is an effective paradigm for implementingknowledge distillation (KD), where both student and teacher models receiveidentical inputs, and KD is treated as a function matching task (FunMatch).However, one limitation of FunMatch is that it does not account for thetransfer of adversarial robustness, a model's resistance to adversarialattacks. To tackle this problem, we propose a simple but effective strategycalled Adversarial Function Matching (AdvFunMatch), which aims to matchdistributions for all data points within the $ell_p$-norm ball of the trainingdata, in accordance with consistent teaching. Formulated as a min-maxoptimization problem, AdvFunMatch identifies the worst-case instances thatmaximizes the KL-divergence between teacher and student model outputs, which werefer to as \"mismatched examples,\" and then matches the outputs on thesemismatched examples. Our experimental results show that AdvFunMatch effectivelyproduces student models with both high clean accuracy and robustness.Furthermore, we reveal that strong data augmentations (emph{e.g.},AutoAugment) are beneficial in AdvFunMatch, whereas prior works have found themless effective in adversarial training. Code is available aturl{", "output": "AdvFunMatch: When Consistent Teaching Meets Adversarial Robustness."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Computational RNA design has broad applications across synthetic biology andtherapeutic development. Fundamental to the diverse biological functions of RNAis its conformational flexibility, enabling single sequences to adopt a varietyof distinct 3D states. Currently, computational biomolecule design tasks areoften posed as inverse problems, where sequences are designed based on adoptinga single desired structural conformation. In this work, we propose gRNAde, ageometric RNA design pipeline that operates on sets of 3D RNA backbonestructures to explicitly account for and reflect RNA conformational diversityin its designs. We demonstrate the utility of gRNAde for improving nativesequence recovery over single-state approaches on a new large-scale 3D RNAdesign dataset, especially for multi-state and structurally diverse RNAs. Ourcode is available at ", "output": "Multi-State RNA Design with Geometric Multi-Graph Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "It is typically understood that the training of modern neural networks is aprocess of fitting the probability distribution of desired output. However,recent paradoxical observations in a number of language generation tasks letone wonder if this canonical probability-based explanation can really accountfor the empirical success of deep learning. To resolve this issue, we proposean alternative utility-based explanation to the standard supervised learningprocedure in deep learning. The basic idea is to interpret the learned neuralnetwork not as a probability model but as an ordinal utility function thatencodes the preference revealed in training data. In this perspective, trainingof the neural network corresponds to a utility learning process. Specifically,we show that for all neural networks with softmax outputs, the SGD learningdynamic of maximum likelihood estimation (MLE) can be seen as an iterationprocess that optimizes the neural network toward an optimal utility function.This utility-based interpretation can explain several otherwise-paradoxicalobservations about the neural networks thus trained. Moreover, ourutility-based theory also entails an equation that can transform the learnedutility values back to a new kind of probability estimation with whichprobability-compatible decision rules enjoy dramatic (double-digits)performance improvements. These evidences collectively reveal a phenomenon ofutility-probability duality in terms of what modern neural networks are (truly)modeling: We thought they are one thing (probabilities), until theunexplainable showed up; changing mindset and treating them as another thing(utility values) largely reconcile the theory, despite remaining subtletiesregarding its original (probabilistic) identity.", "output": "Utility-Probability Duality of Neural Networks."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Current state-of-the-art object-centric models use slots and attention-basedrouting for binding. However, this class of models has several conceptuallimitations: the number of slots is hardwired; all slots have equal capacity;training has high computational cost; there are no object-level relationalfactors within slots. Synchrony-based models in principle can address theselimitations by using complex-valued activations which store binding informationin their phase components. However, working examples of such synchrony-basedmodels have been developed only very recently, and are still limited to toygrayscale datasets and simultaneous storage of less than three objects inpractice. Here we introduce architectural modifications and a novel contrastivelearning method that greatly improve the state-of-the-art synchrony-basedmodel. For the first time, we obtain a class of synchrony-based models capableof discovering objects in an unsupervised manner in multi-object color datasetsand simultaneously representing more than three objects", "output": "Contrastive Training of Complex-Valued Autoencoders for Object Discovery."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Recent studies have shown competitive performance in protein design that aimsto find the amino acid sequence folding into the desired structure. However,most of them disregard the importance of predictive confidence, fail to coverthe vast protein space, and do not incorporate common protein knowledge. Afterwitnessing the great success of pretrained models on diverse protein-relatedtasks and the fact that recovery is highly correlated with confidence, wewonder whether this knowledge can push the limits of protein design further. Asa solution, we propose a knowledge-aware module that refines low-qualityresidues. We also introduce a memory-retrieval mechanism to save more than 50%of the training time. We extensively evaluate our proposed method on the CATH,TS50, and TS500 datasets and our results show that our Knowledge-Design methodoutperforms the previous PiFold method by approximately 9% on the CATHdataset. Specifically, Knowledge-Design is the first method that achieves 60+%recovery on CATH, TS50 and TS500 benchmarks. We also provide additionalanalysis to demonstrate the effectiveness of our proposed method. The code willbe publicly available.", "output": "Knowledge-Design: Pushing the Limit of Protein Deign via Knowledge Refinement."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "The regulation of various cellular processes heavily relies on the proteincomplexes within a living cell, necessitating a comprehensive understanding oftheir three-dimensional structures to elucidate the underlying mechanisms.While neural docking techniques have exhibited promising outcomes in binaryprotein docking, the application of advanced neural architectures to multimericprotein docking remains uncertain. This study introduces SyNDock, an automatedframework that swiftly assembles precise multimeric complexes within seconds,showcasing performance that can potentially surpass or be on par with recentadvanced approaches. SyNDock possesses several appealing advantages not presentin previous approaches. Firstly, SyNDock formulates multimeric protein dockingas a problem of learning global transformations to holistically depict theplacement of chain units of a complex, enabling a learning-centric solution.Secondly, SyNDock proposes a trainable two-step SE(3) algorithm, involvinginitial pairwise transformation and confidence estimation, followed by globaltransformation synchronization. This enables effective learning for assemblingthe complex in a globally consistent manner. Lastly, extensive experimentsconducted on our proposed benchmark dataset demonstrate that SyNDockoutperforms existing docking software in crucial performance metrics, includingaccuracy and runtime. For instance, it achieves a 4.5% improvement inperformance and a remarkable millionfold acceleration in speed.", "output": "SyNDock: N Rigid Protein Docking via Learnable Group Synchronization."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Training visual reinforcement learning (RL) models in offline datasets ischallenging due to overfitting issues in representation learning andoverestimation problems in value function. In this paper, we propose a transferlearning method called Collaborative World Models (CoWorld) to improve theperformance of visual RL under offline conditions. The core idea is to use aneasy-to-interact, off-the-shelf simulator to train an auxiliary RL model as theonline \"test bed\" for the offline policy learned in the target domain, whichprovides a flexible constraint for the value function -- Intuitively, we wantto mitigate the overestimation problem of value functions outside the offlinedata distribution without impeding the exploration of actions with potentialadvantages. Specifically, CoWorld performs domain-collaborative representationlearning to bridge the gap between online and offline hidden statedistributions. Furthermore, it performs domain-collaborative behavior learningthat enables the source RL agent to provide target-aware value estimation,allowing for effective offline policy regularization. Experiments show thatCoWorld significantly outperforms existing methods in offline visual controltasks in DeepMind Control and Meta-World.", "output": "Collaborative World Models: An Online-Offline Transfer RL Approach."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Relation extraction (RE), which has relied on structurally annotated corporafor model training, has been particularly challenging in low-resource scenariosand domains. Recent literature has tackled low-resource RE by self-supervisedlearning, where the solution involves pretraining the relation embedding byRE-based objective and finetuning on labeled data by classification-basedobjective. However, a critical challenge to this approach is the gap inobjectives, which prevents the RE model from fully utilizing the knowledge inpretrained representations. In this paper, we aim at bridging the gap andpropose to pretrain and finetune the RE model using consistent objectives ofcontrastive learning. Since in this kind of representation learning paradigm,one relation may easily form multiple clusters in the representation space, wefurther propose a multi-center contrastive loss that allows one relation toform multiple clusters to better align with pretraining. Experiments on twodocument-level RE datasets, BioRED and Re-DocRED, demonstrate the effectivenessof our method. Particularly, when using 1% end-task training data, our methodoutperforms PLM-based RE classifier by 10.5% and 5.8% on the two datasets,respectively.", "output": "Continual Contrastive Finetuning Improves Low-Resource Relation Extraction."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Integrating renewable energy into the power grid while balancing supply anddemand is a complex issue, given its intermittent nature. Demand sidemanagement (DSM) offers solutions to this challenge. We propose a new methodfor DSM, in particular the problem of controlling a large population ofelectrical devices to follow a desired consumption signal. We model it as afinite horizon Markovian mean field control problem. We develop a newalgorithm, MD-MFC, which provides theoretical guarantees for convex andLipschitz objective functions. What distinguishes MD-MFC from the existing loadcontrol literature is its effectiveness in directly solving the target trackingproblem without resorting to regularization techniques on the main problem. Anon-standard Bregman divergence on a mirror descent scheme allows dynamicprogramming to be used to obtain simple closed-form solutions. In addition, weshow that general mean-field game algorithms can be applied to this problem,which expands the possibilities for addressing load control problems. Weillustrate our claims with experiments on a realistic data set.", "output": "Reimagining Demand-Side Management with Mean Field Learning."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Speech processing techniques are useful for analyzing speech and languagedevelopment in children with Autism Spectrum Disorder (ASD), who are oftenvaried and delayed in acquiring these skills. Early identification andintervention are crucial, but traditional assessment methodologies such ascaregiver reports are not adequate for the requisite behavioral phenotyping.Natural Language Sample (NLS) analysis has gained attention as a promisingcomplement. Researchers have developed benchmarks for spoken languagecapabilities in children with ASD, obtainable through the analysis of NLS. Thispaper proposes applications of speech processing technologies in support ofautomated assessment of children's spoken language development byclassification between child and adult speech and between speech and nonverbalvocalization in NLS, with respective F1 macro scores of 82.6% and 67.8%,underscoring the potential for accurate and scalable tools for ASD research andclinical use.", "output": "Understanding Spoken Language Development of Children with ASD Using Pre-trained Speech Embeddings."}, {"instruction": "If you are an expert in writing papers, please generate a good paper title for this paper based on other authors' descriptions of their abstracts.", "input": "Since acquiring perfect supervision is usually difficult, real-world machinelearning tasks often confront inaccurate, incomplete, or inexact supervision,collectively referred to as weak supervision. In this work, we present WSAUC, aunified framework for weakly supervised AUC optimization problems, which coversnoisy label learning, positive-unlabeled learning, multi-instance learning, andsemi-supervised learning scenarios. Within the WSAUC framework, we first framethe AUC optimization problems in various weakly supervised scenarios as acommon formulation of minimizing the AUC risk on contaminated sets, anddemonstrate that the empirical risk minimization problems are consistent withthe true AUC. Then, we introduce a new type of partial AUC, specifically, thereversed partial AUC (rpAUC), which serves as a robust training objective forAUC maximization in the presence of contaminated labels. WSAUC offers auniversal solution for AUC optimization in various weakly supervised scenariosby maximizing the empirical rpAUC. Theoretical and experimental results undermultiple settings support the effectiveness of WSAUC on a range of weaklysupervised AUC optimization tasks.", "output": "Weakly Supervised AUC Optimization: A Unified Partial AUC Approach."}]